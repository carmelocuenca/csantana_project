\chapter{Desarrollo}
\label{cha:development}

En este capítulo se expone el análisis de la aplicación web Ruby on Rails, de 3 capas, utilizada para el desarrollo del presente trabajo. A partir de la misma se irán elaborando una serie de iteraciones en las que se detallarán las decisiones de diseño y desarrollo tomadas para cada una de ellas. El resultado final de esta fase será la obtención de la infraestructura como código que permita realizar el despliegue de la aplicación con las tecnologías Docker y CoreOS en la nube pública de Amazon Web Services, así como en el proveedor VirtualBox.

\section{Análisis de la aplicación Ruby on Rails}

La aplicación web \kode{sample\_app\_rails\_4} es parte de un tutorial\cite{rubytutorial} sobre el uso del \textit{framework} de desarrollo de aplicaciones web Ruby on Rails. Está desarrollada mediante una combinación de simulaciones, pruebas de desarrollo TDD y pruebas de integración. 

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/figures/sampleapp.png}
\caption{Aplicación web \kode{sample\_app\_rails\_4}. \label{fig:sampleapp}}
\end{figure}

Tiene una arquitectura de 3 capas: cliente, aplicación y base de datos. Está creada a partir de páginas estáticas con contenido dinámico. Tiene un diseño de sitio web, un modelo de datos de usuario y un sistema completo de registro y autenticación, incluida la activación de cuentas y restablecimiento de contraseñas. Además, cuenta con funciones de \textit{microblogging} y redes sociales. Así, la aplicación tendrá usuarios que crearán \textit{microposts} dentro de un marco de autenticación e inicio de sesión, donde la interfaz gráfica de la página principal de un usuario concreto se visualizará como muestra la \hyperref[fig:sampleapp]{Figura \ref{fig:sampleapp}}.

La arquitectura Ruby on Rails de esta aplicación se pueden apreciar en la \hyperref[fig:architecturesampleapp]{Figura \ref{fig:architecturesampleapp}}. Sus características son:
\begin{itemize}
\item Arquitectura Modelo-Vista-Controlador (MVC): Mejora la capacidad de mantenimiento, desacoplamiento y pruebas de la aplicación.
\item Arquitectura \textit{Representational State Transfer} (REST) para servicios web.
\item Soporta las principales bases de datos como MySQL y PostgreSQL.
\item Generadores de \textit{scripts} para automatizar tareas.
\item Uso del formato de serialización de datos \textit{YAML}.
\end{itemize}

Las características se distribuyen en los siguientes componentes de Rails:
\begin{itemize}
\item \textit{Action Mailer}: Proporciona servicios de correo electrónico. 
\item \textit{Action Pack}: Capta las solicitudes de usuario realizadas por el navegador y las asigna a acciones definidas en la capa de controladores.
\subitem-- \textit{Action Controller}: Enruta solicitudes al controlador. 
\subitem-- \textit{Action Dispatcher}: Controla, analiza y procesa el enrutamiento de la solicitud del navegador web.
\subitem-- \textit{Action View}: Realiza la presentación de la página web solicitada.
\item \textit{Active Model}: Define la interfaz entre el \textit{Action Pack} y los módulos \textit{Active Record}.
\item \textit{Active Record}: Proporciona la capacidad de crear relaciones o asociaciones entre modelos y construye la capa Modelo que conecta las tablas de la base de datos con su representación en las clases Ruby.
\item \textit{Active Resource}: Administra la conexión entre servicios web \textit{RESTful} y objetos de negocio.
\item \textit{Active Support}: Colección de clases de utilidad y extensiones de bibliotecas estándar de Ruby útiles para el desarrollo en Ruby on Rails.
\item \textit{Railties}: Código básico de Rails que construye nuevas aplicaciones. 
\end{itemize}	

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/rubyonrails.png}
\caption{Integración entre características y componentes de Ruby on Rails.\footnotemark \label{fig:architecturesampleapp}}
\end{figure}

\footnotetext{Mejia (2011). Ruby on Rails Architectural Design [Figura]. Recuperado de \url{http://adrianmejia.com/blog/2011/08/11/ruby-on-rails-architectural-design}} 

Una vez vistas las características y componentes de la aplicación Ruby on Rails en estudio, se elabora un trabajo adicional a partir de ella, aplicando una serie de cambios y adiciones para la consecución de los objetivos inicialmente propuestos.

\section[Iteración 1: Arquitectura de microservicios]{Iteración 1: Conversión a una arquitectura de microservicios con el uso de Docker}

Como se muestra en la \hyperref[fig:figure_docker_microservices]{Figura \ref{fig:figure_docker_microservices}}, la primera iteración consiste en adecuar la aplicación para que utilice contenedores en su funcionamiento. Esta tarea supone el uso de la tecnología de contenedores Docker.

\begin{figure}[H]
\image{images/figures/iteration1.png}
\caption{Infraestructura de la aplicación con contenedores Docker.\label{fig:figure_docker_microservices}}
\end{figure}

La aplicación \kode{sample\_app\_rails\_4} está disponible en un repositorio GitHub. Para trabajar con ella se hará una copia del repositorio a través de la opción \textit{fork}. Esto crea una bifurcación que permite la libre experimentación de cambios, sin afectar el proyecto original, utilizando el proyecto de otra persona como punto de partida de una idea propia.

Mediante Docker se obtienen las imágenes pertenecientes a los servicios \kode{PostgreSQL} y \kode{Nginx}. El primero implementa la funcionalidad de la base de datos y el segundo el servidor web y proxy inverso. El proxy inverso es un proxy que aparenta ser un servidor web ante los clientes, pero que en realidad reenvía las solicitudes que recibe a uno o más servidores de origen. Se escoge \kode{Nginx} por ser multiplataforma, ligero, de alto rendimiento y software libre.

La aplicación en su origen utiliza una base de datos \kode{SQLite} que se cambia por \kode{PostgreSQL}, presente en el contenedor \kode{some-postgres}. 

Otro de los cambios a implementar es la sustitución del servidor web \kode{rails s} por \kode{puma}, construido para ofrecer mayor velocidad y paralelismo.

Para crear un contenedor que contenga la aplicación habrá que crear su imagen Docker, que se llamará \kode{sample\_app\_rails\_4\_image}. A efectos de mantenerla remotamente se subirá al repositorio de imágenes Docker Hub con la etiqueta \kode{initial}.

El principal propósito de este despliegue es que pueda realizarse automáticamente tras la ejecución de un \textit{script}. Este fichero comprobará que las variables de entorno que especifican el nombre y la contraseña del usuario de las distintas bases de datos PostgreSQL existentes (test, desarrollo y producción) están correctamente establecidas.

Luego se creará el contenedor \kode{some-postgres} con el servidor \kode{PostgreSQL} y, seguidamente, se crearán los contenedores \kode{app-job} y \kode{app-task}, a partir de la imagen de la aplicación web, enlazándolos con el contenedor \kode{some-postgres}, que será su base de datos.

Con la intención de utilizar como servidor web de la aplicación un contenedor diferente al propio, se crea el contenedor \kode{some-nginx} que proporciona el servidor \kode{Nginx}. Se le indicará que el tráfico del puerto 8080 en el sistema anfitrión se redirija al 80 del contenedor. Así, el sitio web será accesible por la dirección local en el puerto 8080. Además, se enlazará a la aplicación web mediante el contenedor en el que se ejecuta.

También se crea un volumen Docker de datos, llamado \kode{volume-public}, para compartirlo entre la aplicación web, la base de datos y el servidor proxy.

Con todo ello, quedará construir, migrar y poblar la base de datos, desde \kode{app-job}, para, finalmente, ejecutar el servidor \kode{puma} dentro del contenedor \kode{app-task} y comprobar desde el sistema anfitrión que la página principal de la aplicación \kode{sample\_app\_rails\_4} está disponible en el puerto 8080. Esto producirá el resultado de la \hyperref[fig:resultado1]{Figura \ref{fig:resultado1}}.

\subsection{Preparación del repositorio local y remoto}

En primer lugar se realiza un \textit{fork} del repositorio GitHub de la aplicación \kode{sample\_app\_rails\_4} y se clona esta copia para trabajar localmente:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }git clone https://github.com/CarolinaSantana/sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZhy{}1.git 
\end{Verbatim}
\end{framed_shaded}

Luego se entra en la carpeta local que lo contiene, seleccionando como \textit{gemset}, conjunto aislado de gemas incorporadas en la aplicación para la versión Ruby en uso, \kode{2.0.0@railstutorial\_rails\_4\_0}. Además se deja lista la configuración de base de datos de ejemplo, con la intención de probar que los test que comprueban su funcionalidad pasen positivamente: 

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }\PY{n+nb}{cd }sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZhy{}1
\PY{n+nv}{\PYZdl{} }cp config/database.yml.example config/database.yml
\PY{n+nv}{\PYZdl{} }bundle install \PYZhy{}\PYZhy{}without production
\PY{n+nv}{\PYZdl{} }bundle \PY{n+nb}{exec }rake db:migrate
\PY{n+nv}{\PYZdl{} }bundle \PY{n+nb}{exec }rake db:test:prepare
\PY{n+nv}{\PYZdl{} }bundle \PY{n+nb}{exec }rspec spec/
\end{Verbatim}
\end{framed_shaded}

\subsection{Cambio y configuración de la base de datos} \label{postgrescredentials}

Con la intención de realizar el cambio de la base de datos se elimina del fichero \kode{Gemfile} la gema \kode{sqlite3 1.3.8}, como base de datos en desarrollo, y se cambia por una base de datos PosgreSQL, concretamente la versión \kode{pg 0.15.1}. Para instalar la nueva gema se ejecuta:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }bundle install \PYZhy{}\PYZhy{}without production
\end{Verbatim}
\end{framed_shaded}

La instalación de \kode{PostgreSQL} requiere credenciales de usuario con contraseña para acceder y se especifica mediante las variables de entorno \kode{\$POSTGRES\_USER} y \kode{\$POSTGRES\_PASSWORD} en el fichero local \kode{\textasciitilde{}/.postgres/credentials}, cuyo contenido es:

\begin{codelisting}
\label{code:credentials}
\codecaption{Fichero \kode{\textasciitilde{}/.postgres/credentials}}
\begin{code}
export POSTGRES_USER=postgres
export POSTGRES_PASSWORD=postgres
\end{code}
\end{codelisting}

Se da permiso de ejecución tanto al fichero como a la carpeta que lo contiene y se exportan las variables de entorno en el directorio de la aplicación:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }chmod \PY{l+m}{0700} \PYZti{}/.postgres \PY{o}{\PYZam{}\PYZam{}} chmod \PY{l+m}{0700} \PYZti{}/.postgres/credentials
\PY{n+nv}{\PYZdl{} }. \PYZti{}/.postgres/credentials
\end{Verbatim}
\end{framed_shaded}

El fichero de configuración de la base de datos ha de especificar lo siguiente:

\begin{codelisting}
\label{code:database}
\codecaption{Cambios en el fichero \kode{config/database.yml}}
\begin{code}
default: &default
  adapter: postgresql
  encoding: unicode
  pool: 5
  username: <%= ENV['POSTGRES_USER'] %>
  password: <%= ENV['POSTGRES_PASSWORD'] %>
  port: 5432
development:
  <<: *default
  database: sample_app_development  
  host: db
test:
  <<: *default
  database: sample_app_test
  host: db
production:
  <<: *default
  database: sample_app_production
\end{code}
\end{codelisting}

El nombre de usuario y contraseña de la base de datos de desarrollo, test y producción se indica con variables de entorno. Además, se especifica que ha de conectarse a \textit{db}, que será el nombre por el que descubrir el contenedor Docker que provisiona el servidor \kode{PostgreSQL}.

Por motivos de seguridad el fichero de configuración de la base de datos no debe subirse al repositorio remoto, por lo que se hace una copia para tener esta configuración de ejemplo:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }cp config/database.yml config/database.yml.postgres
\end{Verbatim}
\end{framed_shaded}

\subsection{Cambio del alimentador idempotente de base de datos}

Cuando se alimenta la base de datos solo han de crearse datos en ella si éstos aún no existen. Para ello se cambia a \textit{seed} idempotente el fichero que viene por defecto, \kode{lib/tasks/sample\_data.rake}, quedando como:

\begin{codelisting}
\label{code:idempotentseed}
\codecaption{Fichero \kode{lib/tasks/sample\_data.rake}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n}{namespace} \PY{l+s+ss}{:db} \PY{k}{do}
  \PY{n}{desc} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Fill database with sample data}\PY{l+s+s2}{\PYZdq{}}
  \PY{n}{task} \PY{l+s+ss}{populate}\PY{p}{:} \PY{l+s+ss}{:environment} \PY{k}{do}
    \PY{n}{make\PYZus{}users\PYZus{}microposts\PYZus{}relationships}
  \PY{k}{end}
\PY{k}{end}
\PY{k}{def} \PY{n+nf}{make\PYZus{}users\PYZus{}microposts\PYZus{}relationships}
  \PY{n+no}{User}\PY{o}{.}\PY{n}{find\PYZus{}or\PYZus{}create\PYZus{}by}\PY{p}{(}\PY{l+s+ss}{email}\PY{p}{:}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{example@railstutorial.org}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{k}{do} \PY{o}{|}\PY{n}{admin}\PY{o}{|}
  	\PY{n}{admin}\PY{o}{.}\PY{n}{name} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Example User}\PY{l+s+s2}{\PYZdq{}}
	\PY{n}{admin}\PY{o}{.}\PY{n}{password} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{foobar}\PY{l+s+s2}{\PYZdq{}}
        \PY{n}{admin}\PY{o}{.}\PY{n}{password\PYZus{}confirmation} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{foobar}\PY{l+s+s2}{\PYZdq{}}
        \PY{n}{admin}\PY{o}{.}\PY{n}{admin} \PY{o}{=} \PY{k+kp}{true}
	\PY{l+m+mi}{99}\PY{o}{.}\PY{n}{times} \PY{k}{do} \PY{o}{|}\PY{n}{n}\PY{o}{|}
	    \PY{n+nb}{name}  \PY{o}{=} \PY{n+no}{Faker}\PY{o}{::}\PY{n+no}{Name}\PY{o}{.}\PY{n}{name}
	    \PY{n}{email} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{example\PYZhy{}}\PY{l+s+si}{\PYZsh{}\PYZob{}}\PY{n}{n}\PY{o}{+}\PY{l+m+mi}{1}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{@railstutorial.org}\PY{l+s+s2}{\PYZdq{}}
	    \PY{n}{password}  \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{password}\PY{l+s+s2}{\PYZdq{}}
	    \PY{n+no}{User}\PY{o}{.}\PY{n}{create!}\PY{p}{(}\PY{n+nb}{name}\PY{p}{:}     \PY{n+nb}{name}\PY{p}{,}
                 \PY{l+s+ss}{email}\PY{p}{:}    \PY{n}{email}\PY{p}{,}
                 \PY{l+s+ss}{password}\PY{p}{:} \PY{n}{password}\PY{p}{,}
                 \PY{l+s+ss}{password\PYZus{}confirmation}\PY{p}{:} \PY{n}{password}\PY{p}{)}
        \PY{k}{end}
	\PY{n}{users} \PY{o}{=} \PY{n+no}{User}\PY{o}{.}\PY{n}{all}\PY{p}{(}\PY{l+s+ss}{limit}\PY{p}{:} \PY{l+m+mi}{6}\PY{p}{)}
  	\PY{l+m+mi}{50}\PY{o}{.}\PY{n}{times} \PY{k}{do}
	    \PY{n}{content} \PY{o}{=} \PY{n+no}{Faker}\PY{o}{::}\PY{n+no}{Lorem}\PY{o}{.}\PY{n}{sentence}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}
	    \PY{n}{users}\PY{o}{.}\PY{n}{each} \PY{p}{\PYZob{}} \PY{o}{|}\PY{n}{user}\PY{o}{|} \PY{n}{user}\PY{o}{.}\PY{n}{microposts}\PY{o}{.}\PY{n}{create!}\PY{p}{(}\PY{l+s+ss}{content}\PY{p}{:} \PY{n}{content}\PY{p}{)} \PY{p}{\PYZcb{}}
	\PY{k}{end}  
	\PY{n}{users} \PY{o}{=} \PY{n+no}{User}\PY{o}{.}\PY{n}{all}
	\PY{n}{user}  \PY{o}{=} \PY{n}{users}\PY{o}{.}\PY{n}{first}
	\PY{n}{followed\PYZus{}users} \PY{o}{=} \PY{n}{users}\PY{o}{[}\PY{l+m+mi}{2}\PY{o}{.}\PY{n}{.}\PY{l+m+mi}{50}\PY{o}{]}
	\PY{n}{followers}      \PY{o}{=} \PY{n}{users}\PY{o}{[}\PY{l+m+mi}{3}\PY{o}{.}\PY{n}{.}\PY{l+m+mi}{40}\PY{o}{]}
	\PY{n}{followed\PYZus{}users}\PY{o}{.}\PY{n}{each} \PY{p}{\PYZob{}} \PY{o}{|}\PY{n}{followed}\PY{o}{|} \PY{n}{user}\PY{o}{.}\PY{n}{follow!}\PY{p}{(}\PY{n}{followed}\PY{p}{)} \PY{p}{\PYZcb{}}
	\PY{n}{followers}\PY{o}{.}\PY{n}{each}      \PY{p}{\PYZob{}} \PY{o}{|}\PY{n}{follower}\PY{o}{|} \PY{n}{follower}\PY{o}{.}\PY{n}{follow!}\PY{p}{(}\PY{n}{user}\PY{p}{)} \PY{p}{\PYZcb{}}
  \PY{k}{end}
\PY{k}{end}
\end{Verbatim}
\end{codelisting}

\subsection{Cambio y configuración del servidor web}

Para acceder a la aplicación se escoge cambiar el servidor web \kode{rails s} por \kode{puma}. Para ello se añade al fichero \kode{Gemfile} la gema \kode{puma} y se instala: 

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }bundle install \PYZhy{}\PYZhy{}without production
\end{Verbatim}
\end{framed_shaded}

\subsection{Configuración para la creación de los contenedores}

En primer lugar se configuran los registros o \textit{logs} que crearán los contenedores añadiendo en el fichero \kode{config/application.rb} lo siguiente:

\begin{codelisting}
\label{code:application.rb}
\codecaption{Fichero \kode{config/application.rb}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{o}{.}
\PY{n}{config}\PY{o}{.}\PY{n}{logger} \PY{o}{=} \PY{n+no}{Logger}\PY{o}{.}\PY{n}{new}\PY{p}{(}\PY{n+no}{STDOUT}\PY{p}{)}
\PY{o}{.}
\end{Verbatim}
\end{codelisting}

Para que la imagen Docker que se construya del repositorio sea más ligera se añade el fichero \kode{.dockerignore}, como copia de \kode{.gitignore}:

\begin{codelisting}
\label{code:.dockerignore}
\codecaption{Fichero \kode{.dockerignore}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{o}{.}
\PY{n}{cp} \PY{o}{.}\PY{n}{gitignore} \PY{o}{.}\PY{n}{dockerignore}
\PY{o}{.}
\end{Verbatim}
\end{codelisting}

La creación de los contenedores se lleva a cabo ejecutando el \textit{script} \kode{docker-microservice.sh}.

Primero comprueba que las variables de entorno con el nombre y la contraseña del usuario de la base de datos PostgreSQL están establecidas.

Para crear el contenedor de la aplicación hay que construir su imagen, \kode{sample\_app\_rails\_4\_image}, previamente. El fichero \kode{Dockerfile} especifica cómo crearla, indicando que lo hará a partir de una imagen Ruby y que debe actualizar todos los paquetes e instalar \textit{nodejs}, requerido por la aplicación, y \textit{netcat}, para comprobar que el servicio PostgreSQL está listo, terminando con una limpieza de las capas intermedias. 

La lógica de construcción de la aplicación, basada en su imagen, pasa por la creación de dos contenedores. El primer contenedor, \kode{app-job}, se configurará a partir de un \textit{entrypoint}, llamado \kode{setup.sh}, que va a permitir ejecutar el contenedor como un ejecutable. Este contenedor esperará a que el servicio \textit{PostgreSQL} esté activo en el contenedor \kode{some-postgres} para crear, migrar, alimentar y poblar las bases de datos. Además, la aplicación implementa el modo de autenticación por medio de \textit{tokens}. Cuando el cliente se autentica mediante su usuario y contraseña cada petición HTTP que hace irá acompañada de un \textit{token} en la cabecera, permitiendo que sea identificado. En el actual entorno de desarrollo va a ser utilizado un determinado \textit{token} de prueba. Para ello se realiza la copia del actual \kode{.secret}, localizado en el directorio principal, como \kode{.secret.example}. Luego se incluye como \textit{token} en el fichero. Una vez acaba, el contenedor \kode{app-task} vuelve al estado inactivo. 

\begin{codelisting}
\label{code:dockerfile}
\codecaption{Fichero \kode{setup.sh}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}!/bin/sh}

\PY{n}{cp} \PY{n}{config}\PY{o}{/}\PY{n}{database}\PY{o}{.}\PY{n}{yml}\PY{o}{.}\PY{n}{postgresql} \PY{n}{config}\PY{o}{/}\PY{n}{database}\PY{o}{.}\PY{n}{yml}
\PY{n}{echo} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Waiting PostgreSQL to start on 5432...}\PY{l+s+s2}{\PYZdq{}}
\PY{k}{while} \PY{o}{!} \PY{n}{nc} \PY{o}{\PYZhy{}}\PY{n}{z} \PY{n}{some}\PY{o}{\PYZhy{}}\PY{n}{postgres} \PY{l+m+mi}{5432}\PY{p}{;} \PY{k}{do}
  \PY{n+nb}{sleep} \PY{l+m+mi}{0}\PY{o}{.}\PY{l+m+mi}{1}
\PY{n}{done}
\PY{n}{echo} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{PostgreSQL started}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{echo} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Creating databases...}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{rake} \PY{l+s+ss}{db}\PY{p}{:}\PY{n}{create}
\PY{n}{cp} \PY{o}{.}\PY{n}{/}\PY{o}{.}\PY{n}{secret}\PY{o}{.}\PY{n}{example} \PY{o}{.}\PY{n}{/}\PY{o}{.}\PY{n}{secret}
\PY{n}{echo} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Migrating to databases...}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{rake} \PY{l+s+ss}{db}\PY{p}{:}\PY{n}{migrate}
\PY{n}{echo} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Seeding databases...}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{rake} \PY{l+s+ss}{db}\PY{p}{:}\PY{n}{seed}
\PY{n}{echo} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Populating databases...}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{rake} \PY{l+s+ss}{db}\PY{p}{:}\PY{n}{populate}
\PY{n}{echo} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ready databases}\PY{l+s+s2}{\PYZdq{}}
\end{Verbatim}
\end{codelisting}

Así como se tiene el contenedor \textit{job} se necesita un segundo contenedor \textit{task}, llamado \kode{app-task}. Este contenedor se construye tras el anterior.

\begin{codelisting}
\label{code:dockerfile}
\codecaption{Contenido de \kode{Dockerfile}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+no}{FROM} \PY{l+s+ss}{ruby}\PY{p}{:}\PY{l+m+mi}{2}\PY{o}{.}\PY{l+m+mi}{0}\PY{o}{\PYZhy{}}\PY{n}{onbuild}
\PY{n+no}{LABEL} \PY{n}{sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image}\PY{o}{.}\PY{n}{version}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{0.1}\PY{l+s+s2}{\PYZdq{}} 
      \PY{n}{sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image}\PY{o}{.}\PY{n}{release}\PY{o}{\PYZhy{}}\PY{n}{date}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{2016\PYZhy{}12\PYZhy{}10}\PY{l+s+s2}{\PYZdq{}}
\PY{n+no}{MAINTAINER} \PY{n+no}{Carolina} \PY{n+no}{Santana} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{c.santanamartel@gmail.com}\PY{l+s+s2}{\PYZdq{}}
\PY{n+no}{RUN} \PY{n}{apt}\PY{o}{\PYZhy{}}\PY{n}{get} \PY{n}{update} \PY{o}{\PYZam{}\PYZam{}} \PY{n}{apt}\PY{o}{\PYZhy{}}\PY{n}{get} \PY{o}{\PYZhy{}}\PY{n}{y} \PY{n}{install} \PY{n}{nodejs} \PY{o}{\PYZam{}\PYZam{}} \PY{p}{\PYZbs{}}
    \PY{n}{apt}\PY{o}{\PYZhy{}}\PY{n}{get} \PY{o}{\PYZhy{}}\PY{n}{y} \PY{n}{install} \PY{n}{netcat} \PY{o}{\PYZam{}\PYZam{}} \PY{p}{\PYZbs{}}
    \PY{n}{apt}\PY{o}{\PYZhy{}}\PY{n}{get} \PY{n}{autoclean} \PY{o}{\PYZam{}\PYZam{}} \PY{n}{apt}\PY{o}{\PYZhy{}}\PY{n}{get} \PY{n}{clean} \PY{o}{\PYZam{}\PYZam{}} \PY{p}{\PYZbs{}}
    \PY{n}{rm} \PY{o}{\PYZhy{}}\PY{n}{rf} \PY{l+s+sr}{/}\PY{l+s+sr}{var}\PY{l+s+sr}{/}\PY{n}{lib}\PY{o}{/}\PY{n}{apt}\PY{o}{/}\PY{n}{lists}\PY{o}{/}\PY{o}{*} \PY{o}{\PYZam{}\PYZam{}} \PY{p}{\PYZbs{}}
    \PY{n}{rm} \PY{o}{\PYZhy{}}\PY{n}{f} \PY{l+s+sr}{/}\PY{l+s+sr}{tmp}\PY{l+s+sr}{/}\PY{o}{*} \PY{l+s+sr}{/}\PY{l+s+sr}{var}\PY{l+s+sr}{/}\PY{n}{tmp}\PY{o}{/}\PY{o}{*}
\end{Verbatim}
\end{codelisting}

Con la intención de exportar un volumen compartido entre los contenedores de la aplicación y \kode{some-nginx} para que compartan el directorio \kode{/usr/src/app/public} y el directorio \kode{/var/lib/postgresql} con el contenedor \kode{some-postgres} se crea un volumen Docker llamado \kode{volume-public}.

La siguiente acción es la creación del contenedor \kode{some-postgres} con el servidor \kode{PostgreSQL}, a partir de la imagen \kode{postgres} indicada con \textit{\--d}. Para acceder a la base de datos es necesario pasarle las credenciales, con \textit{\--e}. El volumen creado se exporta mediante \textit{\--v}.

Ahora se crea el contenedor \textit{job} de la aplicación web, llamado \kode{app-job}, a partir de la imagen de ésta, \textit{\--d}, indicando como \textit{\---entrypoint} el \textit{script} comentado y enlazando la base de datos de la misma con el contenedor \kode{some-postgres} a través del indicador \textit{\---link}. Además, se le pasan las variables de entorno para acceder a la base de datos, con \textit{\--e}. Por su parte, el indicador \textit{\--it} ofrece un terminal interactivo dentro del contedor y \textit{\--w} indica que se va a compartir el directorio inicial del proyecto local en el contenedor. El volumen creado se exporta con \textit{\--v}.

El siguiente paso crea el contenedor \textit{task} de la aplicación web, llamado \kode{app-task}. Se construye como el anterior sin el \textit{entrypoint}, configurando y preparando las bases de datos, el \textit{token} de prueba y ejecutando el servidor \kode{puma} en el puerto \kode{9292}. Todo ello a través de \textit{/bin/bash \--c}

Para crear el contenedor \kode{some-nginx} que proporciona el servidor Nginx se indica la imagen con \textit{\--d}, que el tráfico del puerto 8080 en el sistema anfitrión se redirija al 80 del contenedor, \textit{\--p}, y se enlaza la aplicación web con el contenedor en el que se ejecuta, a través del indicador \textit{\---link}. Además se monta el volumen \kode{volume-public} con el indicador \textit{\--v}. Por último, se crea localmente el fichero de configuración \kode{/etc/nginx/conf.d/nginx.conf} en el que se especifica que este servidor escuchará en el puerto 80, hará uso del directorio \kode{/usr/src/app/public}, se agregan los campos de redirección y nombre del servidor al encabezado de solicitud, se deshabilita la redirección a otra ruta, se indican los ficheros para el procesamiento de solicitudes, así como que ha de resolver la aplicación en el puerto 9292, usado por \kode{puma}.  

\begin{codelisting}
\label{code:nginxconf}
\codecaption{Fichero \kode{nginx.conf}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n}{server} \PY{o}{\PYZob{}}
  \PY{n}{listen} \PY{l+m+mi}{80}\PY{o}{;}
  \PY{n}{root} \PY{o}{/}\PY{n}{usr}\PY{o}{/}\PY{n}{src}\PY{o}{/}\PY{n}{app}\PY{o}{/}\PY{k+kd}{public}\PY{o}{;}
  \PY{n}{location} \PY{o}{/} \PY{o}{\PYZob{}}
    \PY{n}{proxy\PYZus{}set\PYZus{}header} \PY{n}{X}\PY{o}{\PYZhy{}}\PY{n}{Forwarded}\PY{o}{\PYZhy{}}\PY{n}{For} \PY{n}{\PYZdl{}proxy\PYZus{}add\PYZus{}x\PYZus{}forwarded\PYZus{}for}\PY{o}{;}
    \PY{n}{proxy\PYZus{}set\PYZus{}header} \PY{n}{Host} \PY{n}{\PYZdl{}http\PYZus{}host}\PY{o}{;}
    \PY{n}{proxy\PYZus{}redirect} \PY{n}{off}\PY{o}{;}
    \PY{n}{try\PYZus{}files} \PY{n}{\PYZdl{}uri} \PY{o}{/}\PY{n}{page\PYZus{}cache}\PY{o}{/}\PY{n}{\PYZdl{}uri} \PY{o}{/}\PY{n}{page\PYZus{}cache}\PY{o}{/}\PY{n}{\PYZdl{}uri}\PY{o}{.}\PY{n+na}{html} \PY{n+nd}{@app}\PY{o}{;}
  \PY{o}{\PYZcb{}}
  \PY{n}{location} \PY{n+nd}{@app}\PY{o}{\PYZob{}}
    \PY{n}{proxy\PYZus{}pass} \PY{n+nl}{http:}\PY{c+c1}{//app:9292;}
    \PY{k}{break}\PY{o}{;}
  \PY{o}{\PYZcb{}}
\PY{o}{\PYZcb{}}
\end{Verbatim}
\end{codelisting}

El \textit{script} \kode{./docker-microservices.sh}, con permiso \kode{chmod +x} es:

\begin{codelisting}
\label{code:scriptdocker}
\codecaption{Contenido de \kode{docker-microservices.sh}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{c}{\PYZsh{}!/bin/bash}

\PY{k}{if} \PY{o}{[} \PY{l+s+s2}{\PYZdq{}\PYZdl{}POSTGRES\PYZus{}USER\PYZdq{}} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}\PYZdq{}} \PY{o}{]} \PY{o}{||} \PY{o}{[} \PY{l+s+s2}{\PYZdq{}\PYZdl{}POSTGRES\PYZus{}PASSWORD\PYZdq{}} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}\PYZdq{}} \PY{o}{]}\PY{p}{;} \PY{k}{then}
	\PY{n+nb}{echo} \PY{l+s+s2}{\PYZdq{}Environment variables for POSTGRES not found\PYZdq{}}
        \PY{n+nb}{exit}
\PY{k}{fi}
docker build \PYZhy{}t sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image .
docker volume create \PYZhy{}\PYZhy{}name volume\PYZhy{}public
docker run \PYZhy{}\PYZhy{}name some\PYZhy{}postgres \PYZhy{}e \PY{n+nv}{POSTGRES\PYZus{}USER}\PY{o}{=}\PY{n+nv}{\PYZdl{}POSTGRES\PYZus{}USER} \PY{l+s+se}{\PYZbs{}}
  \PYZhy{}e \PY{n+nv}{POSTGRES\PYZus{}PASSWORD}\PY{o}{=}\PY{n+nv}{\PYZdl{}POSTGRES\PYZus{}PASSWORD} \PY{l+s+se}{\PYZbs{}}
  \PYZhy{}v volume\PYZhy{}public:/var/lib/postgresql \PYZhy{}d postgres
docker run \PYZhy{}i \PYZhy{}\PYZhy{}name app\PYZhy{}job \PYZhy{}\PYZhy{}entrypoint ./setup.sh \PY{l+s+se}{\PYZbs{}}
  \PYZhy{}e \PY{n+nv}{POSTGRES\PYZus{}USER}\PY{o}{=}\PY{n+nv}{\PYZdl{}POSTGRES\PYZus{}USER} \PYZhy{}e \PY{n+nv}{POSTGRES\PYZus{}PASSWORD}\PY{o}{=}\PY{n+nv}{\PYZdl{}POSTGRES\PYZus{}PASSWORD} \PY{l+s+se}{\PYZbs{}}
  \PYZhy{}w /usr/src/app \PYZhy{}v volume\PYZhy{}public:/var/lib/postgresql \PY{l+s+se}{\PYZbs{}}
  \PYZhy{}\PYZhy{}link some\PYZhy{}postgres:db sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image
docker run \PYZhy{}d \PYZhy{}it \PYZhy{}\PYZhy{}name app\PYZhy{}task \PYZhy{}e \PY{n+nv}{POSTGRES\PYZus{}USER}\PY{o}{=}\PY{n+nv}{\PYZdl{}POSTGRES\PYZus{}USER} \PY{l+s+se}{\PYZbs{}}
  \PYZhy{}e \PY{n+nv}{POSTGRES\PYZus{}PASSWORD}\PY{o}{=}\PY{n+nv}{\PYZdl{}POSTGRES\PYZus{}PASSWORD} \PYZhy{}w /usr/src/app \PY{l+s+se}{\PYZbs{}}
  \PYZhy{}v volume\PYZhy{}public:/usr/src/app/public \PYZhy{}\PYZhy{}link some\PYZhy{}postgres:db \PY{l+s+se}{\PYZbs{}}
  sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image \PY{l+s+se}{\PYZbs{}}
  /bin/bash \PYZhy{}c \PY{l+s+s2}{\PYZdq{}cp config/database.yml.postgresql config/database.yml \PYZam{}\PYZam{} \PYZbs{}}
\PY{l+s+s2}{  cp ./.secret.example ./.secret \PYZam{}\PYZam{} puma \PYZhy{}p 9292\PYZdq{}}
docker run \PYZhy{}\PYZhy{}name some\PYZhy{}nginx \PY{l+s+se}{\PYZbs{}}
  \PYZhy{}v \PY{l+s+s2}{\PYZdq{}\PYZdl{}\PYZob{}PWD\PYZcb{}/nginx.conf\PYZdq{}}:/etc/nginx/conf.d/default.conf \PY{l+s+se}{\PYZbs{}}
  \PYZhy{}p 8080:80 \PYZhy{}\PYZhy{}link app\PYZhy{}task:app \PYZhy{}v volume\PYZhy{}public:/usr/src/app/public \PYZhy{}d nginx
\end{Verbatim}
\end{codelisting}

\subsection{Resultado}

Como resultado el \textit{script} anterior crea los elementos propuestos en el planteamiento.

La imagen Docker creada y las descargadas son:

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{images/figures/dockerimages.png}
\caption{Imágenes docker mostradas con \kode{docker images}.}
\end{figure}

Los contenedores Docker creados son:

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{images/figures/dockerps.png}
\caption{Contenedores docker mostrados con \kode{docker ps -a}.}
\end{figure}

Así, cuando se hace una petición, mediante el comando \kode{curl}, el contenedor \kode{some-nginx} la recibe y redirige al contenedor \kode{app-task} donde se encuentra el servidor web:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }curl http://localhost:8080
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/figures/curldocker.png}
\caption{Resultado de la petición \kode{curl http://localhost:8080}.}
\end{figure}

También puede visualizarse desde el navegador web:
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/figures/resultado1.png}
\caption{Resultado de la Iteración 1 con el uso de Docker. \label{fig:resultado1}}
\end{figure}

\section[Iteración 2: Subida de la imagen a Docker Hub]{Iteración 2: Subida de la imagen resultante de la aplicación al repositorio de imágenes Docker Hub}

A efectos de mantener la imagen remotamente se sube al repositorio Docker Hub, previa creación de una cuenta en él. Para ello se inicia sesión por y se añade una etiqueta a la imagen \kode{sample\_app\_rails\_4\_image}, especificando su identificador y el repositorio escogido:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }docker login
\PY{n+nv}{\PYZdl{} }docker tag \PYZlt{}image\PYZhy{}id\PYZgt{} carolina/sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image:initial
\PY{n+nv}{\PYZdl{} }sudo docker push carolina/sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image
\end{Verbatim}
\end{framed_shaded}

Puede ser comprobado localmente mediante:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }docker images carolina/sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image:initial
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{images/figures/dockerimages3.png}
\caption{Versión \textit{initial} de la imagen \kode{sample\_a\_rails\_4} en local.}
\end{figure}

La imagen etiquetada como \kode{initial} se aprecia ahora en Docker Hub:

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{images/figures/dockerhubinitial.png}
\caption{Versión \textit{initial} de la imagen \kode{sample\_a\_rails\_4} en Docker Hub.}
\end{figure}

\section[Iteración 3: Integración y Despliegue continuos]{Iteración 3: Integración y Despliegue continuos de la aplicación con Travis CI, Docker Hub y GitHub}

En esta iteración se busca adaptar la aplicación Ruby on Rails para que funcione con Travis CI, definiendo la estructura presentada en la \hyperref[fig:iteration2]{Figura \ref{fig:iteration2}}. 

\begin{figure}[H]
\image{images/figures/iteration3.png}
\caption{Integración y Despliegue continuos con Travis CI. \label{fig:iteration2}}
\end{figure}

Travis CI se conectará al repositorio de GitHub \kode{sample\_app\_rails\_4-1} con la intención de poder generar una imagen Docker de la aplicación tras cada cambio. Esto permite trabajar en un entorno de desarrollo en el que la integración de los cambios en el proyecto es continua, así como también su despliegue es automatizado.

Las precondiciones, condiciones y postcondiciones necesarias para construir y desplegar la aplicación en uso de manera automática se especifican en el fichero \kode{.travis.yml}. A este fichero se le añadirán, además, las credenciales de la cuenta de Docker Hub y GitHub mediante variables de entorno encriptadas. Las acciones a realizar serán la comprobación de que los tests pasan satisfactoriamente, la construcción de una nueva imagen Docker y su subida al repositorio Docker Hub.

Para llevarlo a cabo es necesario añadir una configuración de base de datos específica. En este caso se llamará \kode{travis\_ci\_test}.

Finalmente, el resultado permitirá que cuando se haga un nuevo cambio o \textit{commit} acompañado de una subida del mismo al repositorio GitHub comenzará, también, la construcción en Travis CI. Tanto en caso de fallo como de éxito se configura el envío de un correo electrónico que lo anuncie. Como se puede ver en la \hyperref[fig:dockerhub_images]{figura \ref{fig:dockerhub_images}}, al repositorio en Docker Hub se subirán 3 copias comprimidas de la imagen. La primera etiquetada con el hash del \textit{commit}, la segunda con el número de construcción en Travis CI y la tercera como la última versión, \textit{latest}.

\subsection{Vinculación con el repositorio remoto}

En primer lugar hay que ingresar en el sitio Travis CI con la cuenta de GitHub y añadir el repositorio con el que se trabaja actualmente:

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/travis_github.png}
\caption{Vinculación de GitHub con Travis CI.\label{fig:travis_github}}
\end{figure}

\subsection{Configuración de condiciones}

En segundo lugar se procede a instalar la gema \kode{travis}. Para ello se edita el fichero \kode{Gemfile}, incluyéndola en el grupo de desarrollo y test.

Luego se instala mediante:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }bundle install \PYZhy{}\PYZhy{}without production
\end{Verbatim}
\end{framed_shaded}

Para probar el correcto funcionamiento es necesario crear un nuevo fichero de configuración para la base de datos en Travis CI:

\begin{codelisting}
\label{code:travisdatabase}
\codecaption{Fichero \kode{config/database.yml.travis} }
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nb}{test}\PY{p}{:}
  \PY{l+s+ss}{adapter}\PY{p}{:} \PY{n}{postgresql}
  \PY{l+s+ss}{database}\PY{p}{:} \PY{n}{travis\PYZus{}ci\PYZus{}test}
  \PY{l+s+ss}{username}\PY{p}{:} \PY{n}{postgres}
\end{Verbatim}
\end{codelisting}

Para configurar Travis CI se crea el fichero \kode{.travis.yml} y se establecen las variables de entorno de las credenciales de Docker Hub:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }touch .travis.yml
\PY{n+nv}{\PYZdl{} }travis env \PY{n+nb}{set }DOCKER\PYZus{}USERNAME carolina
\PY{n+nv}{\PYZdl{} }travis env \PY{n+nb}{set }DOCKER\PYZus{}PASSWORD **************
\end{Verbatim}
\end{framed_shaded}

También se agrega una variable de entorno que contenga los 8 primeros caracteres del \textit{hash} del \textit{git commit}, justo debajo de las anteriores.

En la sección \kode{after\_success} del fichero se inicia sesión en Docker Hub y luego se construye la imagen. Al repositorio en Docker Hub se subirán 3 copias comprimidas de la imagen. La primera etiquetada con el hash del \textit{commit} correspondiente, la segunda con el número de construcción en Travis CI y la tercera como la última versión, \textit{latest}. Además se le indica la base de datos de prueba para la ejecución de los tests y la versión de Ruby en uso.

\begin{codelisting}
\label{code:travis}
\codecaption{Fichero \kode{.travis.yml} }
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{l+s+ss}{env}\PY{p}{:}
  \PY{l+s+ss}{global}\PY{p}{:}
  \PY{o}{\PYZhy{}} \PY{n+no}{COMMIT}\PY{o}{=}\PY{err}{\PYZdl{}}\PY{p}{\PYZob{}}\PY{n+no}{TRAVIS\PYZus{}COMMIT}\PY{o}{::}\PY{l+m+mi}{8}\PY{p}{\PYZcb{}}
\PY{l+s+ss}{language}\PY{p}{:} \PY{n}{ruby}
\PY{l+s+ss}{rvm}\PY{p}{:}
\PY{o}{\PYZhy{}} \PY{l+m+mi}{2}\PY{o}{.}\PY{l+m+mi}{0}\PY{o}{.}\PY{l+m+mi}{0}\PY{o}{\PYZhy{}}\PY{n}{p648}
\PY{l+s+ss}{bundler\PYZus{}args}\PY{p}{:} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n}{without} \PY{n}{production}
\PY{l+s+ss}{addons}\PY{p}{:}
  \PY{l+s+ss}{postgresql}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}9.3\PYZsq{}}
\PY{l+s+ss}{services}\PY{p}{:}
\PY{o}{\PYZhy{}} \PY{n}{docker}
\PY{l+s+ss}{before\PYZus{}script}\PY{p}{:}
\PY{o}{\PYZhy{}} \PY{n}{cp} \PY{n}{config}\PY{o}{/}\PY{n}{database}\PY{o}{.}\PY{n}{yml}\PY{o}{.}\PY{n}{travis} \PY{n}{config}\PY{o}{/}\PY{n}{database}\PY{o}{.}\PY{n}{yml}
\PY{o}{\PYZhy{}} \PY{n}{psql} \PY{o}{\PYZhy{}}\PY{n}{c} \PY{l+s+s1}{\PYZsq{}create database travis\PYZus{}ci\PYZus{}test;\PYZsq{}} \PY{o}{\PYZhy{}}\PY{n}{U} \PY{n}{postgres}
\PY{o}{\PYZhy{}} \PY{n+no}{RAILS\PYZus{}ENV}\PY{o}{=}\PY{n+nb}{test} \PY{n}{bundle} \PY{n+nb}{exec} \PY{n}{rake} \PY{l+s+ss}{db}\PY{p}{:}\PY{n}{migrate} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n}{trace}
\PY{l+s+ss}{script}\PY{p}{:}
\PY{o}{\PYZhy{}} \PY{n}{bundle} \PY{n+nb}{exec} \PY{n}{rspec}
\PY{l+s+ss}{notifications}\PY{p}{:}
  \PY{l+s+ss}{email}\PY{p}{:}
    \PY{l+s+ss}{recipients}\PY{p}{:}
    \PY{o}{\PYZhy{}} \PY{n}{c}\PY{o}{.}\PY{n}{santanamartel}\PY{n+nv+vi}{@gmail}\PY{o}{.}\PY{n}{com}
    \PY{l+s+ss}{on\PYZus{}success}\PY{p}{:} \PY{n}{always}
    \PY{l+s+ss}{on\PYZus{}failure}\PY{p}{:} \PY{n}{always}
\PY{l+s+ss}{sudo}\PY{p}{:} \PY{n}{required}
\PY{l+s+ss}{after\PYZus{}success}\PY{p}{:}
\PY{o}{\PYZhy{}} \PY{n}{docker} \PY{n}{login} \PY{o}{\PYZhy{}}\PY{n}{u} \PY{n+nv+vg}{\PYZdl{}DOCKER\PYZus{}USERNAME} \PY{o}{\PYZhy{}}\PY{n+nb}{p} \PY{n+nv+vg}{\PYZdl{}DOCKER\PYZus{}PASSWORD}
\PY{o}{\PYZhy{}} \PY{n}{export} \PY{n+no}{REPO}\PY{o}{=}\PY{n+nv+vg}{\PYZdl{}DOCKER\PYZus{}USERNAME}\PY{o}{/}\PY{n}{sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image}
\PY{o}{\PYZhy{}} \PY{n}{docker} \PY{n}{build} \PY{o}{\PYZhy{}}\PY{n}{f} \PY{n+no}{Dockerfile} \PY{o}{\PYZhy{}}\PY{n}{t} \PY{n+nv+vg}{\PYZdl{}REPO}\PY{p}{:}\PY{n+nv+vg}{\PYZdl{}COMMIT} \PY{o}{.}
\PY{o}{\PYZhy{}} \PY{n}{docker} \PY{n}{tag} \PY{n+nv+vg}{\PYZdl{}REPO}\PY{p}{:}\PY{n+nv+vg}{\PYZdl{}COMMIT} \PY{n+nv+vg}{\PYZdl{}REPO}\PY{l+s+ss}{:latest}
\PY{o}{\PYZhy{}} \PY{n}{docker} \PY{n}{tag} \PY{n+nv+vg}{\PYZdl{}REPO}\PY{p}{:}\PY{n+nv+vg}{\PYZdl{}COMMIT} \PY{n+nv+vg}{\PYZdl{}REPO}\PY{l+s+ss}{:travis}\PY{o}{\PYZhy{}}\PY{n+nv+vg}{\PYZdl{}TRAVIS\PYZus{}BUILD\PYZus{}NUMBER}
\PY{o}{\PYZhy{}} \PY{n}{docker} \PY{n}{push} \PY{n+nv+vg}{\PYZdl{}REPO}  
\end{Verbatim}
\end{codelisting}

\subsection{Resultado}

Finalmente, cuando se haga un nuevo \textit{commit} y subida al repositorio GitHub comenzará también la construcción en Travis CI. Tanto si se produce un fallo como si termina con éxito se ha configurado el envío de un correo electrónico para informarlo. 

A continuación, se prueba el caso de fallo comentando la línea que crea la base de datos de prueba \kode{travis\_ci\_test} y el de éxito con ella. Los correos electrónicos recibidos son:

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/figures/travisfailure.png}
\caption{Correo electrónico de Travis CI en caso de fallo.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/figures/travissuccess.png}
\caption{Correo electrónico de Travis CI en caso de éxito.}
\end{figure}

Dirigiendo a Travis CI donde se comenta el fallo o el éxito:

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/travisfailure2.png}
\caption{Caso de fallo en Travis CI.}
\end{figure}

\begin{figure}[H]
\centering
\image{images/figures/travisfailure3.png}
\caption{Motivo de fallo en Travis CI.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/travissuccess2.png}
\caption{Caso de éxito en Travis CI.}
\end{figure}

Esta construcción terminará con la creación y subida de las tres imágenes, comentadas en el planteamiento de esta iteración, en el repositorio Docker Hub:

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/dockerhubimages.png}
\caption{Repositorio de \kode{sample\_app\_rails\_4\_image}.\label{fig:dockerhub_images}}
\end{figure}

\section[Iteración 4: Despliegue en VirtualBox]{Iteración 4: Despliegue monomáquina con unidades de servicio usando Vagrant, VirtualBox y CoreOS}

En esta nueva iteración se aplica el uso del sistema operativo orientado a contenedores CoreOS. De esta manera se crearán unidades de servicio \kode{systemd} que permitirán la correcta aplicación y funcionamiento de los contenedores Docker en los que se divide la aplicación. La infraestructura del nuevo planteamiento se corresponde con la \hyperref[fig:coreosdiagram]{Figura \ref{fig:coreosdiagram}}.

\begin{figure}[H] 
\centering
\image{images/figures/coreosdiagram.png}
\caption{Infraestructura de la aplicación con unidades de servicio CoreOS.\label{fig:coreosdiagram}}
\end{figure}

En local existen dos herramientas de línea de comandos llamadas \kode{fleectl} y \kode{etcdctl} utilizadas internamente para comunicarse con los elementos del sistema CoreOS \kode{fleetd} y \kode{etcd}. Fleet se encarga de dotar a la máquina CoreOS con los servicios que se desean implementar. Etcd2 permite almacenar los datos de las máquinas CoreOS.

Para llevar a cabo esta iteración se utiliza el repositorio GitHub \kode{coreos/coreos-vagrant} que proporciona una plantilla \kode{Vagrantfile} para configurar el entorno de una máquina virtual CoreOS, usando el hipervisor software VirtualBox.

En este procedimiento se utilizan tres archivos relevantes que se describirán a lo largo del proceso. De esta forma se utilizará la misma configuración para el fichero \kode{config.rb}, se creará un nuevo \kode{cloud-config} que será llamado \kode{user-data.sampleapp.vbox} y se modificará el fichero \kode{Vagranfile}.

\subsection{Preparación del repositorio local y remoto}

En primer lugar se realiza un \textit{fork} del repositorio \kode{coreos/coreos-vagrant}. Automáticamente se crea la copia en la cuenta personal y se clona en local:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }git clone https://github.com/CarolinaSantana/coreos\PYZhy{}vagrant.git 
\end{Verbatim}
\end{framed_shaded}

Luego, se accede a la carpeta local que lo contiene y se prepara para comprobar su funcionamiento con los datos de usuario y la configuración de ejemplo. Esto se prueba ejecutando el \kode{Vagrantfile} y conectándose a la máquina virtual creada. En este caso se utiliza VirtualBox, lo que se conseguiría con \kode{vagrant up ---provider=virtualbox} o \kode{vagrant up}, puesto que es el proveedor por defecto. 

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }\PY{n+nb}{cd }coreos\PYZhy{}vagrant
\PY{n+nv}{\PYZdl{} }cp user\PYZhy{}data.sample user\PYZhy{}data
\PY{n+nv}{\PYZdl{} }cp config.rb.sample config.rb
\PY{n+nv}{\PYZdl{} }vagrant up \PY{o}{\PYZam{}\PYZam{}} vagrant ssh core\PYZhy{}01
\end{Verbatim}
\end{framed_shaded}

Comprobando que funciona correctamente:

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/vagrantssh.png}
\caption{Conexión ssh a la máquina virtual \kode{core-01}.}
\end{figure}

Además puede visualizarse la máquina mediante VirtualBox:

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/figures/vboxcore01.png}
\caption{Máquina virtual \kode{core-01} desde VirtualBox.}
\end{figure}

\subsection{Preparación de la cabecera del fichero user-data}

El fichero \kode{user-data} es el archivo de configuración \kode{cloud-config}. Especifica el \textit{discovery token} que forma parte de la \textit{URL} de descubrimiento que conecta las instancias. También indica las variables de entorno y la lista de unidades de servicio que se deben iniciar de forma predeterminada. Los parámetros \kode{coreos.etcd}, \kode{coreos.fleet} y \kode{coreos.flannel} son traducidos a una unidad \kode{systemd} parcial actuando como un archivo de configuración \kode{etcd2}, \kode{fleet} y \kode{flannel}, respectivamente. 

Como el entorno de plataforma admite la función de plantilla de \kode{coreos-cloudinit}, es posible automatizar la configuración de \kode{etcd2} con los campos \kode{\$private\_ipv4} y \kode{\$public\_ipv4}. Al generar un \textit{discovery token} se establece el tamaño del clúster, ya que \kode{etcd2} lo utiliza para determinar si todos los miembros se han unido a él. Después de inicializarse, el clúster puede crecer o reducirse. Por su parte \kode{fleet} y \kode{flannel} usan variables de entorno en su configuración.

La siguiente sección de este fichero pertenece a las unidades de servicio, de las que ya se encuentran definidas \kode{etcd2.service}, \kode{fleet.service}, \kode{flanneld.service}, donde esta última especifica la red en CoreOS, siendo utilizada la 10.1.0.0/16 como pública para los contenedores, y \kode{docker-tcp.socket}, el cual permite que se pueda usar el servicio \kode{docker-service} dentro de cada máquina.

CoreOS posee la opción de actualizar automáticamente el sistema operativo cuando una actualización es detectada o la opción de que no sean reiniciadas con la intención de que lo haga el usuario. Como se está en un entorno de desarrollo se escoge la segunda opción.

\begin{codelisting}
\label{code:cloud-config1}
\codecaption{Fichero \kode{user-data}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}cloud\PYZhy{}config}
\PY{n+nn}{\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+lScalar+lScalarPlain}{coreos}\PY{p+pIndicator}{:}
  \PY{l+lScalar+lScalarPlain}{update}\PY{p+pIndicator}{:}
    \PY{l+lScalar+lScalarPlain}{reboot\PYZhy{}strategy}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{off}\PY{l+s}{\PYZdq{}}
  \PY{l+lScalar+lScalarPlain}{etcd2}\PY{p+pIndicator}{:}
    \PY{l+lScalar+lScalarPlain}{advertise\PYZhy{}client\PYZhy{}urls}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{http://\PYZdl{}public\PYZus{}ipv4:2379}
    \PY{l+lScalar+lScalarPlain}{initial\PYZhy{}advertise\PYZhy{}peer\PYZhy{}urls}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{http://\PYZdl{}private\PYZus{}ipv4:2380}
    \PY{l+lScalar+lScalarPlain}{listen\PYZhy{}client\PYZhy{}urls}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{http://0.0.0.0:2379,http://0.0.0.0:4001}
    \PY{l+lScalar+lScalarPlain}{listen\PYZhy{}peer\PYZhy{}urls}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{http://\PYZdl{}private\PYZus{}ipv4:2380,http://\PYZdl{}private\PYZus{}ipv4:7001}
    \PY{l+lScalar+lScalarPlain}{discovery}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{https://discovery.etcd.io/acd611cb07df434a400bbd7e36d793a0}
  \PY{l+lScalar+lScalarPlain}{fleet}\PY{p+pIndicator}{:}
    \PY{l+lScalar+lScalarPlain}{public\PYZhy{}ip}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{\PYZdl{}public\PYZus{}ipv4}\PY{l+s}{\PYZdq{}}
  \PY{l+lScalar+lScalarPlain}{flannel}\PY{p+pIndicator}{:}
    \PY{l+lScalar+lScalarPlain}{interface}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{\PYZdl{}public\PYZus{}ipv4}\PY{l+s}{\PYZdq{}}
  \PY{l+lScalar+lScalarPlain}{units}\PY{p+pIndicator}{:}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{etcd2.service}
    \PY{l+lScalar+lScalarPlain}{command}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{start}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{fleet.service}
    \PY{l+lScalar+lScalarPlain}{command}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{start}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{flanneld.service}
    \PY{l+lScalar+lScalarPlain}{drop\PYZhy{}ins}\PY{p+pIndicator}{:}
    \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{50\PYZhy{}network\PYZhy{}config.conf}
      \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
        \PY{n+no}{[Service]}
        \PY{n+no}{ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config \PYZbs{}}
        \PY{n+no}{\PYZsq{}\PYZob{} \PYZdq{}Network\PYZdq{}: \PYZdq{}10.1.0.0/16\PYZdq{} \PYZcb{}\PYZsq{}}
    \PY{l+lScalar+lScalarPlain}{command}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{start}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{docker\PYZhy{}tcp.socket}
    \PY{l+lScalar+lScalarPlain}{command}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{start}
    \PY{l+lScalar+lScalarPlain}{enable}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{true}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit]}
      \PY{n+no}{Description=Docker Socket for the API}
      \PY{n+no}{[Socket]}
      \PY{n+no}{ListenStream=2375}
      \PY{n+no}{Service=docker.service}
      \PY{n+no}{BindIPv6Only=both}
      \PY{n+no}{[Install]}
      \PY{n+no}{WantedBy=sockets.target}
\end{Verbatim}
\end{codelisting}

\subsection{Interpretación del fichero config.rb}

El segundo fichero importante para este despliegue es \kode{config.rb}. Especifica el número de nodos CoreOS y proporciona una opción para generar automáticamente el \textit{discovery token}, valor que se reemplaza automáticamente cuando se inicia el despliegue o levantamiento de una máquina. Además permite la necesaria corrección del valor del parámetro perteneciente a la estrategia de reinicio de CoreOS, al tener que ser la entrada en el fichero \kode{user-data} de una manera distinta a como es traducido por YAML.

\begin{codelisting}
\label{code:cloud-config1}
\codecaption{Fichero \kode{config.rb}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv+vg}{\PYZdl{}num\PYZus{}instances}\PY{o}{=}\PY{l+m+mi}{1}
\PY{n+nv+vg}{\PYZdl{}new\PYZus{}discovery\PYZus{}url}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{https://discovery.etcd.io/new?size=}\PY{l+s+si}{\PYZsh{}\PYZob{}}\PY{n+nv+vg}{\PYZdl{}num\PYZus{}instances}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}
\PY{k}{if} \PY{n+no}{File}\PY{o}{.}\PY{n}{exists?}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}user\PYZhy{}data\PYZsq{}}\PY{p}{)} \PY{o}{\PYZam{}\PYZam{}} \PY{n+no}{ARGV}\PY{o}{[}\PY{l+m+mi}{0}\PY{o}{]}\PY{o}{.}\PY{n}{eql?}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}up\PYZsq{}}\PY{p}{)}
  \PY{n+nb}{require} \PY{l+s+s1}{\PYZsq{}open\PYZhy{}uri\PYZsq{}}
  \PY{n+nb}{require} \PY{l+s+s1}{\PYZsq{}yaml\PYZsq{}}
  \PY{n}{token} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{n+nv+vg}{\PYZdl{}new\PYZus{}discovery\PYZus{}url}\PY{p}{)}\PY{o}{.}\PY{n}{read}
  \PY{n}{data} \PY{o}{=} \PY{n+no}{YAML}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n+no}{IO}\PY{o}{.}\PY{n}{readlines}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}user\PYZhy{}data\PYZsq{}}\PY{p}{)}\PY{o}{[}\PY{l+m+mi}{1}\PY{o}{.}\PY{n}{.}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{o}{]}\PY{o}{.}\PY{n}{join}\PY{p}{)}
  \PY{k}{if} \PY{n}{data}\PY{o}{.}\PY{n}{key?} \PY{l+s+s1}{\PYZsq{}coreos\PYZsq{}} \PY{o+ow}{and} \PY{n}{data}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}coreos\PYZsq{}}\PY{o}{]}\PY{o}{.}\PY{n}{key?} \PY{l+s+s1}{\PYZsq{}etcd\PYZsq{}}
    \PY{n}{data}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}coreos\PYZsq{}}\PY{o}{]}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}etcd\PYZsq{}}\PY{o}{]}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}discovery\PYZsq{}}\PY{o}{]} \PY{o}{=} \PY{n}{token}
  \PY{k}{end}
  \PY{k}{if} \PY{n}{data}\PY{o}{.}\PY{n}{key?} \PY{l+s+s1}{\PYZsq{}coreos\PYZsq{}} \PY{o+ow}{and} \PY{n}{data}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}coreos\PYZsq{}}\PY{o}{]}\PY{o}{.}\PY{n}{key?} \PY{l+s+s1}{\PYZsq{}etcd2\PYZsq{}}
    \PY{n}{data}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}coreos\PYZsq{}}\PY{o}{]}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}etcd2\PYZsq{}}\PY{o}{]}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}discovery\PYZsq{}}\PY{o}{]} \PY{o}{=} \PY{n}{token}
  \PY{k}{end}
  \PY{c+c1}{\PYZsh{} Fix for YAML.load() converting reboot\PYZhy{}strategy from \PYZsq{}off\PYZsq{} to `false`}
  \PY{k}{if} \PY{n}{data}\PY{o}{.}\PY{n}{key?} \PY{l+s+s1}{\PYZsq{}coreos\PYZsq{}} \PY{o+ow}{and} \PY{n}{data}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}coreos\PYZsq{}}\PY{o}{]}\PY{o}{.}\PY{n}{key?} \PY{l+s+s1}{\PYZsq{}update\PYZsq{}} 
    \PY{o+ow}{and} \PY{n}{data}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}coreos\PYZsq{}}\PY{o}{]}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}update\PYZsq{}}\PY{o}{]}\PY{o}{.}\PY{n}{key?} \PY{l+s+s1}{\PYZsq{}reboot\PYZhy{}strategy\PYZsq{}}
    \PY{k}{if} \PY{n}{data}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}coreos\PYZsq{}}\PY{o}{]}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}update\PYZsq{}}\PY{o}{]}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}reboot\PYZhy{}strategy\PYZsq{}}\PY{o}{]} \PY{o}{==} \PY{k+kp}{false}
      \PY{n}{data}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}coreos\PYZsq{}}\PY{o}{]}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}update\PYZsq{}}\PY{o}{]}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}reboot\PYZhy{}strategy\PYZsq{}}\PY{o}{]} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}off\PYZsq{}}
    \PY{k}{end}
  \PY{k}{end}
  \PY{n}{yaml} \PY{o}{=} \PY{n+no}{YAML}\PY{o}{.}\PY{n}{dump}\PY{p}{(}\PY{n}{data}\PY{p}{)}
  \PY{n+no}{File}\PY{o}{.}\PY{n}{open}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}user\PYZhy{}data\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}w\PYZsq{}}\PY{p}{)} \PY{p}{\PYZob{}} \PY{o}{|}\PY{n}{file}\PY{o}{|} \PY{n}{file}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}}\PY{l+s+s2}{cloud\PYZhy{}config}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+si}{\PYZsh{}\PYZob{}}\PY{n}{yaml}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{p}{\PYZcb{}}
\PY{k}{end}
\end{Verbatim}
\end{codelisting}

\subsection{Preparación del fichero Vagrantfile}

El fichero viene preparado para el uso de los proveedores VirtualBox y VMware. Al no utilizar el segundo se prescinde de sus configuraciones y se reagrupan las instrucciones pertinentes a VirtualBox.

El fichero \kode{Vagrantfile} establece el número de máquinas CoreOS que ha de ser creado por Vagrant. En este caso una sola instancia. Otras especificaciones son las opciones de configuración por defecto a utilizar para crear las máquinas y la imagen a partir de la que hacerlo. El fichero viene preparado para aplicar la redirección de puertos de la máquina virtual al sistema anfitrión. Las direcciónes IP que tomarán las máquinas en la red privada empezarán en 172.17.8.10X. Además, se facilita la opción para habilitar el almacenamiento compartido entre anfitrión y máquina. En último lugar se realiza la copia de la configuración del \kode{user-data} en \kode{/var/lib/coreos-vagrant/vagrantfile-user-data}, dentro de la máquina virtual. Así, \kode{coreos-cloudinit} lee \kode{vagrantfile-user-data} en cada incio y lo utiliza para crear y reproducir lo que indica.

\begin{codelisting}
\label{code:cloud-config1}
\codecaption{Fichero \kode{Vagrantfile}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} \PYZhy{}*\PYZhy{} mode: ruby \PYZhy{}*\PYZhy{}}
\PY{c+c1}{\PYZsh{} \PYZsh{} vi: set ft=ruby :}
\PY{n+nb}{require} \PY{l+s+s1}{\PYZsq{}fileutils\PYZsq{}}
\PY{n+no}{Vagrant}\PY{o}{.}\PY{n}{require\PYZus{}version} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZgt{}= 1.6.0}\PY{l+s+s2}{\PYZdq{}}
\PY{k}{if} \PY{p}{(}\PY{o}{!}\PY{n+no}{ARGV}\PY{o}{.}\PY{n}{nil?} \PY{o}{\PYZam{}\PYZam{}} \PY{n+no}{ARGV}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{include?}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}provider=virtualbox\PYZsq{}}\PY{p}{)}\PY{p}{)} \PY{o}{||} 
   \PY{p}{(}\PY{o}{!}\PY{n+no}{ARGV}\PY{o}{.}\PY{n}{nil?} \PY{o}{\PYZam{}\PYZam{}} \PY{o}{!}\PY{n+no}{ARGV}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{include?}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}provider\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{n+no}{FileUtils}\PY{o}{.}\PY{n}{cp\PYZus{}r}\PY{p}{(}\PY{n+no}{File}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n+no}{File}\PY{o}{.}\PY{n}{dirname}\PY{p}{(}\PY{n+nb+bp}{\PYZus{}\PYZus{}FILE\PYZus{}\PYZus{}}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{user\PYZhy{}data.sampleapp.vbox}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
  \PY{n+no}{File}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n+no}{File}\PY{o}{.}\PY{n}{dirname}\PY{p}{(}\PY{n+nb+bp}{\PYZus{}\PYZus{}FILE\PYZus{}\PYZus{}}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{user\PYZhy{}data}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,} \PY{l+s+ss}{:remove\PYZus{}destination} \PY{o}{=}\PY{o}{\PYZgt{}} \PY{k+kp}{true}\PY{p}{)}
\PY{k}{end}
\PY{n+no}{CLOUD\PYZus{}CONFIG\PYZus{}PATH} \PY{o}{=} \PY{n+no}{File}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n+no}{File}\PY{o}{.}\PY{n}{dirname}\PY{p}{(}\PY{n+nb+bp}{\PYZus{}\PYZus{}FILE\PYZus{}\PYZus{}}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{user\PYZhy{}data}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+no}{CONFIG} \PY{o}{=} \PY{n+no}{File}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n+no}{File}\PY{o}{.}\PY{n}{dirname}\PY{p}{(}\PY{n+nb+bp}{\PYZus{}\PYZus{}FILE\PYZus{}\PYZus{}}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{config.rb}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nv+vg}{\PYZdl{}num\PYZus{}instances} \PY{o}{=} \PY{l+m+mi}{1}
\PY{n+nv+vg}{\PYZdl{}instance\PYZus{}name\PYZus{}prefix} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{core}\PY{l+s+s2}{\PYZdq{}}
\PY{n+nv+vg}{\PYZdl{}update\PYZus{}channel} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{alpha}\PY{l+s+s2}{\PYZdq{}}
\PY{n+nv+vg}{\PYZdl{}image\PYZus{}version} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{current}\PY{l+s+s2}{\PYZdq{}}
\PY{n+nv+vg}{\PYZdl{}vm\PYZus{}gui} \PY{o}{=} \PY{k+kp}{false}
\PY{n+nv+vg}{\PYZdl{}vm\PYZus{}memory} \PY{o}{=} \PY{l+m+mi}{1024}
\PY{n+nv+vg}{\PYZdl{}vm\PYZus{}cpus} \PY{o}{=} \PY{l+m+mi}{1}
\PY{n+nv+vg}{\PYZdl{}vb\PYZus{}cpuexecutioncap} \PY{o}{=} \PY{l+m+mi}{100}
\PY{n+nv+vg}{\PYZdl{}shared\PYZus{}folders} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
\PY{n+nv+vg}{\PYZdl{}forwarded\PYZus{}ports} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
\PY{k}{if} \PY{n+no}{ENV}\PY{o}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{NUM\PYZus{}INSTANCES}\PY{l+s+s2}{\PYZdq{}}\PY{o}{]}\PY{o}{.}\PY{n}{to\PYZus{}i} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0} \PY{o}{\PYZam{}\PYZam{}} \PY{n+no}{ENV}\PY{o}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{NUM\PYZus{}INSTANCES}\PY{l+s+s2}{\PYZdq{}}\PY{o}{]}
  \PY{n+nv+vg}{\PYZdl{}num\PYZus{}instances} \PY{o}{=} \PY{n+no}{ENV}\PY{o}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{NUM\PYZus{}INSTANCES}\PY{l+s+s2}{\PYZdq{}}\PY{o}{]}\PY{o}{.}\PY{n}{to\PYZus{}i}
\PY{k}{end}
\PY{k}{if} \PY{n+no}{File}\PY{o}{.}\PY{n}{exist?}\PY{p}{(}\PY{n+no}{CONFIG}\PY{p}{)}
  \PY{n+nb}{require} \PY{n+no}{CONFIG}
\PY{k}{end}
\PY{k}{def} \PY{n+nf}{vm\PYZus{}gui}
  \PY{n+nv+vg}{\PYZdl{}vb\PYZus{}gui}\PY{o}{.}\PY{n}{nil?} \PY{p}{?} \PY{n+nv+vg}{\PYZdl{}vm\PYZus{}gui} \PY{p}{:} \PY{n+nv+vg}{\PYZdl{}vb\PYZus{}gui}
\PY{k}{end}
\PY{k}{def} \PY{n+nf}{vm\PYZus{}memory}
  \PY{n+nv+vg}{\PYZdl{}vb\PYZus{}memory}\PY{o}{.}\PY{n}{nil?} \PY{p}{?} \PY{n+nv+vg}{\PYZdl{}vm\PYZus{}memory} \PY{p}{:} \PY{n+nv+vg}{\PYZdl{}vb\PYZus{}memory}
\PY{k}{end}
\PY{k}{def} \PY{n+nf}{vm\PYZus{}cpus}
  \PY{n+nv+vg}{\PYZdl{}vb\PYZus{}cpus}\PY{o}{.}\PY{n}{nil?} \PY{p}{?} \PY{n+nv+vg}{\PYZdl{}vm\PYZus{}cpus} \PY{p}{:} \PY{n+nv+vg}{\PYZdl{}vb\PYZus{}cpus}
\PY{k}{end}
\PY{n+no}{Vagrant}\PY{o}{.}\PY{n}{configure}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{k}{do} \PY{o}{|}\PY{n}{config}\PY{o}{|}
  \PY{n}{config}\PY{o}{.}\PY{n}{ssh}\PY{o}{.}\PY{n}{insert\PYZus{}key} \PY{o}{=} \PY{k+kp}{false}
  \PY{n}{config}\PY{o}{.}\PY{n}{ssh}\PY{o}{.}\PY{n}{forward\PYZus{}agent} \PY{o}{=} \PY{k+kp}{true}
  \PY{n}{config}\PY{o}{.}\PY{n}{vm}\PY{o}{.}\PY{n}{box} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{coreos\PYZhy{}\PYZpc{}s}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n+nv+vg}{\PYZdl{}update\PYZus{}channel}
  \PY{k}{if} \PY{n+nv+vg}{\PYZdl{}image\PYZus{}version} \PY{o}{!=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{current}\PY{l+s+s2}{\PYZdq{}}
      \PY{n}{config}\PY{o}{.}\PY{n}{vm}\PY{o}{.}\PY{n}{box\PYZus{}version} \PY{o}{=} \PY{n+nv+vg}{\PYZdl{}image\PYZus{}version}
  \PY{k}{end}
  \PY{n}{config}\PY{o}{.}\PY{n}{vm}\PY{o}{.}\PY{n}{provider} \PY{l+s+ss}{:virtualbox} \PY{k}{do} \PY{o}{|}\PY{n}{vb}\PY{p}{,} \PY{n}{override}\PY{o}{|}
    \PY{n}{override}\PY{o}{.}\PY{n}{vm}\PY{o}{.}\PY{n}{box\PYZus{}url} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{https://storage.googleapis.com/\PYZpc{}s.release.core\PYZhy{}os.net/}
\PY{l+s+s2}{    amd64\PYZhy{}usr/\PYZpc{}s/coreos\PYZus{}production\PYZus{}vagrant.json}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}}
    \PY{o}{[}\PY{n+nv+vg}{\PYZdl{}update\PYZus{}channel}\PY{p}{,}\PY{n+nv+vg}{\PYZdl{}image\PYZus{}version}\PY{o}{]}
    \PY{n}{vb}\PY{o}{.}\PY{n}{check\PYZus{}guest\PYZus{}additions} \PY{o}{=} \PY{k+kp}{false}
    \PY{n}{vb}\PY{o}{.}\PY{n}{functional\PYZus{}vboxsf}     \PY{o}{=} \PY{k+kp}{false}
  \PY{k}{end}
  \PY{k}{if} \PY{n+no}{Vagrant}\PY{o}{.}\PY{n}{has\PYZus{}plugin?}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{vagrant\PYZhy{}vbguest}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{k}{then}
    \PY{n}{config}\PY{o}{.}\PY{n}{vbguest}\PY{o}{.}\PY{n}{auto\PYZus{}update} \PY{o}{=} \PY{k+kp}{false}
  \PY{k}{end}
  \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{.}\PY{n}{.}\PY{n+nv+vg}{\PYZdl{}num\PYZus{}instances}\PY{p}{)}\PY{o}{.}\PY{n}{each} \PY{k}{do} \PY{o}{|}\PY{n}{i}\PY{o}{|}
    \PY{n}{config}\PY{o}{.}\PY{n}{vm}\PY{o}{.}\PY{n}{define} \PY{n}{vm\PYZus{}name} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZpc{}s\PYZhy{}\PYZpc{}02d}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{o}{[}\PY{n+nv+vg}{\PYZdl{}instance\PYZus{}name\PYZus{}prefix}\PY{p}{,} \PY{n}{i}\PY{o}{]} \PY{k}{do} \PY{o}{|}\PY{n}{config}\PY{o}{|}
      \PY{n}{config}\PY{o}{.}\PY{n}{vm}\PY{o}{.}\PY{n}{hostname} \PY{o}{=} \PY{n}{vm\PYZus{}name}
      \PY{n}{config}\PY{o}{.}\PY{n}{vm}\PY{o}{.}\PY{n}{provider} \PY{l+s+ss}{:virtualbox} \PY{k}{do} \PY{o}{|}\PY{n}{vb}\PY{p}{,} \PY{n}{override}\PY{o}{|}
        \PY{n+nv+vg}{\PYZdl{}forwarded\PYZus{}ports}\PY{o}{.}\PY{n}{each} \PY{k}{do} \PY{o}{|}\PY{n}{guest}\PY{p}{,} \PY{n}{host}\PY{o}{|}
          \PY{n}{override}\PY{o}{.}\PY{n}{vm}\PY{o}{.}\PY{n}{network} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{forwarded\PYZus{}port}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+ss}{guest}\PY{p}{:} \PY{n}{guest}\PY{p}{,} \PY{l+s+ss}{host}\PY{p}{:} \PY{n}{host}\PY{p}{,} 
          \PY{l+s+ss}{auto\PYZus{}correct}\PY{p}{:} \PY{k+kp}{true}
        \PY{k}{end}
        \PY{n}{vb}\PY{o}{.}\PY{n}{gui} \PY{o}{=} \PY{n}{vm\PYZus{}gui}
        \PY{n}{vb}\PY{o}{.}\PY{n}{memory} \PY{o}{=} \PY{n}{vm\PYZus{}memory}
        \PY{n}{vb}\PY{o}{.}\PY{n}{cpus} \PY{o}{=} \PY{n}{vm\PYZus{}cpus}
        \PY{n}{vb}\PY{o}{.}\PY{n}{customize} \PY{o}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{modifyvm}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+ss}{:id}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}cpuexecutioncap}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} 
                      \PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZsh{}\PYZob{}}\PY{n+nv+vg}{\PYZdl{}vb\PYZus{}cpuexecutioncap}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{]}
        \PY{n}{ip} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{172.17.8.}\PY{l+s+si}{\PYZsh{}\PYZob{}}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{100}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}
        \PY{n}{override}\PY{o}{.}\PY{n}{vm}\PY{o}{.}\PY{n}{network} \PY{l+s+ss}{:private\PYZus{}network}\PY{p}{,} \PY{l+s+ss}{ip}\PY{p}{:} \PY{n}{ip}
      \PY{k}{end}
      \PY{n}{config}\PY{o}{.}\PY{n}{vm}\PY{o}{.}\PY{n}{provider} \PY{l+s+ss}{:virtualbox} \PY{k}{do} \PY{o}{|}\PY{n}{vb}\PY{p}{,} \PY{n}{override}\PY{o}{|}
        \PY{c+c1}{\PYZsh{} Uncomment below to enable NFS for sharing the host machine into the VM.}
        \PY{c+c1}{\PYZsh{}override.vm.synced\PYZus{}folder \PYZdq{}.\PYZdq{}, \PYZdq{}/home/core/share\PYZdq{}, id: \PYZdq{}core\PYZdq{}, }
        \PY{c+c1}{\PYZsh{}:nfs =\PYZgt{} true, :mount\PYZus{}options =\PYZgt{} [\PYZsq{}nolock,vers=3,udp\PYZsq{}]}
        \PY{n+nv+vg}{\PYZdl{}shared\PYZus{}folders}\PY{o}{.}\PY{n}{each\PYZus{}with\PYZus{}index} \PY{k}{do} \PY{o}{|}\PY{p}{(}\PY{n}{host\PYZus{}folder}\PY{p}{,} \PY{n}{guest\PYZus{}folder}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{|}
          \PY{n}{override}\PY{o}{.}\PY{n}{vm}\PY{o}{.}\PY{n}{synced\PYZus{}folder} \PY{n}{host\PYZus{}folder}\PY{o}{.}\PY{n}{to\PYZus{}s}\PY{p}{,} \PY{n}{guest\PYZus{}folder}\PY{o}{.}\PY{n}{to\PYZus{}s}\PY{p}{,} 
          \PY{n+nb}{id}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{core\PYZhy{}share\PYZpc{}02d}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{index}\PY{p}{,} \PY{l+s+ss}{nfs}\PY{p}{:} \PY{k+kp}{true}\PY{p}{,} 
          \PY{l+s+ss}{mount\PYZus{}options}\PY{p}{:} \PY{o}{[}\PY{l+s+s1}{\PYZsq{}nolock,vers=3,udp\PYZsq{}}\PY{o}{]}
        \PY{k}{end}
      \PY{k}{end}
      \PY{k}{if} \PY{n+no}{File}\PY{o}{.}\PY{n}{exist?}\PY{p}{(}\PY{n+no}{CLOUD\PYZus{}CONFIG\PYZus{}PATH}\PY{p}{)} \PY{o}{\PYZam{}\PYZam{}} \PY{n+no}{ARGV}\PY{o}{[}\PY{l+m+mi}{0}\PY{o}{]}\PY{o}{.}\PY{n}{eql?}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}up\PYZsq{}}\PY{p}{)}
        \PY{n}{config}\PY{o}{.}\PY{n}{vm}\PY{o}{.}\PY{n}{provider} \PY{l+s+ss}{:virtualbox} \PY{k}{do} \PY{o}{|}\PY{n}{vb}\PY{p}{,} \PY{n}{override}\PY{o}{|}
          \PY{n}{override}\PY{o}{.}\PY{n}{vm}\PY{o}{.}\PY{n}{provision} \PY{l+s+ss}{:file}\PY{p}{,} \PY{l+s+ss}{:source} \PY{o}{=}\PY{o}{\PYZgt{}} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZsh{}\PYZob{}}\PY{n+no}{CLOUD\PYZus{}CONFIG\PYZus{}PATH}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} 
          \PY{l+s+ss}{:destination} \PY{o}{=}\PY{o}{\PYZgt{}} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/tmp/vagrantfile\PYZhy{}user\PYZhy{}data}\PY{l+s+s2}{\PYZdq{}}
          \PY{n}{override}\PY{o}{.}\PY{n}{vm}\PY{o}{.}\PY{n}{provision} \PY{l+s+ss}{:shell}\PY{p}{,} \PY{l+s+ss}{:inline} \PY{o}{=}\PY{o}{\PYZgt{}} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mv /tmp/vagrantfile\PYZhy{}user\PYZhy{}data }
\PY{l+s+s2}{          /var/lib/coreos\PYZhy{}vagrant/}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+ss}{:privileged} \PY{o}{=}\PY{o}{\PYZgt{}} \PY{k+kp}{true}
        \PY{k}{end}
      \PY{k}{end}
    \PY{k}{end}
  \PY{k}{end}
\PY{k}{end}
\end{Verbatim}
\end{codelisting}

\subsection{Despliegue manual de las unidades systemd}

El primer paso consiste en permitir que exista almacenamiento NFS compartido entre la máquina local y la máquina virtual CoreOS, para pasar a esta última las unidades de servicio a implementar. Para ello es necesario descomentar la siguiente línea, dentro del fichero \kode{Vagrantfile}:

\begin{codelisting}
\label{code:vagrantfile2}
\codecaption{Fichero \kode{Vagrantfile}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{o}{.}
\PY{n}{override}\PY{o}{.}\PY{n}{vm}\PY{o}{.}\PY{n}{synced\PYZus{}folder} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/home/core/share}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{id}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{core}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+ss}{:nfs} \PY{o}{=}\PY{o}{\PYZgt{}} \PY{k+kp}{true}\PY{p}{,} \PY{p}{\PYZbs{}}
\PY{l+s+ss}{:mount\PYZus{}options} \PY{o}{=}\PY{o}{\PYZgt{}} \PY{o}{[}\PY{l+s+s1}{\PYZsq{}nolock,vers=3,udp\PYZsq{}}\PY{o}{]}
\PY{o}{.}
\end{Verbatim}
\end{codelisting}

También será necesario instalar el adaptador NFS en el sistema anfitrión y cargar los cambios. Esto creará el directorio \kode{share}, compartido con el sistema anfitrión:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }sudo apt\PYZhy{}get install nfs\PYZhy{}kernel\PYZhy{}server \PYZhy{}y
\PY{n+nv}{\PYZdl{} }vagrant reload \PYZhy{}\PYZhy{}provision
\end{Verbatim}
\end{framed_shaded}

\subsubsection{Unidad volume-public.service}

La primera unidad de servicio será el volumen Docker compartido entre los contenedores. Se crea la ruta \kode{etc/sysctl/volume-public.service}. En este fichero se configura el servicio \kode{volume-public.service}. Su contenido especifica que comenzará después de que lo haga el servicio Docker, requerido para su funcionamiento. El servicio empezará directamente creando el volumen, añadiendo un enlace simbólico al mismo para todos los usuarios de la máquina. 

\begin{codelisting}
\label{code:volume-public.service}
\codecaption{Fichero \kode{etc/sysctl/volume-public.service}.}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{o}{[}\PY{n+no}{Unit}\PY{o}{]} 
  \PY{n+no}{Description}\PY{o}{=} \PY{n}{volume}\PY{o}{\PYZhy{}}\PY{k+kp}{public} \PY{n}{share} \PY{n}{between} \PY{n}{some}\PY{o}{\PYZhy{}}\PY{n}{postgres}\PY{p}{,} \PY{n}{app}\PY{o}{\PYZhy{}}\PY{n}{job}\PY{p}{,} \PY{n}{app}\PY{o}{\PYZhy{}}\PY{n}{task} \PY{o+ow}{and} 
               \PY{n}{some}\PY{o}{\PYZhy{}}\PY{n}{nginx} 
  \PY{n+no}{After}\PY{o}{=}\PY{n}{docker}\PY{o}{.}\PY{n}{service}
  \PY{n+no}{Requires}\PY{o}{=}\PY{n}{docker}\PY{o}{.}\PY{n}{service}
\PY{o}{[}\PY{n+no}{Service}\PY{o}{]} 
  \PY{n+no}{TimeoutStartSec}\PY{o}{=}\PY{l+m+mi}{0} 
  \PY{n+no}{ExecStart}\PY{o}{=}\PY{l+s+sr}{/}\PY{l+s+sr}{usr}\PY{l+s+sr}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{volume} \PY{n}{create} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n+nb}{name} \PY{n}{volume}\PY{o}{\PYZhy{}}\PY{k+kp}{public}
\PY{o}{[}\PY{n+no}{Install}\PY{o}{]} 
  \PY{n+no}{WantedBy}\PY{o}{=}\PY{n}{multi}\PY{o}{\PYZhy{}}\PY{n}{user}\PY{o}{.}\PY{n}{target}
\end{Verbatim}
\end{codelisting}

\subsubsection{Unidad postgresql.service}

La siguiente es la unidad de servicio correspondiente a la base de datos de la aplicación. Para ello se crea la ruta \kode{etc/sysctl/postgresql.service}. En este fichero se configura el servicio \kode{postgresql.service}. Se especifica que comenzará después del servicio Docker y \kode{volume-public.service}, requeridos para el correcto funcionamiento. Empezará directamente sin tiempo de espera, se reiniciará siempre si ocurre algún problema y se le indica el fichero del que leer las variables de entorno. Si este servicio ya se ha ejecutado se procederá terminándolo y eliminando el contenedor. Seguidamente, se comprobará, si ya existe la imagen del contenedor en la máquina, que sea la última versión disponible, de no ser así se obtendrá de nuevo. Llegado este punto la unidad de servicio comenzará creando el contenedor, especificando la opción a realizar si quiere pararse. Además se añade un enlace simbólico al mismo para todos los usuarios de la máquina. 

\begin{codelisting}
\label{code:postgresql.service}
\codecaption{Fichero \kode{etc/sysctl/postgresql.service}.}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{o}{[}\PY{n+no}{Unit}\PY{o}{]} 
  \PY{n+no}{Description}\PY{o}{=}\PY{n+no}{PostgreSQL} \PY{n}{database} 
  \PY{n+no}{After}\PY{o}{=}\PY{n}{docker}\PY{o}{.}\PY{n}{service} \PY{n}{volume}\PY{o}{\PYZhy{}}\PY{k+kp}{public}\PY{o}{.}\PY{n}{service}
  \PY{n+no}{Requires}\PY{o}{=}\PY{n}{docker}\PY{o}{.}\PY{n}{service} \PY{n}{volume}\PY{o}{\PYZhy{}}\PY{k+kp}{public}\PY{o}{.}\PY{n}{service}
\PY{o}{[}\PY{n+no}{Service}\PY{o}{]} 
  \PY{n+no}{TimeoutStartSec}\PY{o}{=}\PY{l+m+mi}{0}
  \PY{n+no}{Restart}\PY{o}{=}\PY{n}{always}
  \PY{n+no}{EnvironmentFile}\PY{o}{=}\PY{l+s+sr}{/}\PY{l+s+sr}{home}\PY{l+s+sr}{/}\PY{n}{core}\PY{o}{/}\PY{n}{share}\PY{o}{/}\PY{n}{etc}\PY{o}{/}\PY{n}{sysctl}\PY{o}{/}\PY{n}{postgres}\PY{o}{\PYZhy{}}\PY{n}{credentials}\PY{o}{.}\PY{n}{env}
  \PY{n+no}{ExecStartPre}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{o}{/}\PY{n}{usr}\PY{o}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{kill} \PY{n}{some}\PY{o}{\PYZhy{}}\PY{n}{postgres} 
  \PY{n+no}{ExecStartPre}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{o}{/}\PY{n}{usr}\PY{o}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{rm} \PY{n}{some}\PY{o}{\PYZhy{}}\PY{n}{postgres} 
  \PY{n+no}{ExecStartPre}\PY{o}{=}\PY{l+s+sr}{/}\PY{l+s+sr}{usr}\PY{l+s+sr}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{pull} \PY{n}{postgres} 
  \PY{n+no}{ExecStart}\PY{o}{=}\PY{l+s+sr}{/}\PY{l+s+sr}{usr}\PY{l+s+sr}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{run} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n}{rm} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n+nb}{name} \PY{n}{some}\PY{o}{\PYZhy{}}\PY{n}{postgres} \PY{p}{\PYZbs{}}
  \PY{o}{\PYZhy{}}\PY{n}{e} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{POSTGRES\PYZus{}USER=\PYZdl{}\PYZob{}POSTGRES\PYZus{}USER\PYZcb{}}\PY{l+s+s2}{\PYZdq{}} \PY{p}{\PYZbs{}}
  \PY{o}{\PYZhy{}}\PY{n}{e} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{POSTGRES\PYZus{}PASSWORD=\PYZdl{}\PYZob{}POSTGRES\PYZus{}PASSWORD\PYZcb{}}\PY{l+s+s2}{\PYZdq{}} \PY{p}{\PYZbs{}}
  \PY{o}{\PYZhy{}}\PY{n}{v} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{volume\PYZhy{}public:/var/lib/postgresql}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZhy{}}\PY{n+nb}{p} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{5432:5432}\PY{l+s+s2}{\PYZdq{}} \PY{n}{postgres} 
  \PY{n+no}{ExecStop}\PY{o}{=}\PY{l+s+sr}{/}\PY{l+s+sr}{usr}\PY{l+s+sr}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{stop} \PY{n}{some}\PY{o}{\PYZhy{}}\PY{n}{postgres} 
\PY{o}{[}\PY{n+no}{Install}\PY{o}{]} 
  \PY{n+no}{WantedBy}\PY{o}{=}\PY{n}{multi}\PY{o}{\PYZhy{}}\PY{n}{user}\PY{o}{.}\PY{n}{target}
\end{Verbatim}
\end{codelisting}

El contenido del fichero que especifica las variables de entorno hace referencia a las credenciales de la base de datos.

\begin{codelisting}
\label{code:credentials}
\codecaption{Contenido del fichero \kode{/etc/postgres-credentials.env}}
\begin{code}
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
\end{code}
\end{codelisting}

\subsubsection{Unidad app-job.service}

La siguiente unidad de servicio será el contenedor ejecutable de la aplicación que crea, migra y alimenta la base de datos. Para ello se crea la ruta \kode{etc/sysctl/app-job.service}. En este fichero se configura el servicio \kode{app-job.service}. Se especifica que comenzará después de que lo haga el servicio Docker, \kode{volume-public.service} y \kode{postgresql.service}, requeridos para el correcto funcionamiento. Empezará directamente sin tiempo de espera, indicándole el fichero del que leer las variables de entorno necesarias. Si este servicio ya se ha ejecutado se procederá terminándolo y eliminando el contenedor. Seguidamente, se comprobará, si ya existe la imagen del contenedor en la máquina, que sea la última versión disponible, de no ser así se obtendrá de nuevo. La unidad de servicio comenzará creando el contenedor, especificando la opción a realizar si quiere pararse. Además se añade un enlace simbólico al mismo para todos los usuarios de la máquina. 

\begin{codelisting}
\label{code:app-job.service}
\codecaption{Fichero \kode{etc/sysctl/app-job.service}.}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{o}{[}\PY{n+no}{Unit}\PY{o}{]} 
  \PY{n+no}{Description}\PY{o}{=}\PY{n}{executable} \PY{n}{app}\PY{o}{\PYZhy{}}\PY{n}{job} \PY{n}{container} \PY{n}{that} \PY{n}{creates}\PY{p}{,} \PY{n}{migrates}\PY{p}{,} \PY{n}{seeds} \PY{o+ow}{and} 
              \PY{n}{populates} \PY{n}{the} \PY{n}{database}
  \PY{n+no}{After}\PY{o}{=}\PY{n}{docker}\PY{o}{.}\PY{n}{service} \PY{n}{volume}\PY{o}{\PYZhy{}}\PY{k+kp}{public}\PY{o}{.}\PY{n}{service} \PY{n}{postgresql}\PY{o}{.}\PY{n}{service}
  \PY{n+no}{Requires}\PY{o}{=}\PY{n}{docker}\PY{o}{.}\PY{n}{service} \PY{n}{volume}\PY{o}{\PYZhy{}}\PY{k+kp}{public}\PY{o}{.}\PY{n}{service} \PY{n}{postgresql}\PY{o}{.}\PY{n}{service}
\PY{o}{[}\PY{n+no}{Service}\PY{o}{]} 
  \PY{n+no}{TimeoutStartSec}\PY{o}{=}\PY{l+m+mi}{0} 
  \PY{n+no}{EnvironmentFile}\PY{o}{=}\PY{l+s+sr}{/}\PY{l+s+sr}{home}\PY{l+s+sr}{/}\PY{n}{core}\PY{o}{/}\PY{n}{share}\PY{o}{/}\PY{n}{etc}\PY{o}{/}\PY{n}{sysctl}\PY{o}{/}\PY{n}{postgres}\PY{o}{\PYZhy{}}\PY{n}{credentials}\PY{o}{.}\PY{n}{env}
  \PY{n+no}{ExecStartPre}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{o}{/}\PY{n}{usr}\PY{o}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{kill} \PY{n}{app}\PY{o}{\PYZhy{}}\PY{n}{job} 
  \PY{n+no}{ExecStartPre}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{o}{/}\PY{n}{usr}\PY{o}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{rm} \PY{n}{app}\PY{o}{\PYZhy{}}\PY{n}{job} 
  \PY{n+no}{ExecStartPre}\PY{o}{=}\PY{l+s+sr}{/}\PY{l+s+sr}{usr}\PY{l+s+sr}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{pull} \PY{n}{carolina}\PY{o}{/}\PY{l+s+ss}{sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image}\PY{p}{:}\PY{n}{latest} 
  \PY{n+no}{ExecStart}\PY{o}{=}\PY{l+s+sr}{/}\PY{l+s+sr}{usr}\PY{l+s+sr}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{run} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n}{rm} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n+nb}{name} \PY{n}{app}\PY{o}{\PYZhy{}}\PY{n}{job} \PY{p}{\PYZbs{}}
  \PY{o}{\PYZhy{}}\PY{n}{v} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{volume\PYZhy{}public:/usr/src/app/public}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n}{entrypoint} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./setup.sh}\PY{l+s+s2}{\PYZdq{}} \PY{p}{\PYZbs{}}
  \PY{o}{\PYZhy{}}\PY{n}{e} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{POSTGRES\PYZus{}USER=\PYZdl{}\PYZob{}POSTGRES\PYZus{}USER\PYZcb{}}\PY{l+s+s2}{\PYZdq{}} \PY{p}{\PYZbs{}}
  \PY{o}{\PYZhy{}}\PY{n}{e} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{POSTGRES\PYZus{}PASSWORD=\PYZdl{}\PYZob{}POSTGRES\PYZus{}PASSWORD\PYZcb{}}\PY{l+s+s2}{\PYZdq{}} \PY{p}{\PYZbs{}}
  \PY{o}{\PYZhy{}}\PY{n}{w} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/usr/src/app}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n}{link} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{some\PYZhy{}postgres:db}\PY{l+s+s2}{\PYZdq{}} \PY{p}{\PYZbs{}}
  \PY{n}{carolina}\PY{o}{/}\PY{l+s+ss}{sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image}\PY{p}{:}\PY{n}{latest}
\PY{o}{[}\PY{n+no}{Install}\PY{o}{]} 
  \PY{n+no}{WantedBy}\PY{o}{=}\PY{n}{multi}\PY{o}{\PYZhy{}}\PY{n}{user}\PY{o}{.}\PY{n}{target}
\end{Verbatim}
\end{codelisting}

\subsubsection{Unidad app-task.service}

La siguiente unidad de servicio será el contenedor de la aplicación que ejecuta el servidor puma. Se crea la ruta \kode{etc/sysctl/app-task.service}. En este fichero se configura el servicio \kode{app-task.service}. El contenido del mismo especifica que comenzará después de que lo haga el servicio Docker, \kode{volume-public.service}, \kode{postgresql.service} y \kode{app-job.service}, requeridos para el correcto funcionamiento. El servicio empezará  sin tiempo de espera, se reiniciará si falla y se le indica el fichero del que leer las variables de entorno necesarias. Si este servicio ya se ha ejecutado se procederá terminándolo y eliminando el contenedor. Seguidamente, se comprobará, si ya existe la imagen del contenedor en la máquina, que sea la última versión disponible, de no ser así se obtendrá de nuevo. Llegado este punto la unidad de servicio comenzará creando el contenedor, especificando la opción a realizar si quiere pararse. Además se añade un enlace simbólico al mismo para todos los usuarios de la máquina. 

\begin{codelisting}
\label{code:app-task.service}
\codecaption{Fichero \kode{etc/sysctl/app-task.service}.}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{o}{[}\PY{n+no}{Unit}\PY{o}{]} 
  \PY{n+no}{Description}\PY{o}{=}\PY{n}{app}\PY{o}{\PYZhy{}}\PY{n}{task} \PY{n}{container} \PY{n}{that} \PY{n}{runs} \PY{n}{the} \PY{n}{server} \PY{n}{puma}
  \PY{n+no}{After}\PY{o}{=}\PY{n}{docker}\PY{o}{.}\PY{n}{service} \PY{n}{volume}\PY{o}{\PYZhy{}}\PY{k+kp}{public}\PY{o}{.}\PY{n}{service} \PY{n}{postgresql}\PY{o}{.}\PY{n}{service} \PY{n}{app}\PY{o}{\PYZhy{}}\PY{n}{job}\PY{o}{.}\PY{n}{service}
  \PY{n+no}{Requires}\PY{o}{=}\PY{n}{docker}\PY{o}{.}\PY{n}{service} \PY{n}{volume}\PY{o}{\PYZhy{}}\PY{k+kp}{public}\PY{o}{.}\PY{n}{service} \PY{n}{postgresql}\PY{o}{.}\PY{n}{service} \PY{n}{app}\PY{o}{\PYZhy{}}\PY{n}{job}\PY{o}{.}\PY{n}{service}
\PY{o}{[}\PY{n+no}{Service}\PY{o}{]} 
  \PY{n+no}{TimeoutStartSec}\PY{o}{=}\PY{l+m+mi}{0}
  \PY{n+no}{Restart}\PY{o}{=}\PY{n}{always} 
  \PY{n+no}{EnvironmentFile}\PY{o}{=}\PY{l+s+sr}{/}\PY{l+s+sr}{home}\PY{l+s+sr}{/}\PY{n}{core}\PY{o}{/}\PY{n}{share}\PY{o}{/}\PY{n}{etc}\PY{o}{/}\PY{n}{sysctl}\PY{o}{/}\PY{n}{postgres}\PY{o}{\PYZhy{}}\PY{n}{credentials}\PY{o}{.}\PY{n}{env}
  \PY{n+no}{ExecStartPre}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{o}{/}\PY{n}{usr}\PY{o}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{kill} \PY{n}{app}\PY{o}{\PYZhy{}}\PY{n}{task} 
  \PY{n+no}{ExecStartPre}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{o}{/}\PY{n}{usr}\PY{o}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{rm} \PY{n}{app}\PY{o}{\PYZhy{}}\PY{n}{task}
  \PY{n+no}{ExecStartPre}\PY{o}{=}\PY{l+s+sr}{/}\PY{l+s+sr}{usr}\PY{l+s+sr}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{pull} \PY{n}{carolina}\PY{o}{/}\PY{l+s+ss}{sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image}\PY{p}{:}\PY{n}{latest} 
  \PY{n+no}{ExecStart}\PY{o}{=}\PY{l+s+sr}{/}\PY{l+s+sr}{usr}\PY{l+s+sr}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{run} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n}{rm} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n+nb}{name} \PY{n}{app}\PY{o}{\PYZhy{}}\PY{n}{task} \PY{p}{\PYZbs{}}
  \PY{o}{\PYZhy{}}\PY{n}{e} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{POSTGRES\PYZus{}USER=\PYZdl{}\PYZob{}POSTGRES\PYZus{}USER\PYZcb{}}\PY{l+s+s2}{\PYZdq{}} \PY{p}{\PYZbs{}}
  \PY{o}{\PYZhy{}}\PY{n}{e} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{POSTGRES\PYZus{}PASSWORD=\PYZdl{}\PYZob{}POSTGRES\PYZus{}PASSWORD\PYZcb{}}\PY{l+s+s2}{\PYZdq{}} \PY{p}{\PYZbs{}}
  \PY{o}{\PYZhy{}}\PY{n}{w} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/usr/src/app}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZhy{}}\PY{n}{v} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{volume\PYZhy{}public:/usr/src/app/public}\PY{l+s+s2}{\PYZdq{}} \PY{p}{\PYZbs{}}
  \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n}{link} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{some\PYZhy{}postgres:db}\PY{l+s+s2}{\PYZdq{}} \PY{n}{carolina}\PY{o}{/}\PY{l+s+ss}{sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image}\PY{p}{:}\PY{n}{latest} \PY{p}{\PYZbs{}}
  \PY{l+s+sr}{/}\PY{l+s+sr}{bin}\PY{l+s+sr}{/}\PY{n}{bash} \PY{o}{\PYZhy{}}\PY{n}{c} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cp config/database.yml.postgresql config/database.yml \PYZam{}\PYZam{} }\PY{l+s+s2}{\PYZbs{}}
\PY{l+s+s2}{  cp ./.secret.example ./.secret \PYZam{}\PYZam{} puma \PYZhy{}p 9292}\PY{l+s+s2}{\PYZdq{}}
  \PY{n+no}{ExecStop}\PY{o}{=}\PY{l+s+sr}{/}\PY{l+s+sr}{usr}\PY{l+s+sr}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{stop} \PY{n}{app}\PY{o}{\PYZhy{}}\PY{n}{task}
\PY{o}{[}\PY{n+no}{Install}\PY{o}{]} 
  \PY{n+no}{WantedBy}\PY{o}{=}\PY{n}{multi}\PY{o}{\PYZhy{}}\PY{n}{user}\PY{o}{.}\PY{n}{target}
\end{Verbatim}
\end{codelisting}

\subsubsection{Unidad nginx.service}

La siguiente unidad del servicio será el contenedor que ejecuta el servidor Nginx. Para ello se crea la ruta \kode{etc/sysctl/nginx.service}. En este fichero se configura el servicio \kode{nginx.service}. El contenido del mismo especifica que comenzará después de que lo haga el servicio Docker, \kode{volume-public.service}, \kode{postgresql.service}, \kode{app-job.service} y \kode{app-task.service}, requeridos para el correcto funcionamiento. El servicio empezará directamente sin tiempo de espera y se reiniciará si falla. Si ya se ha ejecutado se procederá terminándolo y eliminando el contenedor. Seguidamente, se comprobará, si ya existe la imagen del contenedor en la máquina, que sea la última versión disponible, de no ser así se obtendrá de nuevo. La unidad de servicio comenzará creando el contenedor, teniendo en cuenta que ahora la redirección de puertos se ha establecido para trabajar el puerto 80 de la máquina CoreOS con el puerto 80 de la aplicación, especificando la opción a realizar si dicho contenedor quiere pararse. Además se añade un enlace simbólico al mismo para todos los usuarios de la máquina. 

\begin{codelisting}
\label{code:nginx.service}
\codecaption{Fichero \kode{etc/sysctl/nginx.service}.}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{o}{[}\PY{n+no}{Unit}\PY{o}{]} 
  \PY{n+no}{Description}\PY{o}{=}\PY{n}{some}\PY{o}{\PYZhy{}}\PY{n}{nginx} \PY{n}{container} \PY{n}{that} \PY{n}{runs} \PY{n}{a} \PY{n}{reverse} \PY{n}{proxy} \PY{n}{server} \PY{o+ow}{and} \PY{n}{a} 
              \PY{n}{web} \PY{n}{server}
  \PY{n+no}{After}\PY{o}{=}\PY{n}{docker}\PY{o}{.}\PY{n}{service} \PY{n}{volume}\PY{o}{\PYZhy{}}\PY{k+kp}{public}\PY{o}{.}\PY{n}{service} \PY{n}{postgresql}\PY{o}{.}\PY{n}{service} \PY{n}{app}\PY{o}{\PYZhy{}}\PY{n}{job}\PY{o}{.}\PY{n}{service} 
        \PY{n}{app}\PY{o}{\PYZhy{}}\PY{n}{task}\PY{o}{.}\PY{n}{service}
  \PY{n+no}{Requires}\PY{o}{=}\PY{n}{docker}\PY{o}{.}\PY{n}{service} \PY{n}{volume}\PY{o}{\PYZhy{}}\PY{k+kp}{public}\PY{o}{.}\PY{n}{service} \PY{n}{postgresql}\PY{o}{.}\PY{n}{service} 
           \PY{n}{app}\PY{o}{\PYZhy{}}\PY{n}{job}\PY{o}{.}\PY{n}{service} \PY{n}{app}\PY{o}{\PYZhy{}}\PY{n}{task}\PY{o}{.}\PY{n}{service}
\PY{o}{[}\PY{n+no}{Service}\PY{o}{]} 
  \PY{n+no}{TimeoutStartSec}\PY{o}{=}\PY{l+m+mi}{0}
  \PY{n+no}{Restart}\PY{o}{=}\PY{n}{always} 
  \PY{n+no}{ExecStartPre}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{o}{/}\PY{n}{usr}\PY{o}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{kill} \PY{n}{some}\PY{o}{\PYZhy{}}\PY{n}{nginx} 
  \PY{n+no}{ExecStartPre}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{o}{/}\PY{n}{usr}\PY{o}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{rm} \PY{n}{some}\PY{o}{\PYZhy{}}\PY{n}{nginx}
  \PY{n+no}{ExecStartPre}\PY{o}{=}\PY{l+s+sr}{/}\PY{l+s+sr}{usr}\PY{l+s+sr}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{pull} \PY{n}{nginx} 
  \PY{n+no}{ExecStart}\PY{o}{=}\PY{l+s+sr}{/}\PY{l+s+sr}{usr}\PY{l+s+sr}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{run} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n}{rm} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n+nb}{name} \PY{n}{some}\PY{o}{\PYZhy{}}\PY{n}{nginx} \PY{p}{\PYZbs{}}
  \PY{o}{\PYZhy{}}\PY{n}{v} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/home/core/share/etc/sysctl/nginx.conf:/etc/nginx/conf.d/default.conf}\PY{l+s+s2}{\PYZdq{}} \PY{p}{\PYZbs{}}
  \PY{o}{\PYZhy{}}\PY{n+nb}{p} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{80:80}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n}{link} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{app\PYZhy{}task:app}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZhy{}}\PY{n}{v} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{volume\PYZhy{}public:/usr/src/app/public}\PY{l+s+s2}{\PYZdq{}} \PY{n}{nginx} 
  \PY{n+no}{ExecStop}\PY{o}{=}\PY{l+s+sr}{/}\PY{l+s+sr}{usr}\PY{l+s+sr}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{docker} \PY{n}{stop} \PY{n}{some}\PY{o}{\PYZhy{}}\PY{n}{nginx}
\PY{o}{[}\PY{n+no}{Install}\PY{o}{]} 
  \PY{n+no}{WantedBy}\PY{o}{=}\PY{n}{multi}\PY{o}{\PYZhy{}}\PY{n}{user}\PY{o}{.}\PY{n}{target}
\end{Verbatim}
\end{codelisting}

\subsubsection{Despliegue manual}

Una vez que ya se han configurado y escrito las 5 unidades de servicio necesarias el siguiente paso es preparar su despliegue. Para el reconocimiento de las unidades éstas tienen que ubicarse bajo \kode{systemd}, por lo que se copian al directorio \kode{/etc/systemd/system/}. El funcionamiento pasa por la habilitación del servicio, creando el enlace simbólico de la unidad para todos los usuarios, y el comienzo del mismo.

Se crea un \textit{script} denominado \kode{coreos-service-units-deploy.sh}, con permisos \kode{chmod +x}, que se encargará de todo lo mencionado.

\begin{codelisting}
\label{code:coreos-service-units-deploy}
\codecaption{Fichero \kode{coreos-service-units-deploy.sh}.}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{c}{\PYZsh{}!/bin/bash}

sudo cp share/etc/sysctl/* /etc/systemd/system/
sudo systemctl \PY{n+nb}{enable }volume\PYZhy{}public.service
sudo systemctl \PY{n+nb}{enable }postgresql.service
sudo systemctl \PY{n+nb}{enable }app\PYZhy{}job.service
sudo systemctl \PY{n+nb}{enable }app\PYZhy{}task.service
sudo systemctl \PY{n+nb}{enable }nginx.service
sudo systemctl start volume\PYZhy{}public.service
sudo systemctl start postgresql.service
sudo systemctl start app\PYZhy{}job.service
sudo systemctl start app\PYZhy{}task.service
sudo systemctl start nginx.service
\end{Verbatim}
\end{codelisting}

Con la intención de que se ejecute al iniciar la máquina CoreOS se añade al fichero \kode{Vagrantfile} una línea que indicará la ruta del mismo para provisionar la máquina con las directivas incluidas en él.

\begin{codelisting}
\label{code:vagrantfile3}
\codecaption{Fichero \kode{Vagrantfile}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{o}{.}
\PY{n}{config}\PY{o}{.}\PY{n}{vm}\PY{o}{.}\PY{n}{provision} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{shell}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+ss}{path}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{coreos\PYZhy{}service\PYZhy{}units\PYZhy{}deploy.sh}\PY{l+s+s2}{\PYZdq{}}
\PY{o}{.}
\end{Verbatim}
\end{codelisting}

Por último se inicia la máquina y se accede a ella: 

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }vagrant up \PY{o}{\PYZam{}\PYZam{}} vagrant ssh core\PYZhy{}01
\end{Verbatim}
\end{framed_shaded}

Si se inicia el despliegue manual se comprueba el correcto funcionamiento de la aplicación, donde todos los servicios estarán en estado activo o cargado:

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{images/figures/volume-public.service.png}
\caption{Estado de la unidad \kode{volume-public.service}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/figures/postgresql.service.png}
\caption{Estado de la unidad \kode{postgresql.service}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/figures/app-job.service.png}
\caption{Estado de la unidad \kode{app-job.service}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/figures/app-task.service.png}
\caption{Estado de la unidad \kode{app-task.service}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/figures/nginx.service.png}
\caption{Estado de la unidad \kode{nginx.service}.}
\end{figure}

Finalmente, se accede a la aplicación desde la máquina CoreOS y desde el sistema anfitrión:

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/coreosmanualcurl.png}
\caption{Acceso a la aplicación desde la máquina CoreOS.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/coreosmanualhost.png}
\caption{Acceso a la aplicación desde el sistema anfitrión.}
\end{figure}

\subsection{Despliegue automático de las unidades systemd}

Para realizar el despliegue de manera automática se hace uso del fichero \kode{cloud-config} denominado \kode{user-data.sampleapp.vbox}. Este fichero especifica el orden de las unidades a desplegar y las acciones de habilitación y comienzo de los servicios.

Esta vez no se usará el almacenamiento NFS compartido por lo que ya no es necesaria la línea de provisión del \textit{script} en el fichero \kode{Vagranfile}. Además, puede volver a comentarse la línea que habilitaba este tipo de almacenamiento. De esta manera, también será necesario definir las variables de entorno, indicando la ruta, sus permisos y contenido. Lo mismo se realiza para el fichero que contiene la configuración Nginx.

Así, los añadidos al fichero \kode{user-data.sampleapp.vbox} son:

\begin{codelisting}
\label{code:vagrantfile2}
\codecaption{Fichero \kode{user-data.sampleapp.vbox}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}cloud\PYZhy{}config}
\PY{n+nn}{\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+lScalar+lScalarPlain}{write\PYZus{}files}\PY{p+pIndicator}{:}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{path}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{/etc/postgres\PYZhy{}credentials.env}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{permissions}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{0644}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{POSTGRES\PYZus{}USER=postgres}
      \PY{n+no}{POSTGRES\PYZus{}PASSWORD=postgres}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{path}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{/etc/nginx.conf}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{permissions}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{0644}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{server \PYZob{}}
        \PY{n+no}{listen 80;}
        \PY{n+no}{root /usr/src/app/public;}
        \PY{n+no}{location / \PYZob{}}
          \PY{n+no}{proxy\PYZus{}set\PYZus{}header X\PYZhy{}Forwarded\PYZhy{}For \PYZdl{}proxy\PYZus{}add\PYZus{}x\PYZus{}forwarded\PYZus{}for;}
          \PY{n+no}{proxy\PYZus{}set\PYZus{}header Host \PYZdl{}http\PYZus{}host;}
          \PY{n+no}{proxy\PYZus{}redirect off;}
          \PY{n+no}{try\PYZus{}files \PYZdl{}uri /page\PYZus{}cache/\PYZdl{}uri /page\PYZus{}cache/\PYZdl{}uri.html @app;}
        \PY{n+no}{\PYZcb{}}
        \PY{n+no}{location @app\PYZob{}}
          \PY{n+no}{proxy\PYZus{}pass http://app:9292;}
          \PY{n+no}{break;}
        \PY{n+no}{\PYZcb{}}
      \PY{n+no}{\PYZcb{}}
\PY{l+lScalar+lScalarPlain}{coreos}\PY{p+pIndicator}{:}
  \PY{l+lScalar+lScalarPlain}{.}
  \PY{l+lScalar+lScalarPlain}{units}\PY{p+pIndicator}{:}
  \PY{l+lScalar+lScalarPlain}{.}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{volume\PYZhy{}public.service}
    \PY{l+lScalar+lScalarPlain}{command}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{start}
    \PY{l+lScalar+lScalarPlain}{enable}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{true}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit] }
      \PY{n+no}{Description= volume\PYZhy{}public share between some\PYZhy{}postgres, app\PYZhy{}job, app\PYZhy{}task}
                   \PY{n+no}{and some\PYZhy{}nginx }
      \PY{n+no}{After=docker.service}
      \PY{n+no}{Requires=docker.service}
      \PY{n+no}{[Service] }
      \PY{n+no}{TimeoutStartSec=0 }
      \PY{n+no}{ExecStart=/usr/bin/docker volume create \PYZhy{}\PYZhy{}name volume\PYZhy{}public}
      \PY{n+no}{[Install] }
      \PY{n+no}{WantedBy=multi\PYZhy{}user.target}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{postgresql.service}
    \PY{l+lScalar+lScalarPlain}{command}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{start}
    \PY{l+lScalar+lScalarPlain}{enable}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{true}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit] }
      \PY{n+no}{Description=PostgreSQL database }
      \PY{n+no}{After=docker.service volume\PYZhy{}public.service}
      \PY{n+no}{Requires=docker.service volume\PYZhy{}public.service}
      \PY{n+no}{[Service] }
      \PY{n+no}{TimeoutStartSec=0}
      \PY{n+no}{Restart=always}
      \PY{n+no}{EnvironmentFile=/etc/postgres\PYZhy{}credentials.env}
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker kill some\PYZhy{}postgres }
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker rm some\PYZhy{}postgres }
      \PY{n+no}{ExecStartPre=/usr/bin/docker pull postgres }
      \PY{n+no}{ExecStart=/usr/bin/docker run \PYZhy{}\PYZhy{}rm \PYZhy{}\PYZhy{}name some\PYZhy{}postgres \PYZbs{}}
      \PY{n+no}{\PYZhy{}e \PYZdq{}POSTGRES\PYZus{}USER=\PYZdl{}\PYZob{}POSTGRES\PYZus{}USER\PYZcb{}\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}e \PYZdq{}POSTGRES\PYZus{}PASSWORD=\PYZdl{}\PYZob{}POSTGRES\PYZus{}PASSWORD\PYZcb{}\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}v \PYZdq{}volume\PYZhy{}public:/var/lib/postgresql\PYZdq{} \PYZhy{}p \PYZdq{}5432:5432\PYZdq{} postgres }
      \PY{n+no}{ExecStop=/usr/bin/docker stop some\PYZhy{}postgres}
      \PY{n+no}{[Install] }
      \PY{n+no}{WantedBy=multi\PYZhy{}user.target}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{app\PYZhy{}job.service}
    \PY{l+lScalar+lScalarPlain}{command}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{start}
    \PY{l+lScalar+lScalarPlain}{enable}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{true}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit] }
      \PY{n+no}{Description=executable app\PYZhy{}job container that creates, migrates, seeds}
                  \PY{n+no}{and populates the database}
      \PY{n+no}{After=docker.service volume\PYZhy{}public.service postgresql.service}
      \PY{n+no}{Requires=docker.service volume\PYZhy{}public.service postgresql.service}
      \PY{n+no}{[Service] }
      \PY{n+no}{TimeoutStartSec=0 }
      \PY{n+no}{EnvironmentFile=/etc/postgres\PYZhy{}credentials.env}
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker kill app\PYZhy{}job }
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker rm app\PYZhy{}job }
      \PY{n+no}{ExecStartPre=/usr/bin/docker pull carolina/sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image:latest }
      \PY{n+no}{ExecStart=/usr/bin/docker run \PYZhy{}\PYZhy{}rm \PYZhy{}\PYZhy{}name app\PYZhy{}job \PYZbs{}}
                \PY{n+no}{\PYZhy{}v \PYZdq{}volume\PYZhy{}public:/usr/src/app/public\PYZdq{} \PYZhy{}\PYZhy{}entrypoint \PYZdq{}./setup.sh\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}e \PYZdq{}POSTGRES\PYZus{}USER=\PYZdl{}\PYZob{}POSTGRES\PYZus{}USER\PYZcb{}\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}e \PYZdq{}POSTGRES\PYZus{}PASSWORD=\PYZdl{}\PYZob{}POSTGRES\PYZus{}PASSWORD\PYZcb{}\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}w \PYZdq{}/usr/src/app\PYZdq{} \PYZhy{}\PYZhy{}link \PYZdq{}some\PYZhy{}postgres:db\PYZdq{} \PYZbs{}}
      \PY{n+no}{carolina/sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image:latest}
      \PY{n+no}{[Install] }
      \PY{n+no}{WantedBy=multi\PYZhy{}user.target}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{app\PYZhy{}task.service}
    \PY{l+lScalar+lScalarPlain}{command}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{start}
    \PY{l+lScalar+lScalarPlain}{enable}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{true}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit] }
      \PY{n+no}{Description=app\PYZhy{}task container that runs the server puma}
      \PY{n+no}{After=docker.service volume\PYZhy{}public.service postgresql.service }
            \PY{n+no}{app\PYZhy{}job.service}
      \PY{n+no}{Requires=docker.service volume\PYZhy{}public.service postgresql.service }
               \PY{n+no}{app\PYZhy{}job.service}
      \PY{n+no}{[Service] }
      \PY{n+no}{TimeoutStartSec=0}
      \PY{n+no}{Restart=always }
      \PY{n+no}{EnvironmentFile=/etc/postgres\PYZhy{}credentials.env}
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker kill app\PYZhy{}task }
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker rm app\PYZhy{}task}
      \PY{n+no}{ExecStartPre=/usr/bin/docker pull carolina/sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image:latest }
      \PY{n+no}{ExecStart=/usr/bin/docker run \PYZhy{}\PYZhy{}rm \PYZhy{}\PYZhy{}name app\PYZhy{}task \PYZbs{}}
      \PY{n+no}{\PYZhy{}e \PYZdq{}POSTGRES\PYZus{}USER=\PYZdl{}\PYZob{}POSTGRES\PYZus{}USER\PYZcb{}\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}e \PYZdq{}POSTGRES\PYZus{}PASSWORD=\PYZdl{}\PYZob{}POSTGRES\PYZus{}PASSWORD\PYZcb{}\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}w \PYZdq{}/usr/src/app\PYZdq{} \PYZhy{}v \PYZdq{}volume\PYZhy{}public:/usr/src/app/public\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}\PYZhy{}link \PYZdq{}some\PYZhy{}postgres:db\PYZdq{} carolina/sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image:latest \PYZbs{}}
      \PY{n+no}{/bin/bash \PYZhy{}c \PYZdq{}cp config/database.yml.postgresql \PYZbs{}}
      \PY{n+no}{config/database.yml \PYZam{}\PYZam{} cp ./.secret.example ./.secret \PYZam{}\PYZam{} puma \PYZhy{}p 9292\PYZdq{}}
      \PY{n+no}{ExecStop=/usr/bin/docker stop app\PYZhy{}task}
      \PY{n+no}{[Install] }
      \PY{n+no}{WantedBy=multi\PYZhy{}user.target}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{nginx.service}
    \PY{l+lScalar+lScalarPlain}{command}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{start}
    \PY{l+lScalar+lScalarPlain}{enable}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{true}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit] }
      \PY{n+no}{Description=some\PYZhy{}nginx container that runs a reverse proxy server and }
                  \PY{n+no}{a web server}
      \PY{n+no}{After=docker.service volume\PYZhy{}public.service postgresql.service }
            \PY{n+no}{app\PYZhy{}job.service app\PYZhy{}task.service}
      \PY{n+no}{Requires=docker.service volume\PYZhy{}public.service postgresql.service }
               \PY{n+no}{app\PYZhy{}job.service app\PYZhy{}task.service}
      \PY{n+no}{[Service] }
      \PY{n+no}{TimeoutStartSec=0}
      \PY{n+no}{Restart=always }
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker kill some\PYZhy{}nginx }
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker rm some\PYZhy{}nginx}
      \PY{n+no}{ExecStartPre=/usr/bin/docker pull nginx }
      \PY{n+no}{ExecStart=/usr/bin/docker run \PYZhy{}\PYZhy{}rm \PYZhy{}\PYZhy{}name some\PYZhy{}nginx \PYZbs{}}
      \PY{n+no}{\PYZhy{}v \PYZdq{}/etc/nginx.conf:/etc/nginx/conf.d/default.conf\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}p \PYZdq{}80:80\PYZdq{} \PYZhy{}\PYZhy{}link \PYZdq{}app\PYZhy{}task:app\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}v \PYZdq{}volume\PYZhy{}public:/usr/src/app/public\PYZdq{} nginx }
      \PY{n+no}{ExecStop=/usr/bin/docker stop some\PYZhy{}nginx}
      \PY{n+no}{[Install] }
      \PY{n+no}{WantedBy=multi\PYZhy{}user.target}
\end{Verbatim}
\end{codelisting}

El fichero es validado con la utilidad \textit{Config Validator}. 

Con el despliegue se comprueba el correcto funcionamiento de la aplicación:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }vagrant up \PY{o}{\PYZam{}\PYZam{}} vagrant ssh core\PYZhy{}01
\end{Verbatim}
\end{framed_shaded}

\subsection{Resultado}
Tanto si se decide realizar el despliegue manual o automático con unidades usando CoreOS se obtiene el correcto funcionamiento de la aplicación a través de cada servicio independiente.
El resultado en ambos casos ha sido positivo:

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/coreosmanualcurl.png}
\caption{Despliegue manual y automático. Acceso: máquina CoreOS.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/coreosmanualhost.png}
\caption{Despliegue manual y automático. Acceso: sistema anfitrión.}
\end{figure}

\section[Iteración 5: Despliegue en Amazon Web Services]{Iteración 5: Despliegue en la nube pública de Amazon Web Services}

En esta iteración se pretende desplegar la infraestructura en la nube pública de Amazon Web Services. Para ello se trabajará el mismo fichero \kode{config.rb}, el nuevo fichero \kode{user-data.sampleapp.aws} y el fichero \kode{Vagrantfile} en el que se introducirá el tratamiento e integración de este proveedor.

En la \hyperref[fig:aws-1-iteration]{Figura \ref{fig:aws-1-iteration}} puede distinguirse la distribución de las unidades de servicio, y por tanto, de los contenedores. El proxy, presente en el contenedor \kode{some-nginx}, que redirige al servidor web de la aplicación, en la máquina \kode{core-01}. Nginx será el servicio al cual los clientes y usuarios finales realizan las peticiones para acceder a la aplicación. No solo se va a tener un servidor de la aplicación, éste puede estar replicado desde en una a todas las instancias de la infraestructura y el servicio Nginx deberá balancear la carga entre ellos. Para controlar los servidores web que se registran y/o dejan de estar en el sistema se implementará un nuevo contenenedor, \kode{confd}, y un nuevo volumen Docker, \kode{conf-data}. A su vez, el contenedor \kode{some-postgres} para la base de datos se ubicará en la máquina \kode{core-02}. A él tiene que acceder tanto el contenedor \kode{app-job}, que la crea, migra y alimenta, como el contenedor \kode{app-task} que se dirige al mismo para obtener y escribir la información solicitada y añadida en la aplicación web. Ello se consigue con la implementación del contenedor \kode{skydns} que les servirá como servidor DNS, trabajando con las claves y valores de etc.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/aws-1-iteration.png}
\caption{Distribución de la infraestructura a desplegar en AWS. \label{fig:aws-1-iteration}}
\label{fig:aws-1-iteration}
\end{figure}

La relación entre los contenedores, independientemente de su ubicación y replicación en otras instancias, en la manera en la que se ha comentado, puede apreciarse con mayor detalle en la \hyperref[fig:aws-2-iteration]{Figura \ref{fig:aws-2-iteration}}.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/figures/aws-2-iteration.png}
\caption{Relación entre los contenedores de la Infraestructura. \label{fig:aws-2-iteration}}
\end{figure}

Cuando la infraestructura se despliega, el contenedor \kode{some-postgres} se registra en \kode{skydns}, que utiliza etc para almacenar las claves y valores. De esta manera, \kode{some-postgres} puede ser descubierto por los contenedores \kode{app-job}, donde el primero en iniciarse creará, migrará y alimentará la base de datos. Cuando arrancan los contenedores \kode{app-task}, servidores web de la aplicación, se registran en etc. El contenedor \kode{confd} vigila estos cambios cada 5 segundos para actualizar la configuración de \kode{some-nginx} y éste pueda resolver hacia ellos. Puesto que se comparte un fichero entre dos contenedores se utiliza el volumen \kode{conf-data}. Así, cuando un cliente realiza una petición a través de Internet a \kode{some-nginx}, éste puede redirigirle a uno de los servidores \kode{app-task}, el cuál consultará \kode{skydns} para poder obtener y escribir información en la base de datos. La política para redireccionar a los servidores web será \textit{Round Robin}, del primero al último.

Al ya no estar los contenedores en la misma instancia, compartir el volumen Docker \kode{volume-public} de la manera en la que se ha ido haciendo no es posible. Como no es el propósito directo ya no será implementado y se incorporará a la sección de Trabajos Futuros \hyperref[trabajosfuturos]{\ref{trabajosfuturos}}.

El servicio quedará disponible mediante el despliegue de la infraestructura desde Vagrant, estableciendo antes las variables de entorno correspondientes al proveedor. El acceso al servicio, que puede ser visto en la \hyperref[curl-confd]{Figura \ref{curl-confd}}, se hará desde la máquina \kode{core-01}, que contiene el proxy.

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }. \PYZti{}/.aws\PYZhy{}credentials/aws\PYZhy{}credentials
\PY{n+nv}{\PYZdl{} }vagrant up \PYZhy{}\PYZhy{}provider\PY{o}{=}aws \PY{o}{\PYZam{}\PYZam{}} vagrant ssh core\PYZhy{}01
\PY{c}{\PYZsh{} curl http://localhost:80}
\end{Verbatim}
\end{framed_shaded}

\subsection{Obtención de la cuenta AWS}

Para la realización de esta iteración es necesario crear una cuenta en Amazon Web Services. En el caso presente se utilizará una cuenta que posee una capa gratuita, con una duración de un año, que se usa en conjunto con un crédito de \$50 dólares. Este crédito ha sido facilitado al pertenecer al programa \textit{AWS Educate}\cite{AWS Educate}. Se trata de una iniciativa de Amazon que proporciona a los estudiantes y personal docente los recursos necesarios para acelerar el aprendizaje relacionado con la nube.

Los servicios de AWS se localizan en múltiples ubicaciones por todo el mundo. En este trabajo se escogerá la región \textit{US East} y zona \textit{N. Virginia}.

Se hace uso del servicio AWS \textit{Identity and Access Management} (IAM) con la intención de controlar de forma segura el acceso a servicios y recursos. Con el objetivo de tener un usuario para realizar las actividades de desarrollo, el primer paso será la creación del grupo \textit{deployers} con permiso para manejar las instancias EC2. Luego, se crea el usuario \textit{deployer} con tipo de acceso \textit{Management Console access}. Al mismo se le facilita una contraseña y se le indica que pertenece al grupo recién creado.

\subsection{Creación de la red y subred virtual}

El servicio \textit{Virtual Private Cloud} (VPC) permite controlar todos los aspectos del entorno de red virtual. Será usado para hacer la selección del rango de direcciones IP que se les dará a las máquinas virtuales y para crear una subred acorde.

En primer lugar se crea una VPC llamada \textit{vpc-sampleapp} con el rango de direcciones IPv4 10.0.0.0/16:

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/vpc-sampleapp.png}
\caption{Creación de la VPC \textit{vpc-sampleapp}.}
\end{figure}

En segundo lugar se crea una subred llamada \textit{subnet-sampleapp}, perteneciente a la anterior VPC, con el rango de direcciones IPv4 10.0.0.0/24:

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/figures/subnet-sampleapp.png}
\caption{Creación de la subred \textit{subnet-sampleapp}.}
\end{figure}

\subsection{Creación del par de claves y del grupo de seguridad}

Lo siguiente es preparar una pareja de claves \textit{key pair} para poder entrar por SSH en la instancia. Para ello es necesario crear un \textit{Security Group} que defina los puertos de entrada por los que podrá recibir peticiones.

Lo primero se realiza accediendo al servicio Amazon Elastic Cloud Computing (Amazon EC2) y navegando hasta \textit{Network \& Security, Key Pairs}. Aquí, se crea una nueva pareja de claves con el nombre \kode{keypair.pem}, la cual se guarda en un directorio oculto de nombre \kode{\textasciitilde{}/.aws-credentials}, facilitando permisos de lectura y escritura al propietario.

Ahora se navega hasta \textit{Network \& Security, Security Groups} y se crea el grupo de seguridad \textit{deploying-security-group} con las reglas TCP entrantes que permitan dejar pasar a la instancia las peticiones que llegan para conexiones a los puertos asignados a los protocolos SSH (22) y HTTP (80). También se añaden las reglas TCP a los puertos que utiliza CoreOS en sus comunicaciones con los clientes, siendo 2379, 2380, 4001 y 7001. Para que las distintas máquinas virtuales que se creen, formando un clúster CoreOS, tengan conectividad con los contenedores de otras máquinas virtuales miembro se habilita el puerto 8285 mediante una regla UDP. Finalmente, se añaden las reglas TCP sobre los puertos que afectan a la aplicación, 9292 para el servidor puma y 5432 para el servidor PostgreSQL.

\subsection{Configuración del fichero user-data}

Para llevar a cabo esta parte se necesita una nueva configuración \kode{cloud-config}. Este nuevo \kode{user-data} recibirá el nombre de \kode{user-data.sampleapp.aws}. Tendrá, únicamente, dos diferencias respecto al anterior, por ello se copia:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }cp user\PYZhy{}data.sampleapp.vbox user\PYZhy{}data.sampleapp.aws
\end{Verbatim}
\end{framed_shaded}

La primera de las diferencias será especificar al servicio flannel que ha de usar el rango de direcciones IPv4 privadas, pues los contenedores internos a la máquina usarán una red privada para no ser accesibles desde el exterior. El segundo cambio detalla que la red privada a usar será la 172.17.0.0/16 y el protocolo UDP.

\begin{codelisting}
\label{code:vagrantfile2}
\codecaption{Fichero \kode{user-data.sampleapp.aws}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{l+lScalar+lScalarPlain}{.}
\PY{l+lScalar+lScalarPlain}{coreos}\PY{p+pIndicator}{:}
  \PY{l+lScalar+lScalarPlain}{.}
  \PY{l+lScalar+lScalarPlain}{flannel}\PY{p+pIndicator}{:}
    \PY{l+lScalar+lScalarPlain}{interface}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{\PYZdl{}private\PYZus{}ipv4}\PY{l+s}{\PYZdq{}}
  \PY{l+lScalar+lScalarPlain}{units}\PY{p+pIndicator}{:}
  \PY{l+lScalar+lScalarPlain}{.}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{flanneld.service}
    \PY{l+lScalar+lScalarPlain}{drop\PYZhy{}ins}\PY{p+pIndicator}{:}
    \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{50\PYZhy{}network\PYZhy{}config.conf}
      \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
        \PY{n+no}{[Service]}
        \PY{n+no}{ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config }
               \PY{n+no}{\PYZsq{}\PYZob{} \PYZdq{}Network\PYZdq{}: \PYZdq{}172.17.0.0/16\PYZdq{}, \PYZdq{}Backend\PYZdq{}: \PYZob{} \PYZdq{}Type\PYZdq{}: \PYZdq{}udp\PYZdq{} \PYZcb{} \PYZcb{}\PYZsq{}}
\PY{l+lScalar+lScalarPlain}{.}
\end{Verbatim}
\end{codelisting}

\subsection{Configuración del fichero Vagrantfile}

Primero se instala el plugin necesario para utilizar AWS como proveedor:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }vagrant plugin install vagrant\PYZhy{}aws
\end{Verbatim}
\end{framed_shaded}

Luego se añade al fichero \kode{Vagrantfile} la comprobación de que se encuentre instalado en el sistema anfitrión.

Para iniciar sesión en la cuenta AWS como usuario \textit{deployer} se agregará un fichero local en el que especificar estos datos, con la intención de ser tratados como variables de entorno. Para ello se crea el fichero \kode{\textasciitilde{}/.aws-credentials/aws-credentials} con permiso de ejecución \textit{chmod +x} y contenido:

\begin{codelisting}
\label{code:vagrantfile2}
\codecaption{Fichero \kode{\textasciitilde{}/.aws-credentials/aws-credentials}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nb}{export }\PY{n+nv}{AWS\PYZus{}KEY}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}XXXXXXXXXXXXXXXXXXX\PYZsq{}}
\PY{n+nb}{export }\PY{n+nv}{AWS\PYZus{}SECRET}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}XXXXXXXXXXXXXXXXXXXXXXXXXXX\PYZsq{}}
\PY{n+nb}{export }\PY{n+nv}{AWS\PYZus{}KEYNAME}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}keypair\PYZsq{}}
\PY{n+nb}{export }\PY{n+nv}{AWS\PYZus{}KEYPATH}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}\PYZti{}/.aws\PYZhy{}credentials/keypair.pem\PYZsq{}}
\end{Verbatim}
\end{codelisting}

Así, en el propio \kode{Vagrantfile} se especificará que las variables de entorno \kode{ENV['AWS\_KEY']}, \kode{ENV['AWS\_SECRET']} y \kode{ENV['AWS\_KEYNAME']} deben establecerse para continuar. Para ello se ejecuta en el directorio actual:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }. \PYZti{}/.aws\PYZhy{}credentials/aws\PYZhy{}credentials
\end{Verbatim}
\end{framed_shaded}

Como en el caso anterior, se prepara la copia del fichero \kode{cloud-config} de \kode{user-data.sampleapp.aws} a \kode{user-data}.

Las opciones necesarias para el despliegue desde Vagrant con este proveedor determinan la configuración de la instancia o instancias a lanzar. Las características a definir son:
\begin{itemize}
\item \textit{access\_key\_id}: Primera clave, perteneciente al identificador del usuario.
\item \textit{secret\_access\_key}: Segunda clave, perteneciente a la contraseña secreta.
\item \textit{keypair\_name}: Nombre del archivo que contiene el par de claves.
\item \textit{aws\_region}: Región.
\item \textit{aws\_availability\_zone}: Zona de disponibilidad.
\item \textit{aws\_subnet\_id}: Identificador de la subred.
\item \textit{aws\_security\_groups}: Identificador del grupo de seguridad.
\item \textit{aws\_ami}: Identificador de la \textit{Amazon Machine Image} (AMI). Se escoge \textit{CoreOS-alpha-1339.0.0-hvm - ami-00598116}, de 64 bits con virtualización HVM.
\item \textit{aws\_instance\_type}: Tipo de instancia a lanzar. El tipo será \textit{t2.micro Free tier eligible}.
\item \textit{aws\_elastic\_ip}: Uso de direcciones IP elásticas para el acceso remoto al servicio.
\item \textit{private\_ip\_adresses}: Dirección IP privada a asignar a la instancia.
\item \textit{user\_data}: Contenido del fichero \kode{user-data}.
\end{itemize}

Los valores que tomarán estos parámetros son los relativos a las diferentes creaciones que se han ido realizando con la cuenta en uso.

Otro aspecto a indicar es la deshabilitación de la carpeta compartida por defecto, \kode{/vagrant}, con el directorio actual del sistema anfitrión, pues es prescindible. También será necesario indicar la \textit{url} que utilizará Vagrant como \textit{box}, imagen para la máquina virtual. Dicha indicación ha de ir acompañada del nombre de usuario, ruta del fichero del par de claves y la opción de que no se desea clave para iniciar sesión. Todo ello especificado al protocolo SSH (22), usado para conectarse con la instancia.

Finalmente queda copiar el fichero \kode{cloud-config} a la instancia. Aquí se ha optado por crear una copia del fichero \kode{user-data} por máquina, identificándola con un número. El contenido del fichero original será copiado al segundo mediante métodos propios a \kode{YAML}. Una vez hecha la copia, se indica la dirección IP que la máquina va adquirir, siendo 10.0.0.10X, y se pasa el contenido mediante su lectura, haciendo uso del campo \kode{user\_data}.

Así, al fichero \kode{Vagrantfile} se le añade lo siguiente:

\begin{codelisting}
\label{code:vagrantfile2}
\codecaption{Fichero \kode{Vagrantfile}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
.
\PY{k}{if} \PY{o}{(}!ARGV.nil? \PY{o}{\PYZam{}\PYZam{}} ARGV.join\PY{o}{(}\PY{l+s+s1}{\PYZsq{}\PYZsq{}}\PY{o}{)}.include?\PY{o}{(}\PY{l+s+s1}{\PYZsq{}provider=aws\PYZsq{}}\PY{o}{)}\PY{o}{)}
  unless Vagrant.has\PYZus{}plugin?\PY{o}{(}\PY{l+s+s2}{\PYZdq{}vagrant\PYZhy{}aws\PYZdq{}}\PY{o}{)} 
    abort\PY{o}{(}\PY{l+s+s2}{\PYZdq{}Did not detect vagrant\PYZhy{}aws plugin... }
\PY{l+s+s2}{           vagrant plugin install vagrant\PYZhy{}aws\PYZdq{}}\PY{o}{)}
  end
  unless ENV\PY{o}{[}\PY{l+s+s1}{\PYZsq{}AWS\PYZus{}KEY\PYZsq{}}\PY{o}{]} \PY{o}{\PYZam{}\PYZam{}} ENV\PY{o}{[}\PY{l+s+s1}{\PYZsq{}AWS\PYZus{}SECRET\PYZsq{}}\PY{o}{]} \PY{o}{\PYZam{}\PYZam{}} ENV\PY{o}{[}\PY{l+s+s1}{\PYZsq{}AWS\PYZus{}KEYNAME\PYZsq{}}\PY{o}{]}
    abort\PY{o}{(}\PY{l+s+s2}{\PYZdq{}\PYZdl{}AWS\PYZus{}KEY \PYZam{}\PYZam{} \PYZdl{}AWS\PYZus{}SECRET \PYZam{}\PYZam{} \PYZdl{}AWS\PYZus{}KEYNAME should set before...\PYZdq{}}\PY{o}{)}
  end
  FileUtils.cp\PYZus{}r\PY{o}{(}File.join\PY{o}{(}File.dirname\PY{o}{(}\PYZus{}\PYZus{}FILE\PYZus{}\PYZus{}\PY{o}{)}, \PY{l+s+s2}{\PYZdq{}user\PYZhy{}data.sampleapp.aws\PYZdq{}}\PY{o}{)}, 
  File.join\PY{o}{(}File.dirname\PY{o}{(}\PYZus{}\PYZus{}FILE\PYZus{}\PYZus{}\PY{o}{)}, \PY{l+s+s2}{\PYZdq{}user\PYZhy{}data\PYZdq{}}\PY{o}{)}, :remove\PYZus{}destination \PY{o}{=}\PYZgt{} \PY{n+nb}{true}\PY{o}{)}
end
.
\PY{n+nv}{\PYZdl{}aws\PYZus{}region} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}us\PYZhy{}east\PYZhy{}1\PYZsq{}}
\PY{n+nv}{\PYZdl{}aws\PYZus{}availability\PYZus{}zone} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}us\PYZhy{}east\PYZhy{}1a\PYZsq{}}
\PY{n+nv}{\PYZdl{}aws\PYZus{}subnet\PYZus{}id} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}subnet\PYZhy{}b17d79ea\PYZsq{}}
\PY{n+nv}{\PYZdl{}aws\PYZus{}security\PYZus{}groups} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}sg\PYZhy{}3319964c\PYZsq{}}
\PY{n+nv}{\PYZdl{}aws\PYZus{}ami} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}ami\PYZhy{}00598116\PYZsq{}}
\PY{n+nv}{\PYZdl{}aws\PYZus{}instance\PYZus{}type} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}t2.micro\PYZsq{}}
\PY{n+nv}{\PYZdl{}aws\PYZus{}elastic\PYZus{}ip} \PY{o}{=} \PY{n+nb}{true}
.
Vagrant.configure\PY{o}{(}\PY{l+s+s2}{\PYZdq{}2\PYZdq{}}\PY{o}{)} \PY{k}{do} \PY{p}{|}config\PY{p}{|}
 .
  config.vm.provider :aws \PY{k}{do} \PY{p}{|}aws, override\PY{p}{|}
    aws.access\PYZus{}key\PYZus{}id \PY{o}{=} ENV\PY{o}{[}\PY{l+s+s1}{\PYZsq{}AWS\PYZus{}KEY\PYZsq{}}\PY{o}{]}
    aws.secret\PYZus{}access\PYZus{}key \PY{o}{=} ENV\PY{o}{[}\PY{l+s+s1}{\PYZsq{}AWS\PYZus{}SECRET\PYZsq{}}\PY{o}{]}
    aws.keypair\PYZus{}name \PY{o}{=} ENV\PY{o}{[}\PY{l+s+s1}{\PYZsq{}AWS\PYZus{}KEYNAME\PYZsq{}}\PY{o}{]}
    aws.security\PYZus{}groups \PY{o}{=} \PY{n+nv}{\PYZdl{}aws\PYZus{}security\PYZus{}groups}
    aws.ami \PY{o}{=} \PY{n+nv}{\PYZdl{}aws\PYZus{}ami}
    aws.instance\PYZus{}type \PY{o}{=} \PY{n+nv}{\PYZdl{}aws\PYZus{}instance\PYZus{}type}
    aws.region \PY{o}{=} \PY{n+nv}{\PYZdl{}aws\PYZus{}region}
    aws.subnet\PYZus{}id \PY{o}{=} \PY{n+nv}{\PYZdl{}aws\PYZus{}subnet\PYZus{}id}
    aws.elastic\PYZus{}ip \PY{o}{=} \PY{n+nv}{\PYZdl{}aws\PYZus{}elastic\PYZus{}ip}
    override.vm.synced\PYZus{}folder \PY{l+s+s2}{\PYZdq{}.\PYZdq{}}, \PY{l+s+s2}{\PYZdq{}/vagrant\PYZdq{}}, disabled: \PY{n+nb}{true}
\PY{n+nb}{    }override.ssh.username \PY{o}{=} \PY{l+s+s2}{\PYZdq{}core\PYZdq{}}
    override.ssh.private\PYZus{}key\PYZus{}path \PY{o}{=} ENV\PY{o}{[}\PY{l+s+s1}{\PYZsq{}AWS\PYZus{}KEYPATH\PYZsq{}}\PY{o}{]}
    override.ssh.insert\PYZus{}key \PY{o}{=} \PY{n+nb}{false}
\PY{n+nb}{    }override.vm.box\PYZus{}url \PY{o}{=} \PY{l+s+s2}{\PYZdq{}https://github.com/mitchellh/vagrant\PYZhy{}aws/raw/master/}
\PY{l+s+s2}{                           dummy.box\PYZdq{}}
  end
  .
  \PY{o}{(}1..\PY{n+nv}{\PYZdl{}num\PYZus{}instances}\PY{o}{)}.each \PY{k}{do} \PY{p}{|}i\PY{p}{|}
    config.vm.define \PY{n+nv}{vm\PYZus{}name} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}\PYZpc{}s\PYZhy{}\PYZpc{}02d\PYZdq{}} \PYZpc{} \PY{o}{[}\PY{n+nv}{\PYZdl{}instance\PYZus{}name\PYZus{}prefix}, i\PY{o}{]} \PY{k}{do} \PY{p}{|}config\PY{p}{|}
      .
      \PY{k}{if} File.exist?\PY{o}{(}CLOUD\PYZus{}CONFIG\PYZus{}PATH\PY{o}{)} \PY{o}{\PYZam{}\PYZam{}} ARGV\PY{o}{[}0\PY{o}{]}.eql?\PY{o}{(}\PY{l+s+s1}{\PYZsq{}up\PYZsq{}}\PY{o}{)}
        .
        config.vm.provider :aws \PY{k}{do} \PY{p}{|}aws, override\PY{p}{|}   
          \PY{n+nv}{user\PYZus{}data\PYZus{}specific} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}\PYZsh{}\PYZob{}CLOUD\PYZus{}CONFIG\PYZus{}PATH\PYZcb{}\PYZhy{}\PYZsh{}\PYZob{}i\PYZcb{}\PYZdq{}}
          require \PY{l+s+s1}{\PYZsq{}yaml\PYZsq{}}
          \PY{n+nv}{data} \PY{o}{=} YAML.load\PY{o}{(}IO.readlines\PY{o}{(}CLOUD\PYZus{}CONFIG\PYZus{}PATH\PY{o}{)}\PY{o}{[}1..\PYZhy{}1\PY{o}{]}.join\PY{o}{)}
          \PY{n+nv}{yaml} \PY{o}{=} YAML.dump\PY{o}{(}data\PY{o}{)}
          File.open\PY{o}{(}user\PYZus{}data\PYZus{}specific, \PY{l+s+s1}{\PYZsq{}w\PYZsq{}}\PY{o}{)} \PY{k}{do} \PY{p}{|}file\PY{p}{|}
            file.write\PY{o}{(}\PY{l+s+s2}{\PYZdq{}\PYZsh{}cloud\PYZhy{}config\PYZbs{}n\PYZbs{}n\PYZsh{}\PYZob{}yaml\PYZcb{}\PYZdq{}}\PY{o}{)}
          end
          aws.private\PYZus{}ip\PYZus{}address \PY{o}{=} \PY{l+s+s2}{\PYZdq{}10.0.0.\PYZsh{}\PYZob{}100+i\PYZcb{}\PYZdq{}}
          aws.user\PYZus{}data \PY{o}{=} File.read\PY{o}{(}user\PYZus{}data\PYZus{}specific\PY{o}{)}
        end
      end
    end
  end
end
\end{Verbatim}
\end{codelisting}

\subsection{Despliegue de una instancia}

A continuación se despliega esta infraestructura y se accede a la instancia:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }vagrant up \PYZhy{}\PYZhy{}provider\PY{o}{=}aws \PY{o}{\PYZam{}\PYZam{}} vagrant ssh core\PYZhy{}01
\end{Verbatim}
\end{framed_shaded}

Ahora se prueba que la salud del clúster es positiva:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{c}{\PYZsh{} etcdctl cluster\PYZhy{}health}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/cluster-health-aws-1.png}
\caption{Comprobación de la salud del clúster con 1 instancia.}
\end{figure}

Además se comprueba que la red virtual funciona correctamente:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{c}{\PYZsh{} sudo systemctl status flanneld}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/flanneld-aws-1.png}
\caption{Comprobación de la red virtual - \kode{core01}.}
\end{figure}

Por último se comprueba desde la consola de comandos que se puede acceder al servicio correctamente con el comando \kode{curl}:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{c}{\PYZsh{} curl http://localhost:80 | tail \PYZhy{}n 15}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{images/figures/curl-aws-1.png}
\caption{Acceso al servicio desde la máquina \kode{core01}.}
\end{figure}

Además, se utiliza la IP elástica para acceder a través del navegador web:

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{images/figures/access-aws-1.png}
\caption{Acceso al servicio desde el sistema anfitrión.}
\end{figure}

Desde la consola de AWS se puede apreciar dicha instancia:

\begin{figure}[H]
\image{images/figures/aws-console-1.png}
\caption{Máquina \kode{core-01} desde la consola de AWS.}
\end{figure}

Para destruir la infraestructura y los recursos asociados a ella, se ejecuta:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }vagrant destroy \PYZhy{}f
\end{Verbatim}
\end{framed_shaded}

\subsection{Despliegue de tres instancias}

Para realizar el despliegue de un clúster con varias máquinas virtuales se cambia el valor de la variable \kode{num\_instances} a 3 en los ficheros \kode{Vagrantfile} y \kode{config.rb}. De esta manera se tiene en cada una de las instancias una copia del mismo servicio.

A continuación se despliega esta infraestructura:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }vagrant up \PYZhy{}\PYZhy{}provider\PY{o}{=}aws
\end{Verbatim}
\end{framed_shaded}

Se prueba, desde el sistema anfitrión, que la salud del clúster es positiva en las máquinas:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }\PY{k}{for} i in \PY{l+m}{1} \PY{l+m}{2} 3\PY{p}{;} \PY{k}{do} vagrant ssh core\PYZhy{}0\PY{n+nv}{\PYZdl{}i} \PYZhy{}c \PY{l+s+s1}{\PYZsq{}etcdctl cluster\PYZhy{}health\PYZsq{}}\PY{p}{;} \PY{k}{done}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/cluster-health-aws-3.png}
\caption{Comprobación de la salud del clúster.}
\end{figure}

Además, se comprueba que la red virtual funciona correctamente:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }\PY{k}{for} i in \PY{l+m}{1} \PY{l+m}{2} 3\PY{p}{;} \PY{k}{do} vagrant ssh core\PYZhy{}0\PY{n+nv}{\PYZdl{}i} \PYZhy{}c \PY{l+s+s1}{\PYZsq{}sudo systemctl flanneld \PYZbs{}}
\PY{l+s+s1}{  | head \PYZhy{}n16\PYZsq{}}\PY{p}{;} \PY{k}{done}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/flanneld-aws-3.png}
\caption{Comprobación de la red virtual.}
\end{figure}

Para probar que las máquinas virtuales tienen conexión con los contenedores de otras máquinas virtuales se obtiene, en primer lugar, la dirección IP del contenedor \kode{some-postgres} de la máquina virtual \kode{core-03}:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{c}{\PYZsh{} docker inspect some\PYZhy{}postgres}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/figures/docker-inspect-3.png}
\caption{Obtención de la dirección IP de un contenedor en \kode{core-03}.}
\end{figure}

Luego se prueba que haya conexión desde la máquina virtual \kode{core-01}:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{c}{\PYZsh{} ping 172.17.82.2}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/figures/ping-1.png}
\caption{Prueba de conexión a un contenedor de \kode{core-03} desde \kode{core-01}.}
\end{figure}

Por último se comprueba que se puede acceder al servicio desde la consola de comandos, con el comando \kode{curl}, y desde el navegador web, con la dirección IP elástica. Desde la consola de AWS se podrán observar las instancias: 

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }\PY{k}{for} i in \PY{l+m}{1} \PY{l+m}{2} 3\PY{p}{;} \PY{k}{do} vagrant ssh core\PYZhy{}0\PY{n+nv}{\PYZdl{}i} \PYZhy{}c \PY{l+s+s1}{\PYZsq{}curl http://localhost:80 \PYZbs{}}
\PY{l+s+s1}{  | tail \PYZhy{}n 15\PYZsq{}}\PY{p}{;} \PY{k}{done}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/figures/curl-aws-3.png}
\caption{Acceso al servicio por consola de comandos.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{images/figures/nav-1.png}
\caption{Acceso al servicio por el navegador web desde \kode{core-01}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{images/figures/nav-2.png}
\caption{Acceso al servicio por el navegador web desde \kode{core-02}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{images/figures/nav-3.png}
\caption{Acceso al servicio por el navegador web desde \kode{core-03}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/figures/aws-console-3.png}
\caption{Consola AWS EC2 Instances - \kode{core-01}, \kode{core-02} y \kode{core-03}.}
\end{figure}

\subsection{Cambio de las unidades systemd a fleet}

Para mejorar la distribución de la infraestructura se van a cambiar las unidades de servicio systemd por fleet. Para ello los servicios tienen que ser especificados como ficheros a escribir en el sistema. Concretamente se van a situar en la ruta \kode{/home/core/}. La sección \kode{[Install]} se cambia por la propia a fleet, \kode{[X-Fleet]}. Aquí se especificará el parámetro \kode{Global} que programa la unidad en todos los agentes del clúster, de forma que desde cada una de las instancias que lo componen se puedan ver todos los servicios que hay en ellas. Luego se crean como unidades de servicio systemd los servicios empezados por \kode{fleet-}, para cada uno de ellos, que indicará a fleet que ha de empezar los servicios pasados como ficheros. 

Continuando con el clúster de 3 instancias, se realizan las siguientes modificaciones en el fichero \kode{user-data.sampleapp-aws}:

\begin{codelisting}
\label{code:vagrantfile2}
\codecaption{Fichero \kode{user-data.sampleapp-aws}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}cloud\PYZhy{}config}
\PY{n+nn}{\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+lScalar+lScalarPlain}{write\PYZus{}files}\PY{p+pIndicator}{:}
  \PY{l+lScalar+lScalarPlain}{.}
  \PY{l+lScalar+lScalarPlain}{\PYZhy{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{path}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{/home/core/volume\PYZhy{}public.service}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{permissions}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{0644}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit] }
      \PY{n+no}{Description= volume\PYZhy{}public share between some\PYZhy{}postgres, app\PYZhy{}job, app\PYZhy{}task }
                   \PY{n+no}{and some\PYZhy{}nginx }
      \PY{n+no}{After=docker.service}
      \PY{n+no}{Requires=docker.service}
      \PY{n+no}{[Service] }
      \PY{n+no}{TimeoutStartSec=0 }
      \PY{n+no}{ExecStart=/usr/bin/docker volume create \PYZhy{}\PYZhy{}name volume\PYZhy{}public}
      \PY{n+no}{[X\PYZhy{}Fleet]}
      \PY{n+no}{Global=true}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{path}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{/home/core/postgresql.service}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{permissions}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{0644}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit] }
      \PY{n+no}{Description=PostgreSQL database }
      \PY{n+no}{After=docker.service volume\PYZhy{}public.service}
      \PY{n+no}{Requires=docker.service volume\PYZhy{}public.service}
      \PY{n+no}{[Service] }
      \PY{n+no}{TimeoutStartSec=0}
      \PY{n+no}{Restart=always}
      \PY{n+no}{EnvironmentFile=/etc/postgres\PYZhy{}credentials.env}
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker kill some\PYZhy{}postgres }
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker rm some\PYZhy{}postgres }
      \PY{n+no}{ExecStartPre=/usr/bin/docker pull postgres }
      \PY{n+no}{ExecStart=/usr/bin/docker run \PYZhy{}\PYZhy{}rm \PYZhy{}\PYZhy{}name some\PYZhy{}postgres \PYZbs{}}
      \PY{n+no}{\PYZhy{}e \PYZdq{}POSTGRES\PYZus{}USER=\PYZdl{}\PYZob{}POSTGRES\PYZus{}USER\PYZcb{}\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}e \PYZdq{}POSTGRES\PYZus{}PASSWORD=\PYZdl{}\PYZob{}POSTGRES\PYZus{}PASSWORD\PYZcb{}\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}v \PYZdq{}volume\PYZhy{}public:/var/lib/postgresql\PYZdq{} \PYZhy{}p \PYZdq{}5432:5432\PYZdq{} postgres }
      \PY{n+no}{ExecStop=/usr/bin/docker stop some\PYZhy{}postgres}
      \PY{n+no}{[X\PYZhy{}Fleet]}
      \PY{n+no}{Global=true}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{path}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{/home/core/app\PYZhy{}job.service}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{permissions}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{0644}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit] }
      \PY{n+no}{Description=executable app\PYZhy{}job container that creates, migrates, seeds and }
                  \PY{n+no}{populates the database}
      \PY{n+no}{After=docker.service volume\PYZhy{}public.service postgresql.service}
      \PY{n+no}{Requires=docker.service volume\PYZhy{}public.service postgresql.service}
      \PY{n+no}{[Service] }
      \PY{n+no}{TimeoutStartSec=0 }
      \PY{n+no}{EnvironmentFile=/etc/postgres\PYZhy{}credentials.env}
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker kill app\PYZhy{}job }
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker rm app\PYZhy{}job }
      \PY{n+no}{ExecStartPre=/usr/bin/docker pull carolina/sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image:latest }
      \PY{n+no}{ExecStart=/usr/bin/docker run \PYZhy{}\PYZhy{}rm \PYZhy{}\PYZhy{}name app\PYZhy{}job \PYZbs{}}
      \PY{n+no}{\PYZhy{}v \PYZdq{}volume\PYZhy{}public:/usr/src/app/public\PYZdq{} \PYZhy{}\PYZhy{}entrypoint \PYZdq{}./setup.sh\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}e \PYZdq{}POSTGRES\PYZus{}USER=\PYZdl{}\PYZob{}POSTGRES\PYZus{}USER\PYZcb{}\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}e \PYZdq{}POSTGRES\PYZus{}PASSWORD=\PYZdl{}\PYZob{}POSTGRES\PYZus{}PASSWORD\PYZcb{}\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}w \PYZdq{}/usr/src/app\PYZdq{} \PYZhy{}\PYZhy{}link \PYZdq{}some\PYZhy{}postgres:db\PYZdq{} \PYZbs{}}
      \PY{n+no}{carolina/sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image:latest}
      \PY{n+no}{[X\PYZhy{}Fleet]}
      \PY{n+no}{Global=true}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{path}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{/home/core/app\PYZhy{}task.service}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{permissions}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{0644}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit] }
      \PY{n+no}{Description=app\PYZhy{}task container that runs the server puma}
      \PY{n+no}{After=docker.service volume\PYZhy{}public.service postgresql.service }
            \PY{n+no}{app\PYZhy{}job.service}
      \PY{n+no}{Requires=docker.service volume\PYZhy{}public.service postgresql.service }
               \PY{n+no}{app\PYZhy{}job.service}
      \PY{n+no}{[Service] }
      \PY{n+no}{TimeoutStartSec=0}
      \PY{n+no}{Restart=always }
      \PY{n+no}{EnvironmentFile=/etc/postgres\PYZhy{}credentials.env}
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker kill app\PYZhy{}task }
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker rm app\PYZhy{}task}
      \PY{n+no}{ExecStartPre=/usr/bin/docker pull carolina/sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image:latest }
      \PY{n+no}{ExecStart=/usr/bin/docker run \PYZhy{}\PYZhy{}rm \PYZhy{}\PYZhy{}name app\PYZhy{}task \PYZbs{}}
      \PY{n+no}{\PYZhy{}e \PYZdq{}POSTGRES\PYZus{}USER=\PYZdl{}\PYZob{}POSTGRES\PYZus{}USER\PYZcb{}\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}e \PYZdq{}POSTGRES\PYZus{}PASSWORD=\PYZdl{}\PYZob{}POSTGRES\PYZus{}PASSWORD\PYZcb{}\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}w \PYZdq{}/usr/src/app\PYZdq{} \PYZhy{}v \PYZdq{}volume\PYZhy{}public:/usr/src/app/public\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}\PYZhy{}link \PYZdq{}some\PYZhy{}postgres:db\PYZdq{} \PYZbs{}}
      \PY{n+no}{carolina/sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image:latest \PYZbs{}}
      \PY{n+no}{/bin/bash \PYZhy{}c \PYZdq{}cp config/database.yml.postgresql config/database.yml \PYZam{}\PYZam{} \PYZbs{}}
      \PY{n+no}{cp ./.secret.example ./.secret \PYZam{}\PYZam{} puma \PYZhy{}p 9292\PYZdq{}}
      \PY{n+no}{ExecStop=/usr/bin/docker stop app\PYZhy{}task}
      \PY{n+no}{[X\PYZhy{}Fleet]}
      \PY{n+no}{Global=true}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{path}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{/home/core/nginx.service}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{permissions}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{0644}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit] }
      \PY{n+no}{Description=some\PYZhy{}nginx container that runs a reverse proxy server and a }
                  \PY{n+no}{web server}
      \PY{n+no}{After=docker.service volume\PYZhy{}public.service postgresql.service }
            \PY{n+no}{app\PYZhy{}job.service app\PYZhy{}task.service}
      \PY{n+no}{Requires=docker.service volume\PYZhy{}public.service postgresql.service }
               \PY{n+no}{app\PYZhy{}job.service app\PYZhy{}task.service}
      \PY{n+no}{[Service] }
      \PY{n+no}{TimeoutStartSec=0}
      \PY{n+no}{Restart=always }
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker kill some\PYZhy{}nginx }
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker rm some\PYZhy{}nginx}
      \PY{n+no}{ExecStartPre=/usr/bin/docker pull nginx }
      \PY{n+no}{ExecStart=/usr/bin/docker run \PYZhy{}\PYZhy{}rm \PYZhy{}\PYZhy{}name some\PYZhy{}nginx \PYZbs{}}
      \PY{n+no}{\PYZhy{}v \PYZdq{}/etc/nginx.conf:/etc/nginx/conf.d/default.conf\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}p \PYZdq{}80:80\PYZdq{} \PYZhy{}\PYZhy{}link \PYZdq{}app\PYZhy{}task:app\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}v \PYZdq{}volume\PYZhy{}public:/usr/src/app/public\PYZdq{} nginx }
      \PY{n+no}{ExecStop=/usr/bin/docker stop some\PYZhy{}nginx}
      \PY{n+no}{[X\PYZhy{}Fleet]}
      \PY{n+no}{Global=true}
\PY{l+lScalar+lScalarPlain}{coreos}\PY{p+pIndicator}{:}
  \PY{l+lScalar+lScalarPlain}{.}
  \PY{l+lScalar+lScalarPlain}{units}\PY{p+pIndicator}{:}
  \PY{l+lScalar+lScalarPlain}{.}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{fleet\PYZhy{}volume\PYZhy{}public.service}
    \PY{l+lScalar+lScalarPlain}{command}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{start}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit]}
      \PY{n+no}{Description=Start volume\PYZhy{}public.service using fleet}
      \PY{n+no}{[Service]}
      \PY{n+no}{ExecStart=/usr/bin/fleetctl start /home/core/volume\PYZhy{}public.service}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{fleet\PYZhy{}postgresql.service}
    \PY{l+lScalar+lScalarPlain}{command}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{start}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit]}
      \PY{n+no}{Description=Start postgresql.service using fleet}
      \PY{n+no}{[Service]}
      \PY{n+no}{ExecStart=/usr/bin/fleetctl start /home/core/postgresql.service}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{fleet\PYZhy{}app\PYZhy{}job.service}
    \PY{l+lScalar+lScalarPlain}{command}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{start}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit]}
      \PY{n+no}{Description=Start app\PYZhy{}job.service using fleet}
      \PY{n+no}{[Service]}
      \PY{n+no}{ExecStart=/usr/bin/fleetctl start /home/core/app\PYZhy{}job.service}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{fleet\PYZhy{}app\PYZhy{}task.service}
    \PY{l+lScalar+lScalarPlain}{command}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{start}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit]}
      \PY{n+no}{Description=Start app\PYZhy{}task.service using fleet}
      \PY{n+no}{[Service]}
      \PY{n+no}{ExecStart=/usr/bin/fleetctl start /home/core/app\PYZhy{}task.service}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{fleet\PYZhy{}nginx.service}
    \PY{l+lScalar+lScalarPlain}{command}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{start}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit]}
      \PY{n+no}{Description=Start nginx.service using fleet}
      \PY{n+no}{[Service]}
      \PY{n+no}{ExecStart=/usr/bin/fleetctl start /home/core/nginx.service}
\end{Verbatim}
\end{codelisting}

Para comprobar que funciona se levanta la infraestructura:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }vagrant up \PYZhy{}\PYZhy{}provider\PY{o}{=}aws
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/figures/aws-console-fleet.png}
\caption{Consola AWS EC2 Instances - \kode{core-01}, \kode{core-02} y \kode{core-03}.}
\end{figure}

Una vez terminado basta con entrar en la máquina \kode{core-01} para comprobar que las unidades funcionan correctamente:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }vagrant ssh core\PYZhy{}01
\PY{c}{\PYZsh{} fleetctl list\PYZhy{}units}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/fleetctl-list-units.png}
\caption{Listado de unidades fleet en cada una de las máquinas.}
\end{figure}

Como se puede apreciar, cada una de las máquinas, tiene una copia de cada servicio. Todas se han activado y se encuentran en ejecución, salvo las unidades \kode{app-job.service} y \kode{volume-public.service} que aparecen inactivas porque son contenedores ejecutables, es decir, cuando acaban su cometido terminan.

También se comprueba que la actividad de la aplicación sigue siendo correcta, solicitándola con el comando \kode{curl} en cada una de las máquinas:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }\PY{k}{for} i in \PY{l+m}{1} \PY{l+m}{2} 3\PY{p}{;} \PY{k}{do} vagrant ssh core\PYZhy{}0\PY{n+nv}{\PYZdl{}i} \PYZhy{}c \PY{l+s+s1}{\PYZsq{}curl http://localhost:80 \PYZbs{}}
\PY{l+s+s1}{  | tail \PYZhy{}n 15\PYZsq{}}\PY{p}{;} \PY{k}{done}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{images/figures/curl-fleet.png}
\caption{Acceso al servicio por consola de comandos.}
\end{figure}

\subsection{Configuración DNS usando SkyDNS}

Con el objetivo de aislar la base de datos en una sola máquina se hace uso de un servidor de sistema de nombres de dominio (DNS). Es necesario puesto que los contenedores \kode{app-job} y \kode{app-task} necesitan acceder al contenedor \kode{some-postgres}. Este contenedor, al no estar alojado en la misma máquina, tendrá que ser registrado en el servidor DNS para que pueda ser descubierto desde las otras máquinas. Así, los contenedores \kode{app-job} y \kode{app-task} resolverán la base de datos mediante dicho servidor DNS, que tendrá presencia en todas las máquinas como un nuevo contenedor, llamado \kode{skydns}. SkyDNS es un servicio distribuido para el anuncio y descubrimiento de servicios construidos sobre etcd, creando y leyendo registros en él. Se trata de un proyecto de código abierto que puede ser encontrado en GitHub, bajo el repositorio \kode{skynetservices/skydns}.

Así, el primer paso consistirá en la división de la infraestructura de forma que los contenedores \kode{volume-public}, \kode{app-job}, \kode{app-task} y \kode{some-ningx} sean creados en la máquina \kode{core-01}. Por otro lado, el contenedor \kode{some-postgres} será creado en la máquina \kode{core-02}. Esto se consigue usando la opción \textit{metadata} presente en las unidades fleet. Para especificarlo es necesario añadir al fichero \kode{Vagrantfile} un metadato diferente para cada una de las máquinas, de manera que \kode{core-01} reciba \textit{compute=web} y \kode{core-02} \textit{compute=db}. Esto ha de ser añadido bajo la carga de cada una de las líneas del fichero \kode{cloud-config}.

\begin{codelisting}
\label{code:vagrantfile2}
\codecaption{Fichero \kode{Vagrantfile}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{o}{.}
\PY{n}{data} \PY{o}{=} \PY{n+no}{YAML}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n+no}{IO}\PY{o}{.}\PY{n}{readlines}\PY{p}{(}\PY{n+no}{CLOUD\PYZus{}CONFIG\PYZus{}PATH}\PY{p}{)}\PY{o}{[}\PY{l+m+mi}{1}\PY{o}{.}\PY{n}{.}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{o}{]}\PY{o}{.}\PY{n}{join}\PY{p}{)}
\PY{k}{if} \PY{n}{data}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}coreos\PYZsq{}}\PY{o}{]}\PY{o}{.}\PY{n}{key?} \PY{l+s+s1}{\PYZsq{}fleet\PYZsq{}} \PY{o+ow}{and} \PY{n}{i}\PY{o}{==}\PY{l+m+mi}{1}
  \PY{n}{data}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}coreos\PYZsq{}}\PY{o}{]}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}fleet\PYZsq{}}\PY{o}{]}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}metadata\PYZsq{}}\PY{o}{]} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}compute=web\PYZsq{}}
\PY{k}{end}
\PY{k}{if} \PY{n}{data}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}coreos\PYZsq{}}\PY{o}{]}\PY{o}{.}\PY{n}{key?} \PY{l+s+s1}{\PYZsq{}fleet\PYZsq{}} \PY{o+ow}{and} \PY{n}{i}\PY{o}{==}\PY{l+m+mi}{2}
 \PY{n}{data}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}coreos\PYZsq{}}\PY{o}{]}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}fleet\PYZsq{}}\PY{o}{]}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}metadata\PYZsq{}}\PY{o}{]} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}compute=db\PYZsq{}}
\PY{k}{end}
\PY{o}{.}
\end{Verbatim}
\end{codelisting}

Ahora será necesario añadir el metadato a los ficheros de las unidades de servicio, de los contenedores nombrados, en la sección \textit{[X-Fleet]}. Para la máquina \kode{core-01} el parámetro \textit{MachineMetadata=compute=web} y en el fichero correspondiente a la base de datos el parámetro \textit{MachineMetadata=compute=db}, en la máquina \kode{core-02}.

Como el contenedor \kode{some-postgres} ya no se estará en la misma máquina que el resto de contenedores no es posible compartir \kode{volume-public} con ellos. Por lo tanto se eliminará en su definición el comando \textit{-v}, así como su inclusión en los parámetros \textit{After} y \textit{Require}.

Lo siguiente será crear el contenedor que se corresponde con el servidor DNS, presente en todas las máquinas. Este último hecho es posible no añadiendo ningún metadato en la sección \textit{[X-Fleet]}. Se escribe un nuevo fichero llamado \kode{/home/core/skydns.service} antes del fichero \kode{/home/core/volumen-public.service}. Además se le indica que será iniciado después de Docker y etcd2. Luego se procede de la misma manera que con las otras unidades de servicio. Antes de comenzar el servicio borrará el contenedor si existe previamente y luego descargará la imagen desde Docker Hub. Además, se establece la configuración de SkyDNS de forma que se especifica la dirección IP y puerto en la que debe escuchar, que será la local a la máquina por el puerto 53. También se establece el dominio para el que es autoritario, escogiéndose \kode{sampleapp.local.}, y el tiempo de vida (TTL) de las respuestas a 30 segundos. A la hora de comenzar el servicio restará indicarle que las máquinas etcd se comunican en la dirección IP privada en uso y puerto 2379. Como se trata de una unidad fleet, será necesario añadir el servicio global \kode{fleet-skydns.service} antes de \kode{fleet-volume-public.service} para comenzar el servicio. 

Así, habrá que añadir a \kode{user-data.sampleapp.aws}:

\begin{codelisting}
\label{code:user-data-skydns}
\codecaption{Fichero \kode{user-data.sampleapp.aws}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{l+lScalar+lScalarPlain}{.}
\PY{l+lScalar+lScalarPlain}{write\PYZus{}files}\PY{p+pIndicator}{:}
\PY{l+lScalar+lScalarPlain}{.}
  \PY{l+lScalar+lScalarPlain}{\PYZhy{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{path}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{/home/core/skydns.service}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{permissions}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{0644}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit]}
      \PY{n+no}{Description=SkyDNS}
      \PY{n+no}{After=docker.service etcd2.service}
      \PY{n+no}{Requires=docker.service etcd2.service}
      \PY{n+no}{[Service]}
      \PY{n+no}{Restart=always}
      \PY{n+no}{TimeoutStartSec=0}
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker rm \PYZhy{}f skydns}
      \PY{n+no}{ExecStartPre=/usr/bin/docker pull skynetservices/skydns}
      \PY{n+no}{ExecStartPre=/usr/bin/etcdctl set /skydns/config \PYZbs{}}
        \PY{n+no}{\PYZsq{}\PYZob{}\PYZdq{}dns\PYZus{}addr\PYZdq{}:\PYZdq{}0.0.0.0:53\PYZdq{}, \PYZdq{}domain\PYZdq{}: \PYZdq{}sampleapp.local.\PYZdq{}, \PYZdq{}ttl\PYZdq{}:30\PYZcb{}\PYZsq{}}
      \PY{n+no}{ExecStart=/usr/bin/docker run \PYZhy{}\PYZhy{}rm \PYZhy{}\PYZhy{}name skydns \PYZbs{}}
        \PY{n+no}{\PYZhy{}e ETCD\PYZus{}MACHINES=\PYZdq{}http://\PYZdl{}private\PYZus{}ipv4:2379\PYZdq{} skynetservices/skydns}
      \PY{n+no}{ExecStop=\PYZhy{}/usr/bin/docker stop skydns}
      \PY{n+no}{[X\PYZhy{}Fleet]}
      \PY{n+no}{Global=true}
\PY{l+lScalar+lScalarPlain}{.}
\PY{l+lScalar+lScalarPlain}{coreos}\PY{p+pIndicator}{:}
\PY{l+lScalar+lScalarPlain}{.}
  \PY{l+lScalar+lScalarPlain}{units}\PY{p+pIndicator}{:}
  \PY{l+lScalar+lScalarPlain}{.}
  \PY{l+lScalar+lScalarPlain}{\PYZhy{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{fleet\PYZhy{}skydns.service}
    \PY{l+lScalar+lScalarPlain}{command}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{start}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit]}
      \PY{n+no}{Description=Start skydns.service using fleet}
      \PY{n+no}{[Service]}
      \PY{n+no}{ExecStart=/usr/bin/fleetctl start /home/core/skydns.service}
\PY{l+lScalar+lScalarPlain}{.}
\end{Verbatim}
\end{codelisting}

Además, al fichero \kode{/home/core/postgresql.service} hay que añadirle el registro de la dirección IP del contenedor \kode{some-postgres} para que pueda ser encontrado por los otros contenedores, desde las otras máquinas. Este registro ha de ser añadido después de que el contenedor esté en ejecución, por lo que se especificará como \textit{ExecStartPost}. Los contenedores \kode{app-job} y \kode{app-task} necesitan resolverlo como \textit{db}. Sin embargo, \kode{app-job} también resuelve por \textit{some-postgres}, puesto que antes de comenzar sus tareas espera a que dicho contenedor esté activo en el puerto 5432. Teniendo en cuenta estas condiciones se realizan dos registros, el primero tipo nombre canónico (CNAME) de \textit{some-postgres.sampleapp.local.} a \textit{db.sampleapp.local.} y el segundo tipo dirección IP (A) de \textit{db.sampleapp.local.} a dirección IP de \kode{some-postgres}, accesible por el puerto 5432.

\begin{codelisting}
\label{code:user-data-skydns-postgresql}
\codecaption{Fichero \kode{user-data.sampleapp.aws}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{l+lScalar+lScalarPlain}{.}
\PY{l+lScalar+lScalarPlain}{write\PYZus{}files}\PY{p+pIndicator}{:}
\PY{l+lScalar+lScalarPlain}{.}
  \PY{l+lScalar+lScalarPlain}{\PYZhy{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{path}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{/home/core/postgresql.service}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{.}
      \PY{l+lScalar+lScalarPlain}{[Service]}
      \PY{l+lScalar+lScalarPlain}{.}
      \PY{l+lScalar+lScalarPlain}{ExecStartPost=/bin/bash}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZhy{}c}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZsq{}while}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{!}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
        \PY{l+lScalar+lScalarPlain}{[}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdl{}(/usr/bin/docker}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{inspect}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZhy{}f}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{=\PYZdq{}\PYZob{}\PYZob{}.State.Running\PYZcb{}\PYZcb{}\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{some\PYZhy{}postgres)}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
        \PY{l+lScalar+lScalarPlain}{==}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdq{}=true\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{];}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{do}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{sleep}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{1;}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{done;}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
        \PY{l+lScalar+lScalarPlain}{/usr/bin/etcdctl}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{set}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{/skydns/local/sampleapp/some\PYZhy{}postgres}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
        \PY{l+lScalar+lScalarPlain}{\PYZdq{}\PYZob{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}\PYZbs{}\PYZdq{}host\PYZbs{}\PYZbs{}\PYZdq{}}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{\PYZbs{}\PYZbs{}\PYZdq{}db.sampleapp.local.\PYZbs{}\PYZbs{}\PYZdq{}\PYZcb{}\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{;}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
        \PY{l+lScalar+lScalarPlain}{/usr/bin/etcdctl}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{set}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{/skydns/local/sampleapp/db}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
        \PY{l+lScalar+lScalarPlain}{\PYZdq{}\PYZob{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}\PYZbs{}\PYZdq{}host\PYZbs{}\PYZbs{}\PYZdq{}}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{\PYZbs{}\PYZbs{}\PYZdq{}\PYZdl{}(/usr/bin/docker}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{inspect}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZhy{}f}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
        \PY{l+lScalar+lScalarPlain}{\PYZdq{}\PYZob{}\PYZob{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{.NetworkSettings.IPAddress}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZcb{}\PYZcb{}\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{some\PYZhy{}postgres)\PYZbs{}\PYZbs{}\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{,}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}\PYZbs{}\PYZdq{}port\PYZbs{}\PYZbs{}\PYZdq{}:5432\PYZcb{}\PYZdq{}\PYZsq{}}
\PY{l+lScalar+lScalarPlain}{.}
\end{Verbatim}
\end{codelisting}

En los contenedores \kode{app-job} y \kode{app-task} habrá que eliminar la opción \textit{---link} y utilizar las opciones \textit{---dns} que apuntará a la dirección IP del contenedor \kode{skydns} y \textit{---dns-search} donde se especifica el dominio trabajado \textit{sampleapp.local} para poder resolver por \textit{db} y \textit{some-postgres} sin especificar el dominio. La sección \textit{ExecStart} de ambas queda:

\begin{codelisting}
\label{code:user-data-skydns-app}
\codecaption{Fichero \kode{user-data.sampleapp.aws}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{l+lScalar+lScalarPlain}{.}
\PY{l+lScalar+lScalarPlain}{write\PYZus{}files}\PY{p+pIndicator}{:}
\PY{l+lScalar+lScalarPlain}{.}
  \PY{l+lScalar+lScalarPlain}{\PYZhy{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{path}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{/home/core/app\PYZhy{}job.service}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{.}
      \PY{l+lScalar+lScalarPlain}{[Service]}
      \PY{l+lScalar+lScalarPlain}{.}
      \PY{l+lScalar+lScalarPlain}{ExecStart=/bin/bash}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZhy{}c}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZsq{}usr/bin/docker}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{run}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZhy{}\PYZhy{}rm}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZhy{}\PYZhy{}name}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{app\PYZhy{}job}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
      \PY{l+lScalar+lScalarPlain}{\PYZhy{}v}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdq{}volume\PYZhy{}public:/usr/src/app/public\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZhy{}\PYZhy{}entrypoint}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdq{}./setup.sh\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
      \PY{l+lScalar+lScalarPlain}{\PYZhy{}e}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdq{}POSTGRES\PYZus{}USER=\PYZdl{}\PYZob{}POSTGRES\PYZus{}USER\PYZcb{}\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
      \PY{l+lScalar+lScalarPlain}{\PYZhy{}e}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdq{}POSTGRES\PYZus{}PASSWORD=\PYZdl{}\PYZob{}POSTGRES\PYZus{}PASSWORD\PYZcb{}\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
      \PY{l+lScalar+lScalarPlain}{\PYZhy{}w}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdq{}/usr/src/app\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZhy{}\PYZhy{}dns}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdl{}(/usr/bin/docker}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{inspect}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
      \PY{l+lScalar+lScalarPlain}{\PYZhy{}f}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdq{}\PYZob{}\PYZob{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{.NetworkSettings.IPAddress}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZcb{}\PYZcb{}\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{skydns)}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
      \PY{l+lScalar+lScalarPlain}{\PYZhy{}\PYZhy{}dns\PYZhy{}search}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdq{}sampleapp.local\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{carolina/sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image:latest\PYZsq{}}
      \PY{l+lScalar+lScalarPlain}{.}
  \PY{l+lScalar+lScalarPlain}{\PYZhy{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{path}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{/home/core/app\PYZhy{}task.service}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{.}
      \PY{l+lScalar+lScalarPlain}{[Service]}
      \PY{l+lScalar+lScalarPlain}{.}
      \PY{l+lScalar+lScalarPlain}{ExecStart=/bin/bash}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZhy{}c}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZsq{}usr/bin/docker}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{run}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZhy{}\PYZhy{}rm}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZhy{}\PYZhy{}name}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{app\PYZhy{}task}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
      \PY{l+lScalar+lScalarPlain}{\PYZhy{}e}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdq{}POSTGRES\PYZus{}USER=\PYZdl{}\PYZob{}POSTGRES\PYZus{}USER\PYZcb{}\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
      \PY{l+lScalar+lScalarPlain}{\PYZhy{}e}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdq{}POSTGRES\PYZus{}PASSWORD=\PYZdl{}\PYZob{}POSTGRES\PYZus{}PASSWORD\PYZcb{}\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
      \PY{l+lScalar+lScalarPlain}{\PYZhy{}w}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdq{}/usr/src/app\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZhy{}v}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdq{}volume\PYZhy{}public:/usr/src/app/public\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
      \PY{l+lScalar+lScalarPlain}{\PYZhy{}\PYZhy{}dns}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdl{}(/usr/bin/docker}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{inspect}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
      \PY{l+lScalar+lScalarPlain}{\PYZhy{}f}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdq{}\PYZob{}\PYZob{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{.NetworkSettings.IPAddress}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZcb{}\PYZcb{}\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{skydns)}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
      \PY{l+lScalar+lScalarPlain}{\PYZhy{}\PYZhy{}dns\PYZhy{}search}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdq{}sampleapp.local\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{carolina/sample\PYZus{}app\PYZus{}rails\PYZus{}4\PYZus{}image:latest}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
      \PY{l+lScalar+lScalarPlain}{/bin/bash}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZhy{}c}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdq{}cp}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{config/database.yml.postgresql}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{config/database.yml}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZam{}\PYZam{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
      \PY{l+lScalar+lScalarPlain}{cp}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{./.secret.example}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{./.secret}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZam{}\PYZam{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{puma}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZhy{}p}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{9292\PYZdq{}\PYZsq{}}
      \PY{l+lScalar+lScalarPlain}{.}
\end{Verbatim}
\end{codelisting}

Además habrá que quitar de las secciones \textit{After} y \textit{Requires} el servicio \kode{postgresql.service} en todas las unidades y añadir \kode{skydns.service} antes de \kode{volume-public.service}.

A continuación se desplega la infraestructura:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }vagrant up \PYZhy{}\PYZhy{}provider\PY{o}{=}aws
\end{Verbatim}
\end{framed_shaded}

Primero se comprueba la salud del clúster, contemplada por cada máquina:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }\PY{k}{for} i in \PY{l+m}{1} \PY{l+m}{2} 3\PY{p}{;} \PY{k}{do} vagrant ssh core\PYZhy{}0\PY{n+nv}{\PYZdl{}i} \PYZhy{}c \PY{l+s+s1}{\PYZsq{}etcdctl cluster\PYZhy{}health\PYZsq{}}\PY{p}{;} \PY{k}{done}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/figures/skydns-health.png}
\caption{Salud del clúster contemplada por las 3 máquinas.}
\end{figure}

Luego se listan las máquinas y se comprueba que han recibido el metadato:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{k}{for} i in \PY{l+m}{1} \PY{l+m}{2} 3\PY{p}{;} \PY{k}{do} vagrant ssh core\PYZhy{}0\PY{n+nv}{\PYZdl{}i} \PYZhy{}c \PY{l+s+s1}{\PYZsq{}fleetctl list\PYZhy{}machines\PYZsq{}}\PY{p}{;} \PY{k}{done}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/figures/skydns-machines.png}
\caption{Información de las máquinas contemplada por las 3 máquinas.}
\end{figure}

También se comprueba que las unidades de servicio se crearon donde se indicó:
\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }\PY{k}{for} i in \PY{l+m}{1} \PY{l+m}{2} 3\PY{p}{;} \PY{k}{do} vagrant ssh core\PYZhy{}0\PY{n+nv}{\PYZdl{}i} \PYZhy{}c \PY{l+s+s1}{\PYZsq{}fleetctl list\PYZhy{}units\PYZsq{}}\PY{p}{;} \PY{k}{done}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/figures/skydns-units.png}
\caption{Información de las unidades contemplada por las 3 máquinas.}
\end{figure}

Se comprueba que los registros se hayan establecido correctamente:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }\PY{k}{for} i in \PY{l+m}{1} \PY{l+m}{2} 3\PY{p}{;} \PY{k}{do} vagrant ssh core\PYZhy{}0\PY{n+nv}{\PYZdl{}i} \PYZhy{}c \PY{l+s+s1}{\PYZsq{}/usr/bin/etcdctl get \PYZbs{}}
\PY{l+s+s1}{  /skydns/local/sampleapp/db; \PYZbs{}}
\PY{l+s+s1}{  /usr/bin/etcdctl get /skydns/local/sampleapp/some\PYZhy{}postgres \PYZsq{}}\PY{p}{;} \PY{k}{done}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/figures/skydns-gets.png}
\caption{Lectura de los registros en SkyDNS contemplada por las 3 máquinas.}
\end{figure}

La máquina \kode{core-01} resuelve tanto \textit{db} como \textit{some-postgres}:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{c}{\PYZsh{} docker exec \PYZhy{}it app\PYZhy{}task sh \PYZhy{}c \PYZsq{}ping \PYZhy{}c 1 db\PYZsq{}}
\PY{c}{\PYZsh{} docker exec \PYZhy{}it app\PYZhy{}task sh \PYZhy{}c \PYZsq{}ping \PYZhy{}c 1 some\PYZhy{}postgres\PYZsq{}}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/figures/skydns-ping.png}
\caption{Conexión a \textit{db} como \textit{some-postgres} desde \kode{app-task}.}
\end{figure}

De esta manera, los registros ingresados han quedado como se pretendía:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{c}{\PYZsh{} docker exec \PYZhy{}it skydns sh \PYZhy{}c \PYZsq{}dig @localhost db.sampleapp.local. A\PYZsq{}}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{images/figures/skydns-dig.png}
\caption{Registros DNS desde el contenedor \kode{skydns}.}
\end{figure}

Así, el servicio funciona correctamente en la máquina \kode{core-01}:
\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }\PY{k}{for} i in \PY{l+m}{1} \PY{l+m}{2} 3\PY{p}{;} \PY{k}{do} vagrant ssh core\PYZhy{}0\PY{n+nv}{\PYZdl{}i} \PYZhy{}c \PY{l+s+s1}{\PYZsq{}curl http://localhost:80 | \PYZbs{}}
\PY{l+s+s1}{  tail \PYZhy{}n 15\PYZsq{}}\PY{p}{;} \PY{k}{done}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{images/figures/skydns-curl.png}
\caption{Conexión al servicio mediante consola desde \kode{core-01}.}
\end{figure}

Y mediante la web y la dirección IP elástica se ingresa con uno de los usuarios poblados en la base de datos, comprobando que sigue funcionando correctamente hasta este punto:

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/figures/skydns-web.png}
\caption{Inicio de sesión con el usuario \textit{Example User} desde \kode{core-01}.}
\end{figure}

\subsection{Balanceo de carga con Nginx usando Confd}

Con el propósito de implementar la infraestructura final se van a realizar una serie de cambios. En la primera máquina se sitúa el proxy, que balanceará la carga entre ellos. En la segunda máquina se encontrará la base de datos a la que acudirán los servidores de aplicación.

Al no encontrarse los distintos contenedores en la misma máquina la opción de tener el volumen Docker \kode{volume-public} compartido entre ellos no es posible en la manera en la que se ha ido haciendo. Así, se elimina su definición en el fichero \kode{user-data.sampleapp.aws}, en \kode{/home/core/volume-public.service}, su inclusión en los parámetros \textit{After} y \textit{Requires} en los servicios que lo implementan, el parámetro \textit{-v} que lo exporta en los contenedores \kode{app-job}, \kode{app-task} y \kode{some-nginx}, así como la unidad global \kode{fleet-volume-public.service}.

El despliegue de la infraestructura será controlado con metadatos. Para ello se mantiene a la máquina \kode{core-02} con el metadato \textit{compute=db} y se modifica en el fichero \kode{Vagrantfile}, bajo la carga de la línea que contiene la sección de configuración fleet en el fichero \kode{user-data}, el metadato para la máquina \kode{core-01}, cambiándolo por \textit{compute=proxy}.

\begin{codelisting}
\label{code:vagrantfile2}
\codecaption{Fichero \kode{Vagrantfile}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{o}{.}
\PY{n}{data} \PY{o}{=} \PY{n+no}{YAML}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n+no}{IO}\PY{o}{.}\PY{n}{readlines}\PY{p}{(}\PY{n+no}{CLOUD\PYZus{}CONFIG\PYZus{}PATH}\PY{p}{)}\PY{o}{[}\PY{l+m+mi}{1}\PY{o}{.}\PY{n}{.}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{o}{]}\PY{o}{.}\PY{n}{join}\PY{p}{)}
\PY{k}{if} \PY{n}{data}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}coreos\PYZsq{}}\PY{o}{]}\PY{o}{.}\PY{n}{key?} \PY{l+s+s1}{\PYZsq{}fleet\PYZsq{}} \PY{o+ow}{and} \PY{n}{i}\PY{o}{==}\PY{l+m+mi}{1}
 \PY{n}{data}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}coreos\PYZsq{}}\PY{o}{]}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}fleet\PYZsq{}}\PY{o}{]}\PY{o}{[}\PY{l+s+s1}{\PYZsq{}metadata\PYZsq{}}\PY{o}{]} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}compute=proxy\PYZsq{}}
\PY{k}{end}
\PY{o}{.}
\end{Verbatim}
\end{codelisting}

Ahora será necesario modificar el metadato de la unidad de servicio \kode{postgresql.service} a \textit{MachineMetadata=compute=proxy}, en la sección \textit{[X-Fleet]}. Además, se quita  en las unidades de servicio \kode{app-job.service} y \kode{app-task.service}, para que sean ubicadas en cada máquina.

Para tener siempre el servidor web \kode{app-task} activo en cada máquina, se prepara para que en caso de fallo vuelva a activarse. Esto se consigue con el parámetro \textit{Restart=always}.

Como el servicio \kode{nginx.service} tendrá que balancear la carga entre los distintos servidores web disponibles, será necesario que el servidor \kode{app-task} se registre tras crearse. Este registro se va a realizar en etc, por lo tanto, éste se añade en los parámetros \textit{After} y \textit{Requires} como \kode{etcd2.service} tras \kode{docker.service}. Así, el registro se añade en la sección \textit{ExecStartPost} con clave \kode{/services/app/<dirección IP de la máquina en la que se encuentra>} y valor correspondiente con la dirección IP del contenedor y puerto 9292, pues se hace referencia al servidor puma. Esto permitirá tener tantas entradas \kode{/services/app/*} como máquinas en las que se ejecute hayan. Es importante añadir que cuando el contenedor deje de ejecutarse dicho registro sea borrado. Para interpretar la dirección IP de la máquina es necesario indicar el fichero de variables de entorno \kode{/etc/environment}. 

\begin{codelisting}
\label{code:execstartpost-app-task}
\codecaption{Fichero \kode{user-data.sampleapp.aws}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{l+lScalar+lScalarPlain}{.}
\PY{l+lScalar+lScalarPlain}{write\PYZus{}files}\PY{p+pIndicator}{:}
\PY{l+lScalar+lScalarPlain}{.}
  \PY{l+lScalar+lScalarPlain}{\PYZhy{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{path}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{/home/core/app\PYZhy{}task.service}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{.}
    \PY{l+lScalar+lScalarPlain}{[Service]} 
      \PY{l+lScalar+lScalarPlain}{.}
      \PY{l+lScalar+lScalarPlain}{Restart=always}     
      \PY{l+lScalar+lScalarPlain}{EnvironmentFile=/etc/environment}
      \PY{l+lScalar+lScalarPlain}{.}
      \PY{l+lScalar+lScalarPlain}{ExecStartPost=/bin/bash}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZhy{}c}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZsq{}while}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{!}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{[}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdl{}(/usr/bin/docker}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{inspect}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
      \PY{l+lScalar+lScalarPlain}{\PYZhy{}f}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{=\PYZdq{}\PYZob{}\PYZob{}.State.Running\PYZcb{}\PYZcb{}\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{app\PYZhy{}task)}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{==}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdq{}=true\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{];}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{do}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{sleep}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{1;}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{done;}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
      \PY{l+lScalar+lScalarPlain}{etcdctl}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{set}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{/services/app/\PYZdl{}\PYZob{}COREOS\PYZus{}PRIVATE\PYZus{}IPV4\PYZcb{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdl{}(/usr/bin/docker}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZbs{}}
      \PY{l+lScalar+lScalarPlain}{inspect}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZhy{}f}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZdq{}\PYZob{}\PYZob{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{.NetworkSettings.IPAddress}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{\PYZcb{}\PYZcb{}\PYZdq{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{app\PYZhy{}task):9292\PYZsq{}}
      \PY{l+lScalar+lScalarPlain}{ExecStop=/usr/bin/etcdctl}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{rm}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{/services/app/\PYZdl{}\PYZob{}COREOS\PYZus{}PRIVATE\PYZus{}IPV4\PYZcb{}}
      \PY{l+lScalar+lScalarPlain}{.}
\PY{l+lScalar+lScalarPlain}{.}
\end{Verbatim}
\end{codelisting}

Para que el servicio \kode{nginx.service} pueda registrar los servidores web puma se va a utilizar el servicio confd. Esta utilidad se encarga de controlar los cambios que se produzcan en etc, vigilando las claves que se le indique. Cuando hay un nuevo registro o baja transfiere estos cambios a la configuración de nginx, por medio de una plantilla, modificando el fichero \kode{/etc/nginx.conf} del contenedor \kode{some-nginx}. Luego, hace efectivo el cambio reiniciando el servicio nginx. Este servicio puede ser encontrado en el repositorio \kode{kelseyhightower/confd}, en GitHub, del que se hará una configuración especifica.

Como se pretende compartir el fichero de configuración \kode{nginx.conf} entre el contenedor \kode{some-nginx} y el futuro contenedor \kode{confd} se crea un volumen Docker, llamado \kode{conf-data}, que compartirá el directorio \kode{/etc/nginx} entre ambos. En su definición se indica requiere y ha de ser lanzado después del servicio \kode{docker.service}. Se indica que el servicio será de tipo \textit{oneshot}, lo que significa que el parámetro \textit{ExecStart} ha de ejecutarse solo una vez, sin reintentarlo. También se especifica la opción \textit{RemainAfterExit=yes} para que cuando el contenedor termine permanezca activo. Si el volumen existe en su lanzamiento se detiene y elimina. La imagen que se usa para el contenedor es la de nginx, para poder tener la misma disposición de directorios. Este volumen deberá ubicarse en la máquina \kode{core-01}, por lo que se le añade el metadato \kode{compute=proxy}. Además. será necesario crear la unidad global \kode{fleet-conf-data.service} que de comienzo a este servicio fleet.

\begin{codelisting}
\label{code:conf-data}
\codecaption{Fichero \kode{user-data.sampleapp.aws}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{l+lScalar+lScalarPlain}{.}
\PY{l+lScalar+lScalarPlain}{write\PYZus{}files}\PY{p+pIndicator}{:}
\PY{l+lScalar+lScalarPlain}{.}
  \PY{l+lScalar+lScalarPlain}{\PYZhy{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{path}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{/home/core/conf\PYZhy{}data.service}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{permissions}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{0644}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit] }
      \PY{n+no}{Description=conf data container to share files between confd and some\PYZhy{}nginx}
      \PY{n+no}{After=docker.service}
      \PY{n+no}{Requires=docker.service}
      \PY{n+no}{[Service] }
      \PY{n+no}{Type=oneshot}
      \PY{n+no}{RemainAfterExit=yes}
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker kill conf\PYZhy{}data }
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker rm conf\PYZhy{}data}
      \PY{n+no}{ExecStartPre=/usr/bin/docker pull nginx }
      \PY{n+no}{ExecStart=/usr/bin/docker run \PYZhy{}v /etc/nginx \PYZhy{}\PYZhy{}name conf\PYZhy{}data nginx \PYZbs{}}
      \PY{n+no}{echo \PYZdq{}conf\PYZhy{}data container created\PYZdq{} }
      \PY{n+no}{[X\PYZhy{}Fleet]}
      \PY{n+no}{Global=true}
      \PY{n+no}{MachineMetadata=compute=proxy}
\PY{+Error}{  }\PY{l+lScalar+lScalarPlain}{.}
\PY{l+lScalar+lScalarPlain}{coreos}\PY{p+pIndicator}{:}
  \PY{l+lScalar+lScalarPlain}{.}
  \PY{l+lScalar+lScalarPlain}{units}\PY{p+pIndicator}{:}
  \PY{l+lScalar+lScalarPlain}{.}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{fleet\PYZhy{}conf\PYZhy{}data.service}
    \PY{l+lScalar+lScalarPlain}{command}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{start}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit]}
      \PY{n+no}{Description=Start conf\PYZhy{}data.service using fleet}
      \PY{n+no}{[Service]}
      \PY{n+no}{ExecStart=/usr/bin/fleetctl start /home/core/conf\PYZhy{}data.service}
  \PY{l+lScalar+lScalarPlain}{.}
\end{Verbatim}
\end{codelisting}

El servicio confd necesitará disponer de tres ficheros, que serán escritos en la máquina y exportados al contenedor a través del volumen \kode{conf-data}.

El primero de ellos es la plantilla \kode{nginx.conf.tmpl}, correspondiente a la configuración de nginx. Al incorporarlo a \kode{user-data.sampleapp.app} puede borrarse la definición del fichero \kode{/etc/nginx.conf}, presente en la sección \textit{write-files}. La información a añadir es el bloque \textit{upstream} utilizado para definir los servidores web. Nginx seleccionará uno de ellos dependiendo del método de distribución escogido, en este caso \textit{Round Robin}, del primero al último, que por defecto es el que se implementa. Mediante una plantilla en formato \textit{Go}, que permite gestionar contenido dinámico, se define este nuevo bloque, que ha de estar, a su vez, dentro del bloque \textit{http} y requiere la existencia del bloque \textit{events} para gestionar eventos, aunque no sea utilizado. Así, cuando confd analice etc en busca de cambios para las claves \kode{/services/app/*} sustituirá dinámicamente su contenido por las líneas \textit{server <dirección IP del contenedor \kode{app-task}>:9292}, en lugar de tener que definirlos estáticamente. El servidor escogido pasará a \textit{proxy\_pass http://app;}.

\begin{codelisting}
\label{code:nginx.conf.tmpl}
\codecaption{Fichero \kode{user-data.sampleapp.aws}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{l+lScalar+lScalarPlain}{.}
\PY{l+lScalar+lScalarPlain}{write\PYZus{}files}\PY{p+pIndicator}{:}
\PY{l+lScalar+lScalarPlain}{.}
  \PY{l+lScalar+lScalarPlain}{\PYZhy{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{path}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{/etc/nginx.conf.tmpl}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{permissions}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{0644}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{events \PYZob{}}
      \PY{n+no}{\PYZcb{}}
      \PY{n+no}{http \PYZob{}}
        \PY{n+no}{upstream app \PYZob{}}
        \PY{n+no}{\PYZob{}\PYZob{} range getvs \PYZdq{}/services/app/*\PYZdq{} \PYZcb{}\PYZcb{}}
          \PY{n+no}{server \PYZob{}\PYZob{} . \PYZcb{}\PYZcb{};}
        \PY{n+no}{\PYZob{}\PYZob{} end \PYZcb{}\PYZcb{}}
        \PY{n+no}{\PYZcb{}}
        \PY{n+no}{server \PYZob{}}
          \PY{n+no}{listen 80;}
          \PY{n+no}{root /usr/src/app/public;}
          \PY{n+no}{location / \PYZob{}}
            \PY{n+no}{proxy\PYZus{}set\PYZus{}header X\PYZhy{}Forwarded\PYZhy{}For \PYZdl{}proxy\PYZus{}add\PYZus{}x\PYZus{}forwarded\PYZus{}for;}
            \PY{n+no}{proxy\PYZus{}set\PYZus{}header Host \PYZdl{}http\PYZus{}host;}
            \PY{n+no}{proxy\PYZus{}redirect off;}
            \PY{n+no}{try\PYZus{}files \PYZdl{}uri /page\PYZus{}cache/\PYZdl{}uri /page\PYZus{}cache/\PYZdl{}uri.html @app;}
          \PY{n+no}{\PYZcb{}}
          \PY{n+no}{location @app\PYZob{}}
            \PY{n+no}{proxy\PYZus{}pass http://app;}
            \PY{n+no}{break;}
          \PY{n+no}{\PYZcb{}}
        \PY{n+no}{\PYZcb{}}
      \PY{n+no}{\PYZcb{}}
\PY{l+lScalar+lScalarPlain}{.}
\end{Verbatim}
\end{codelisting}

El siguiente, llamado \kode{nginx.toml}, representa el fichero de configuración confd. Está escrito en formato TOML, comprueba el valor de las claves registradas bajo \kode{/services/app} y realiza la acción del cambio del archivo \kode{/etc/nginx/nginx.conf}, tomando como fuente la plantilla anterior, \kode{nginx.conf.tmpl}. El servicio confd ha de encargarse de reiniciar el servicio Nginx. Para ello ejecuta el fichero \kode{/reload.sh} que permitirá ejecutar una orden mediante el comando curl, haciendo uso del socket de Docker, puesto que éste no estará instalado en el contenedor \kode{confd}. Esta orden hará que capture el identificador del contenedor \kode{some-nginx}, a través del lenguaje JSON \textit{jq}, y detenga el proceso. 

\begin{codelisting}
\label{code:nginx.conf.tmpl}
\codecaption{Fichero \kode{user-data.sampleapp.aws}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{l+lScalar+lScalarPlain}{.}
\PY{l+lScalar+lScalarPlain}{write\PYZus{}files}\PY{p+pIndicator}{:}
\PY{l+lScalar+lScalarPlain}{.}
  \PY{l+lScalar+lScalarPlain}{\PYZhy{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{path}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{/home/core/reload.sh}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{permissions}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{0644}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{\PYZsh{}!/bin/sh}
      \PY{n+no}{curl \PYZhy{}\PYZhy{}unix\PYZhy{}socket /var/run/docker.sock \PYZhy{}XPOST \PYZbs{}}
      \PY{n+no}{\PYZdq{}http://v1.26/containers/\PYZdl{}(curl \PYZhy{}\PYZhy{}unix\PYZhy{}socket /var/run/docker.sock \PYZbs{}}
      \PY{n+no}{\PYZsq{}http://v1.26/containers/json\PYZsq{} | jq \PYZhy{}r \PYZsq{}.[] | \PYZbs{}}
       \PY{n+no}{select(.Names[0] == \PYZdq{}/some\PYZhy{}nginx\PYZdq{}) .Id\PYZsq{})/kill\PYZdq{}}
\PY{+Error}{  }\PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{path}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{/etc/nginx.toml}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{permissions}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{0644}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[template]}
      \PY{n+no}{src = \PYZdq{}nginx.conf.tmpl\PYZdq{}}
      \PY{n+no}{dest = \PYZdq{}/etc/nginx/nginx.conf\PYZdq{}}
      \PY{n+no}{keys = [ \PYZdq{}/services/app\PYZdq{} ] }
      \PY{n+no}{reload\PYZus{}cmd = \PYZdq{}sh /reload.sh\PYZdq{}}
\PY{l+lScalar+lScalarPlain}{.}
\end{Verbatim}
\end{codelisting}

Para disponer de una imagen Docker propia de confd se crea una nueva, llamada \kode{confd\_image}, a partir del fichero Dockerfile. Se utiliza una imagen \textit{Alpine Linux} con un índice de paquetes completo y solo 5 MB de tamaño, en su versión 3.3. Sobre esta imagen se instalarán los paquetes \textit{curl}, para hacer la recarga del servicio, y el \textit{jq}, para obtener el identificador del contenedor \kode{some-nginx}. Luego se obtiene la herramienta confd para sistemas Linux con CPUs AMD de 64 bits, se le dan permisos de lectura, escritura y ejecución al propietario en \kode{/usr/bin/confd} para poder realizar operaciones y se limpia la memoria caché utilizada. Además, se añade el directorio vacío \kode{/etc/confd} que será utilizado para alojar los ficheros. Por último, se indica que una vez arrancado un contenedor, el servicio confd vigilará los cambios producidos en etc, en todo el clúster, cada 5 segundos.

\begin{codelisting}
\label{code:dockerfile-confd}
\codecaption{Contenido de \kode{Dockerfile}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+no}{FROM} \PY{l+s+ss}{alpine}\PY{p}{:}\PY{l+m+mi}{3}\PY{o}{.}\PY{l+m+mi}{3}
\PY{n+no}{LABEL} \PY{n}{confd\PYZus{}image}\PY{o}{.}\PY{n}{version}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{0.1}\PY{l+s+s2}{\PYZdq{}} \PY{n}{confd\PYZus{}image}\PY{o}{.}\PY{n}{release}\PY{o}{\PYZhy{}}\PY{n}{date}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{2017\PYZhy{}05\PYZhy{}02}\PY{l+s+s2}{\PYZdq{}}
\PY{n+no}{MAINTAINER} \PY{n+no}{Carolina} \PY{n+no}{Santana} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{c.santanamartel@gmail.com}\PY{l+s+s2}{\PYZdq{}}
\PY{n+no}{RUN} \PY{n}{apk} \PY{n}{add} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n}{update} \PY{n}{curl} \PY{o}{\PYZam{}\PYZam{}} \PY{n}{apk} \PY{n}{add} \PY{o}{\PYZhy{}}\PY{o}{\PYZhy{}}\PY{n}{update} \PY{n}{jq} \PY{o}{\PYZam{}\PYZam{}} \PY{p}{\PYZbs{}}
    \PY{n}{curl} \PY{o}{\PYZhy{}}\PY{n}{o} \PY{l+s+sr}{/}\PY{l+s+sr}{usr}\PY{l+s+sr}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{confd} \PY{o}{\PYZhy{}}\PY{n}{L} \PY{l+s+ss}{https}\PY{p}{:}\PY{l+s+sr}{/}\PY{l+s+sr}{/}\PY{n}{github}\PY{o}{.}\PY{n}{com}\PY{o}{/}\PY{n}{kelseyhightower}\PY{o}{/}\PY{n}{confd}\PY{o}{/}\PY{n}{releases}\PY{o}{/}
            \PY{n}{download}\PY{o}{/}\PY{n}{v0}\PY{o}{.}\PY{l+m+mi}{7}\PY{o}{.}\PY{l+m+mi}{1}\PY{o}{/}\PY{n}{confd}\PY{o}{\PYZhy{}}\PY{l+m+mi}{0}\PY{o}{.}\PY{l+m+mi}{7}\PY{o}{.}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{linux}\PY{o}{\PYZhy{}}\PY{n}{amd64} \PY{o}{\PYZam{}\PYZam{}} \PY{p}{\PYZbs{}}
    \PY{n}{chmod} \PY{l+m+mi}{755} \PY{o}{/}\PY{n}{usr}\PY{o}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{confd} \PY{o}{\PYZam{}\PYZam{}} \PY{n}{rm} \PY{o}{\PYZhy{}}\PY{n}{rf} \PY{l+s+sr}{/}\PY{l+s+sr}{var}\PY{l+s+sr}{/}\PY{n}{cache}\PY{o}{/}\PY{n}{apk}\PY{o}{/}\PY{o}{*}
\PY{n+no}{ADD} \PY{n}{etc}\PY{o}{/}\PY{n}{confd}\PY{o}{/} \PY{l+s+sr}{/}\PY{l+s+sr}{etc}\PY{l+s+sr}{/}\PY{n}{confd}
\PY{n+no}{CMD} \PY{l+s+sr}{/}\PY{l+s+sr}{usr}\PY{l+s+sr}{/}\PY{n}{bin}\PY{o}{/}\PY{n}{confd} \PY{o}{\PYZhy{}}\PY{n}{interval}\PY{o}{=}\PY{l+m+mi}{5} \PY{o}{\PYZhy{}}\PY{n}{node}\PY{o}{=}\PY{l+s+ss}{http}\PY{p}{:}\PY{l+s+sr}{/}\PY{l+s+sr}{/}\PY{n+nv+vg}{\PYZdl{}COREOS\PYZus{}PRIVATE\PYZus{}IPV4}\PY{p}{:}\PY{l+m+mi}{4001}
\end{Verbatim}
\end{codelisting}

De esta manera, será necesario crear en local el directorio vacío \kode{etc/confd}, construir la imagen y subirla a Docker Hub para poder usarla:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }mkdir etc/confd
\PY{n+nv}{\PYZdl{} }docker login
\PY{n+nv}{\PYZdl{} }docker build \PYZhy{}t confd\PYZus{}image .
\PY{n+nv}{\PYZdl{} }docker tag \PYZlt{}image\PYZhy{}id\PYZgt{} carolina/confd\PYZus{}image:latest
\end{Verbatim}
\end{framed_shaded}

Cuando la subida termina, se define el fichero \kode{/home/core/confd.service} que creará el contenedor \kode{confd}. Así, requiere y ha de ser creado tras los servicios \kode{docker.service}, \kode{etcd2.service}, pues leerá de él, y \kode{conf-data.service}. Se indica que comience la ejecución de \textit{ExecStart} y se añade el fichero de variables de entorno \kode{/etc/environment} para que pueda conocer las direcciones IP de las máquinas que conforman el clúster. Si en el momento de ser lanzado ya existiera se manda a detener el proceso y se elimina el contenedor. Antes de crearlo se descarga la última versión de la imagen \kode{confd\_image}, si no se tiene. Además de pasarle la variable de entorno comentada, se exporta el \textit{socket} de Docker, para poder usarlo a la hora de ejecutar la recarga del servicio nginx y los tres archivos escritos, donde los destinos finales serán \kode{/etc/confd/templates/nginx.conf.tmpl}, \kode{/reload.sh} y \kode{/etc/confd/conf.d/nginx.toml}. Por último se añade el volumen \kode{conf-data}. Este contenedor deberá ubicarse en la máquina \kode{core-01}, por lo que se le añade el metadato \kode{compute=proxy}. También se crea la unidad global \kode{fleet-confd.service} que comienza este servicio.

\begin{codelisting}
\label{code:confd}
\codecaption{Fichero \kode{user-data.sampleapp.aws}}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{l+lScalar+lScalarPlain}{.}
\PY{l+lScalar+lScalarPlain}{write\PYZus{}files}\PY{p+pIndicator}{:}
\PY{l+lScalar+lScalarPlain}{.}
  \PY{l+lScalar+lScalarPlain}{\PYZhy{}}\PY{l+lScalar+lScalarPlain}{ }\PY{l+lScalar+lScalarPlain}{path}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{/home/core/confd.service}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{permissions}\PY{p+pIndicator}{:} \PY{l+s}{\PYZdq{}}\PY{l+s}{0644}\PY{l+s}{\PYZdq{}}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit] }
      \PY{n+no}{Description=confd container that updates nginx.conf file and kill }
                  \PY{n+no}{some\PYZhy{}nginx container to restart nginx service when detects}
                  \PY{n+no}{a new rails server has been registered}
      \PY{n+no}{After=docker.service etcd2.service conf\PYZhy{}data.service}
      \PY{n+no}{Requires=docker.service etcd2.service conf\PYZhy{}data.service}
      \PY{n+no}{[Service] }
      \PY{n+no}{TimeoutStartSec=0 }
      \PY{n+no}{EnvironmentFile=/etc/environment}
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker kill confd }
      \PY{n+no}{ExecStartPre=\PYZhy{}/usr/bin/docker rm confd}
      \PY{n+no}{ExecStartPre=/usr/bin/docker pull carolina/confd\PYZus{}image:latest }
      \PY{n+no}{ExecStart=/bin/bash \PYZhy{}c \PYZsq{}/usr/bin/docker run \PYZhy{}\PYZhy{}rm \PYZhy{}\PYZhy{}name confd \PYZbs{}}
      \PY{n+no}{\PYZhy{}e COREOS\PYZus{}PRIVATE\PYZus{}IPV4=\PYZdl{}\PYZob{}COREOS\PYZus{}PRIVATE\PYZus{}IPV4\PYZcb{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}v \PYZdq{}/var/run/docker.sock:/var/run/docker.sock\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}v \PYZdq{}/etc/nginx.conf.tmpl:/etc/confd/templates/nginx.conf.tmpl\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}v \PYZdq{}/home/core/reload.sh:/reload.sh\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}v \PYZdq{}/etc/nginx.toml:/etc/confd/conf.d/nginx.toml\PYZdq{} \PYZbs{}}
      \PY{n+no}{\PYZhy{}\PYZhy{}volumes\PYZhy{}from=conf\PYZhy{}data carolina/confd\PYZus{}image \PYZsq{}}
      \PY{n+no}{ExecStop=/usr/bin/docker stop confd}
      \PY{n+no}{[X\PYZhy{}Fleet]}
      \PY{n+no}{Global=true}
      \PY{n+no}{MachineMetadata=compute=proxy}
\PY{+Error}{  }\PY{l+lScalar+lScalarPlain}{.}
\PY{l+lScalar+lScalarPlain}{coreos}\PY{p+pIndicator}{:}
  \PY{l+lScalar+lScalarPlain}{.}
  \PY{l+lScalar+lScalarPlain}{units}\PY{p+pIndicator}{:}
  \PY{l+lScalar+lScalarPlain}{.}
  \PY{p+pIndicator}{\PYZhy{}} \PY{l+lScalar+lScalarPlain}{name}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{fleet\PYZhy{}confd.service}
    \PY{l+lScalar+lScalarPlain}{command}\PY{p+pIndicator}{:} \PY{l+lScalar+lScalarPlain}{start}
    \PY{l+lScalar+lScalarPlain}{content}\PY{p+pIndicator}{:} \PY{p+pIndicator}{|}
      \PY{n+no}{[Unit]}
      \PY{n+no}{Description=Start confd.service using fleet}
      \PY{n+no}{[Service]}
      \PY{n+no}{ExecStart=/usr/bin/fleetctl start /home/core/confd.service}
  \PY{l+lScalar+lScalarPlain}{.}
\end{Verbatim}
\end{codelisting}

En último lugar se modifica el fichero \kode{/home/core/nginx.service} de manera que se especifique en las opciones \textit{After} y \textit{Requires} que requiere y ha de ir tras los servicios \kode{docker.service}, \kode{conf-data.service} y \kode{confd.service}. Como el servicio confd se encargará de detener el proceso de \kode{some-nginx} se añade la opción \textit{Restart=always} que hará que la unidad de servicio vuelva a ejecutarse con la nueva configuración Nginx. Además, en la creación del contenedor, \textit{ExecStart}, se elimina la opción \kode{--link app-task:app} y el fichero de configuración que se exportaba anteriormente \kode{-v /etc/nginx.conf:/etc/nginx/nginx.conf} y se añade la opción \kode{--volumes-from=conf-data}.

A continuación se despliega la infraestructura:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }. \PYZti{}/.aws\PYZhy{}credentials/aws\PYZhy{}credentials \PY{o}{\PYZam{}\PYZam{}} vagrant up \PYZhy{}\PYZhy{}provider\PY{o}{=}aws
\end{Verbatim}
\end{framed_shaded}

En primer lugar se comprueba la salud del clúster:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }\PY{k}{for} i in \PY{l+m}{1} \PY{l+m}{2} 3\PY{p}{;} \PY{k}{do} vagrant ssh core\PYZhy{}0\PY{n+nv}{\PYZdl{}i} \PYZhy{}c \PY{l+s+s1}{\PYZsq{}etcdctl cluster\PYZhy{}health\PYZsq{}}\PY{p}{;} \PY{k}{done}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{images/figures/health-confd.png}
\caption{Salud del clúster contemplada por las 3 máquinas.}
\end{figure}

También, se visualizan las máquinas que conforman el clúster, con los metadatos asignados y las unidades fleet presentes en cada máquina:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }\PY{k}{for} i in \PY{l+m}{1} \PY{l+m}{2} 3\PY{p}{;} \PY{k}{do} vagrant ssh core\PYZhy{}0\PY{n+nv}{\PYZdl{}i} \PYZhy{}c \PY{l+s+s1}{\PYZsq{}fleetctl list\PYZhy{}machines\PYZsq{}}\PY{p}{;} \PY{k}{done}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{images/figures/machines-confd.png}
\caption{Información de las máquinas contemplada por las 3 máquinas.}
\end{figure}

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }\PY{k}{for} i in \PY{l+m}{1} \PY{l+m}{2} 3\PY{p}{;} \PY{k}{do} vagrant ssh core\PYZhy{}0\PY{n+nv}{\PYZdl{}i} \PYZhy{}c \PY{l+s+s1}{\PYZsq{}fleetctl list\PYZhy{}units\PYZsq{}}\PY{p}{;} \PY{k}{done}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{images/figures/units-confd.png}
\caption{Información de las unidades fleet contemplada por las 3 máquinas.}
\end{figure}

Para continuar con las pruebas se escoge la máquina \kode{core-01}.

Se comprueban los registros existentes bajo la clave \kode{services/app/<dirección IP de \kode{app-task}>}. Las claves se averiguan con el comando \kode{etcdctl ls --recursive /services/app} y luego los valores con el comando \kode{etcdctl get /services/app/<dirección IP de \kode{app-task}>}:

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/recursive-confd.png}
\caption{Obtención de claves y valores para \kode{services/app/*}.}
\end{figure}

Ahora que los contenedores \kode{app-task} se han registrado se comprueba que el servicio confd ha actualizado el fichero de configuración Nginx, a medida que los ha ido detectando. Esto es observado desde los registros de operación del servicio \kode{confd.service} y dentro del contenedor \kode{some-nginx}, viendo el contenido del fichero \kode{/etc/nginx/nginx.conf}:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{c}{\PYZsh{} journalctl \PYZhy{}u confd.service}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{images/figures/update-journal-confd.png}
\caption{Actualización de la configuración Nginx desde \kode{confd}.}
\end{figure}

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{c}{\PYZsh{} docker exec \PYZhy{}it some\PYZhy{}nginx sh}
\PY{c}{\PYZsh{} cat /etc/nginx/nginx.conf}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/figures/update-nginx-confd.png}
\caption{Fichero \kode{nginx.conf} actualizado.}
\end{figure}

Tras ver el orden en el que se han colocado las distintas direcciones IP de los contenedores \kode{app-task} puede conocerse de qué máquina proviene cada uno inspeccionando el contenedor en cada máquina:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }docker inspect app\PYZhy{}task \PYZhy{}f \PY{l+s+s2}{\PYZdq{}\PYZob{}\PYZob{} .NetworkSettings.IPAddress \PYZcb{}\PYZcb{}\PYZdq{}}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/figures/IP1-confd.png}
\caption{Dirección IP del contenedor \kode{app-task} en la máquina \kode{core-01}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/figures/IP2-confd.png}
\caption{Dirección IP del contenedor \kode{app-task} en la máquina \kode{core-02}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/figures/IP3-confd.png}
\caption{Dirección IP del contenedor \kode{app-task} en la máquina \kode{core-03}.}
\end{figure}

Por lo tanto el orden establecido ha sido: \kode{core-03}, \kode{core-01} y \kode{core-02}.

Así mismo, si se realizan 3 peticiones de servicio prácticamente juntas, desde la máquina \kode{core-01} en el que se encuentra el proxy, se puede comprobar por la hora marcada como las peticiones se envían en dicho orden.

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }\PY{k}{for} i in \PY{l+m}{1} \PY{l+m}{2} 3\PY{p}{;} \PY{k}{do} curl http://localhost:80 \PY{p}{|} tail \PYZhy{}n 15\PY{p}{;} \PY{k}{done}
\end{Verbatim}
\end{framed_shaded}

Para comprobar cuándo fueron resueltas las peticiones se ejecuta:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }docker logs app\PYZhy{}task
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/logs3-confd.png}
\caption{Información de \kode{app-task} sobre una petición en \kode{core-03}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/logs1-confd.png}
\caption{Información de \kode{app-task} sobre una petición en \kode{core-01}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/logs2-confd.png}
\caption{Información de \kode{app-task} sobre una petición en \kode{core-02}.}
\end{figure}

Finalmente se visualiza el servicio, siendo balanceada la carga entre los distintos servidores web de forma transparente al usuario.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/figures/curl-confd.png}
\caption{Respuesta al servicio por comandos en \kode{core-01}. \label{curl-confd}}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/figures/web-confd.png}
\caption{Respuesta al servicio por el navegador web en \kode{core-01}. \label{fig:web-confd}}
\end{figure}

Además se inicia sesión con uno de los usuarios con los que se ha alimentado la base de datos para comprobar el correcto funcionamiento de las solicitudes a la base de datos desde esta infraestructura. El usuario a usar es \textit{example@railstutorial.org} y la contraseña \textit{foobar}:

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/figures/login-confd.png}
\caption{Inicio de sesión en el servicio con el usuario \textit{Example User}.}
\end{figure}

Haciendo una escritura desde el mismo servidor que ha iniciado la sesión:

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/figures/post-confd.png}
\caption{Escritura de un comentario con el usuario \textit{Example User}.}
\end{figure}

La información sobre las instancias puede ser consultada desde la consola de AWS:

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/aws-confd.png}
\caption{Visualización de las instancias desde la consola de AWS.}
\end{figure}

\subsection{Resultado}

En esta iteración se ha conseguido el despliegue de la aplicación distribuida en un clúster, compuesto de tres instancias, en la nube pública de Amazon Web Services.

De esta manera se posee una infraestructura con servicios web de la aplicación replicados en una o tantas instancias como existan en el clúster, apreciando la característica de redundancia. Además, el proxy dispone de la opción de balanceo de carga con el uso del servicio confd, que controla los nuevos registros o bajas de los servidores web y actualiza el fichero de configuración Nginx para que pueda resolver hacia ellos. Con el clúster constituido sobre la nube pública de AWS, permitiendo mantener los servicios basados en contenedores, así como las máquinas monitorizadas entre sí, también está presente la alta disponibilidad de servicios. De esta forma, la configuración establecida responde ante el fallo de una unidad o máquina, recargándola para volver a ofrecer correctamente el servicio.

El correcto funcionamiento del servicio web de la aplicación quedará disponible mediante el despliegue de la infraestructura desde Vagrant, estableciendo las variables de entorno del proveedor. La manera de proceder es accediendo directamente por el proxy, como se puede ver en la \hyperref[fig:web-confd]{Figura \ref{fig:web-confd}}, en la máquina \kode{core-01}:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }. \PYZti{}/.aws\PYZhy{}credentials/aws\PYZhy{}credentials
\PY{n+nv}{\PYZdl{} }vagrant up \PYZhy{}\PYZhy{}provider\PY{o}{=}aws \PY{o}{\PYZam{}\PYZam{}} vagrant ssh core\PYZhy{}01
\PY{c}{\PYZsh{} curl http://localhost:80}
\end{Verbatim}
\end{framed_shaded}

A su vez, se replica la infraestructura final para el proveedor VirtualBox, reemplazando el contenido del fichero \kode{user-data.sampleapp.vbox} por el de \kode{user-data.sampleapp.aws}, manteniendo la interfaz de flannel como pública y la red virtual 10.1.0.0/16. En el fichero \kode{Vagrantfile} se pasarían a parte común la adición de los metadatos a las máquinas.

En este caso el despliegue será de la siguiente manera:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }vagrant up 
\end{Verbatim}
\end{framed_shaded}

Cuando el despliegue termina se comprueba la salud positiva del clúster:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }\PY{k}{for} i in \PY{l+m}{1} \PY{l+m}{2} 3\PY{p}{;} \PY{k}{do} vagrant ssh core\PYZhy{}0\PY{n+nv}{\PYZdl{}i} \PYZhy{}c \PY{l+s+s1}{\PYZsq{}etcdctl cluster\PYZhy{}health\PYZsq{}}\PY{p}{;} \PY{k}{done}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/health-vbox.png}
\caption{Salud del clúster contemplada por las 3 máquinas.}
\end{figure}

También se obtienen las máquinas que componen el clúster desde la máquina \kode{core-03} y las unidades de servicio desde \kode{core-02}, como prueba de que se está implementando la infraestructura final:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }vagrant ssh core\PYZhy{}03
\PY{c}{\PYZsh{} fleetctl list\PYZhy{}machines}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/machines-vbox.png}
\caption{Información de las máquinas del clúster.}
\end{figure}

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }vagrant ssh core\PYZhy{}02
\PY{c}{\PYZsh{} fleetctl list\PYZhy{}units}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/figures/units-vbox.png}
\caption{Información de las unidades de servicio del clúster.}
\end{figure}

Por último, se replican en VirtualBox cada una de las pruebas realizadas para el proveedor AWS, confirmando el correcto funcionamiento del servicio de la aplicación y balanceo de carga entre los diferentes servidores web. Así, se muestra el correcto resultado de una petición desde \kode{core-01}:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PY{n+nv}{\PYZdl{} }vagrant ssh core\PYZhy{}01
\PY{c}{\PYZsh{} curl http://localhost:80 | tail \PYZhy{}n 15}
\end{Verbatim}
\end{framed_shaded}

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{images/figures/curl-vbox.png}
\caption{Acceso al servicio desde \kode{core-01}.}
\end{figure}

El resultado final está presentado en los siguientes repositorios GitHub:
\begin{itemize}
\item \kode{CarolinaSantana/sample\_app\_rails\_4-1}: a partir del que se construye la imagen de la aplicación.
\item \kode{CarolinaSantana/coreos-vagrant}: Infraestructura como Código a desplegar en los proveedores VirtualBox o Amazon Web Services.
\end{itemize}